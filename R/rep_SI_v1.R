#' ---
#' title: <font size="2">Supporting information for </font><br><font size="6">Data aggregation blurs inferred temporal trends in bird sampling</font>
#' author: <font size="3">Martin Bulla & Peter Mikula</font><br><br><font size="2">created by Martin Bulla</font><br>
#' date: <font size="1.5">`r Sys.time()`</font>
#' bibliography: ../Resources/_bib.bib
#' link-citations: true
#' output:
#'     html_document:
#'         toc: true
#'         toc_float: true
#'         toc_depth: 4
#'         code_folding: hide
#'         link-citations: yes
#'         css: ../Resources/styles.css
#' base:  href="/[MA_NHB]/"
#' ---

#' <style> body {text-align: justify}</style>
 
#+ r setup, include=FALSE 

#knitr::opts_knit$set(root.dir = normalizePath(".."))
knitr::opts_chunk$set(
  message = FALSE, warning = FALSE, cache = FALSE, 
  fig.retina = 1)  

#' # General note  
#' 
#' When using this content **PLEASE CITE** our preprint [@bulla_MA_2026] and this repository [@bulla_supporting_2026], which contains all code, data and outputs used in this replication.
#' 
#' Scripts generating the outputs of this html are available upon clicking the `code` button at the top right above each display item!
#' 
#' This project was initiated as a part of the "Promoting reproduction and replication at scale" partnership between *Nature Human Behaviour* and the *Institute for Replication* [@editorial2024]. Thus, the below Supporting information contains computational reproduction (repeating the same analyses on the same dataset, using the same code), as well as robustness replication (addressing the same research question using different data or different analytical procedures on the same dataset) of the key findings from the Ellis-Soto et al. 2023, Nat Hum Beh paper titled "Historical redlining is associated with increasing geographical disparities in bird biodiversity sampling in the United States" [@ellis-soto_historical_2023]. The full version of our report for the Nature Human Behaviour - Institute for Replication initiative is available as a preprint [@bulla_replication_2025].  

#' 
#' ### Content of the repository
#' [The current Supporting information](https://martinbulla.github.io/MA_NHB/)  
#'  
#' [original_paper](https://github.com/MartinBulla/MA_NHB/tree/main/original_paper/) folder used for `Data` and `Code` subfolders, both two folders with the file structure as provided by the authors, obtained by downloading the authors' [repository](https://doi.org/10.5281/zenodo.8052525); note that due to copyright, we do not post the actual `Data` and `Code` subfolders, you need to download those from the authors' [repository](https://doi.org/10.5281/zenodo.8052525); for the actual paper with its supplements see https://www.nature.com/articles/s41562-023-01688-5 
#'  
#' [Data](https://github.com/MartinBulla/MA_NHB/tree/main/Data/) stores further data in folders:  
#' - [provided](https://github.com/MartinBulla/MA_NHB/tree/main/Data/provided/) with additional data shared by the authors upon the request from The Institute for Replication  
#' - [MaPe](https://github.com/MartinBulla/MA_NHB/tree/main/Data/MaPe/) with datasets generated by our scripts; those with `DAT_glmmTMB` the sets of fitted glmmTMB models, `DAT_obs_year_polygon.zip` which contains `DAT_obs_year_polygon.csv` with number of observations per year and each polygon, and `DAT_obs_year_polygon_source.csv` contains the same, but differentiated by the source of the observations. The latter two were generated by [rev_Dat_temporal_trend_v1.R](https://github.com/MartinBulla/MA_NHB/tree/main/R/rev_Dat_temporal_trend_v1.R). Note, the zip files needs unzipping as `DAT_obs_year_polygon.csv` is the key for some of the scripts. Files with `DAT_bam` prefix store fitted bam models, need to be downloaded from [Open Science Framework repository](https://osf.io/mfkan) (as GitHub does not support files >100MB) and stored in the MaPe folder for the scripts to run.
#'  
#' [R](https://github.com/MartinBulla/MA_NHB/tree/main/R/) - scripts used in the analysis:  
#' - [_runRmarkdown.R](https://github.com/MartinBulla/MA_NHB/tree/main/R/_runRmarkdown.R) generates the html [Supplement](https://martinbulla.github.io/MA_NHB/) from the following R-script:  
#' - [rep_SI_v5.R](https://github.com/MartinBulla/MA_NHB/tree/main/R/rep_SI_v5.R) contains all scripts used to generate the paper outputs, including the display items  
#' - [rev_Dat_temporal_trend_v1.R](https://github.com/MartinBulla/MA_NHB/tree/main/R/rev_Dat_temporal_trend_v1.R) is an adjusted author's script that saves the # of sampling observations per each polygon and year  
#' 
#' [Outputs](https://github.com/MartinBulla/MA_NHB/tree/main/Output/) - separate files of all outputs used in our manuscript, the html supporting infomration file  as well as [Model_ass](https://github.com/MartinBulla/MA_NHB/tree/main/Output/Model_ass) folder with model assumptions of all models that we fitted. 
#'
#' [LICENSE](https://github.com/MartinBulla/MA_NHB/tree/main/LICENSE): terms of reuse - applicable only after this work is published as a preprint or in a scientific journal, until then the data are not available for reuse.
#  
#' 
#' ***
#' 
#' ##### Code to load tools and data
#+ start, echo = T, results = 'hide', warning=FALSE

# 1. load or install packages
pkgs <- c("cowplot","data.table","dplyr", "forcats", "ggh4x","ggplot2","ggpp", "ggsci","ggtext", "glmmTMB", "grid", "gtable", "here","kableExtra", "lme4", "MASS", "mgcv", "patchwork", "readr", "readxl", "scales", "sjPlot", "tibble", "tidyverse")  # list of packages

install_if_missing <- function(pkgs) {
  to_install <- setdiff(pkgs, rownames(installed.packages()))
  if (length(to_install)) install.packages(to_install, dependencies = TRUE)
  invisible(lapply(pkgs, require, character.only = TRUE))
}

install_if_missing(pkgs)

# 2. constants and functions
recreate_data = FALSE # use TRUE, if you wish to recreate the per year data (❗runs for long), instead of loading them from .Data/ 

recompute_diag = FALSE # shall the diagnostic plots be recomputed (take time)

# Color palette for redlining
holc_pal <- c('#92BC6B' # green
              , '#92C7C9' # blue
              , '#E7DC6B' # yellow
              , '#E47D67' # red
)#, '#A9A9A9' # dark gray)

# Color palette for effect sizes
col_lz <- ggsci::pal_locuszoom("default")(7)

# add tag in a circle to a ggplot panel
tag_pos <- c(0.045, 0.94)   # x, y in panel [0..1]

offset_tag <- theme(
  plot.tag.location = "panel",
  plot.tag.position = tag_pos,
  plot.tag = element_text(hjust = 0.5, vjust = 0.5, size = 11)
)

circle_at_tag <- function(xy = tag_pos, r_pt = 6){
  annotation_custom(
    grob = grid::circleGrob(
      x = unit(xy[1], "npc"), y = unit(xy[2], "npc"),
      r = unit(r_pt, "pt"),
      gp = grid::gpar(fill = "white", col = "grey25", lwd = .6)
    ),
    xmin = -Inf, xmax = Inf, ymin = -Inf, ymax = Inf
  )
}               

# remove x-axis of some ggplot panels
gtable_filter_remove <- function (x, name, trim = TRUE){
  matches <- !(x$layout$name %in% name)
  x$layout <- x$layout[matches, , drop = FALSE]
  x$grobs <- x$grobs[matches]
  if (trim) 
    x <- gtable_trim(x)
  x
}
#  place log10 minor breaks at 2..9 * 10^k
minor_breaks_log10 <- function(lims) {
  lo <- floor(log10(lims[1])); hi <- ceiling(log10(lims[2]))
  as.numeric(outer(1:9, 10^(lo:hi), `*`))
}

# minor breaks only for >0
minor_breaks_log10_if0 <- function(lims){
  hi <- lims[2]; lo <- max(1, lims[1])
  if (hi <= 1) return(NULL)
  as.numeric(outer(1:9, 10^(floor(log10(lo)):ceiling(log10(hi))), `*`))
}

# function for quick plotting
plot_effects_holc <- function(
          model, data, 
          year = "year", grade = "holc_grade",
          palette = NULL, n = 100, ylab = "Sampling density",
          title_model = TRUE, title_text = NULL, ver = 'v1',
          title_size = 9, title_colour = "grey40",
          save_png = TRUE, outdir = ".", filename = NULL,
          width = 9, height = 4.8, dpi = 300, timestamp = FALSE
  ) {

     safe_slug <- function(x, max = 120) {
        x <- tolower(x)
        x <- gsub("\\s+", " ", x)
        x <- gsub("[^a-z0-9]+", "-", x)
        x <- gsub("(^-+|-+$)", "", x)
        substr(x, 1, max)
    }

     xr <- seq(min(data[[year]], na.rm = TRUE),
          max(data[[year]], na.rm = TRUE), length.out = n)

     eff <- suppressWarnings(effects::Effect(c(year, grade), model,
                         xlevels = setNames(list(xr), year)))

     pp <- as.data.frame(eff)
     pp[[grade]] <- factor(pp[[grade]])
     pp$fit_o   <- exp(pp$fit)
     pp$lower_o <- exp(pp$lower)
     pp$upper_o <- exp(pp$upper) 

     # integer 5-year ticks within data range
      yr_rng   <- range(pp[[year]], na.rm = TRUE)
      yr_breaks <- seq(ceiling(yr_rng[1]/5)*5, floor(yr_rng[2]/5)*5, by = 5)   

     lev <- levels(pp[[grade]])
     if (is.null(palette)) {
     palette <- setNames(scales::hue_pal()(length(lev)), lev)
     } else if (is.null(names(palette)) && length(palette) == length(lev)) {
     names(palette) <- lev
     }

     p1 = 
     ggplot(pp, aes(year, fit_o, color = holc_grade, fill = holc_grade)) +
     geom_ribbon(aes(ymin = lower_o, ymax = upper_o), alpha = .15, colour = NA) +
     geom_line() +
     scale_color_manual(values = holc_pal, name = 'HOLC grade')+
     scale_fill_manual(values = holc_pal, name = 'HOLC grade')+
     scale_x_continuous(breaks = yr_breaks,
                     labels = scales::number_format(accuracy = 1),
                     minor_breaks = NULL) +
     labs(x = "Year", y = ylab, subtitle = 'ln-scale') +
     scale_y_continuous(trans='log')+
     theme_light()+
     theme( plot.subtitle = element_text(size = 10, colour = "grey40"))

     p2 = 
     ggplot(pp, aes(year, fit_o, color = holc_grade, fill = holc_grade)) +
     geom_ribbon(aes(ymin = lower_o, ymax = upper_o), alpha = .15, colour = NA) +
     geom_line() +
     scale_color_manual(values = palette, name = 'HOLC grade')+
     scale_fill_manual(values = palette, name = 'HOLC grade')+
     scale_x_continuous(breaks = yr_breaks,
                     labels = scales::number_format(accuracy = 1),
                     minor_breaks = NULL) +
     labs(x = "Year", y = ylab, subtitle = 'original-scale') +
     theme_light()+
     theme( plot.subtitle = element_text(size = 10, colour = "grey40"))

     p2_ <- p2 + theme(axis.title.y = element_blank())

     title_str <- if (!is.null(title_text)) title_text else
        if (isTRUE(title_model))
            paste0("Model: ",
                gsub("\\s+", " ",
                        paste(trimws(deparse(formula(model), width.cutoff = 500)),
                            collapse = " ")))
        else NULL

      g <- (p1 + p2_) + patchwork::plot_layout(guides = "collect", axis_titles = "collect")

      if (!is.null(title_str)) {
        g <- g + patchwork::plot_annotation(
          title = title_str,
          theme = ggplot2::theme(
            plot.title = element_text(
              size = title_size,
              colour = title_colour,
              hjust = 0))
        )
      }
   
     # Optional save (timestamp at END)
     if (isTRUE(save_png)) {
      if (is.null(filename)) {
        stamp <- if (timestamp) paste0("__", format(Sys.time(), "%Y%m%d-%H%M")) else ""
        fname <- paste0(safe_slug(title_str), stamp, "__", ver, ".png")
       } else {
        fname <- filename
       }
      dir.create(outdir, showWarnings = FALSE, recursive = TRUE)
      ggplot2::ggsave(file.path(outdir, fname), plot = g,
                    width = width, height = height, dpi = dpi, bg = "white")
      message("Saved: ", file.path(outdir, fname))
     }

     g
}

# function to extract HOLC grade and if availalbe year slopes per HOLC fixed effects (works with lmer/glmer (lme4), glmmTMB (cond by default), lm/glm
ext_fixef <- function(m, component = "cond") {
  ## get fixed effects & vcov (SE) by class ##
  get_fe_se <- function(m, component) {
    if (inherits(m, "glmmTMB")) {
      fe <- fixef(m)[[component]]                  # named numeric
      Vfull <- stats::vcov(m)                      # may be a list (cond/zi/disp) or matrix
      V <- if (is.list(Vfull)) Vfull[[component]] else Vfull
      se <- sqrt(diag(as.matrix(V)))
      list(fe = fe, se = se)
    } else if (inherits(m, c("lmerMod","glmerMod"))) {
      fe <- lme4::fixef(m); V <- stats::vcov(m); list(fe = fe, se = sqrt(diag(as.matrix(V))))
    } else if (inherits(m, c("lm","glm"))) {
      fe <- stats::coef(m);   V <- stats::vcov(m); list(fe = fe, se = sqrt(diag(as.matrix(V))))
    } else {
      stop("Unsupported model class: ", paste(class(m), collapse="/"))
    }
  }

  FE <- get_fe_se(m, component)
  fe <- FE$fe; se <- FE$se
  nm <- names(fe); z <- stats::qnorm(0.975)
  pick <- function(p) grep(p, nm, value = TRUE)

  ## SD of scale(year) if available (used only if slope terms exist) ##
  sdy <- tryCatch({
    as.numeric(attr(stats::model.frame(m)[["scale(year)"]], "scaled:scale"))
  }, error = function(e) NA_real_)  

  ## names to extract ##
  ints_n <- pick("^holc_grade[^:]+$")                    # e.g. holc_gradeB/C/D
  slps_n <- pick("^holc_grade[^:]+:scale\\(year\\)$")    # interaction (if present)

  ## intercept rows ##
  ints <- if (length(ints_n)) {
    tibble::tibble(
      type        = "intercept",
      holc_grade  = sub("^holc_grade", "", ints_n),
      estimate    = unname(fe[ints_n]),
      std.error   = unname(se[match(ints_n, nm)]),
      conf.low    = estimate - z*std.error,
      conf.high   = estimate + z*std.error,
      estimate_per_year  = NA_real_,
      conf.low_per_year  = NA_real_,
      conf.high_per_year = NA_real_
    )
  } else tibble::tibble(
    type=character(), holc_grade=character(), estimate=double(), std.error=double(),
    conf.low=double(), conf.high=double(),
    estimate_per_year=double(), conf.low_per_year=double(), conf.high_per_year=double()
  )

  ## slope rows (if the interaction exists) ##
  slps <- if (length(slps_n)) {
    base <- tibble::tibble(
      type        = "slope_per_SDyear",
      holc_grade  = sub(":.*","", sub("^holc_grade","", slps_n)),
      estimate    = unname(fe[slps_n]),
      std.error   = unname(se[match(slps_n, nm)]),
      conf.low    = estimate - z*std.error,
      conf.high   = estimate + z*std.error
    )
    if (is.finite(sdy) && sdy > 0) {
      dplyr::mutate(base,
        estimate_per_year  = estimate / sdy,
        conf.low_per_year  = conf.low / sdy,
        conf.high_per_year = conf.high / sdy
      )
    } else {
      dplyr::mutate(base,
        estimate_per_year  = NA_real_,
        conf.low_per_year  = NA_real_,
        conf.high_per_year = NA_real_
      )
    }
  } else tibble::tibble(
    type=character(), holc_grade=character(), estimate=double(), std.error=double(),
    conf.low=double(), conf.high=double(),
    estimate_per_year=double(), conf.low_per_year=double(), conf.high_per_year=double()
  )

  dplyr::bind_rows(ints, slps)
}

# function to extract fixed effects from lm
ext_fixef_lm <- function(m) {
  mf  <- model.frame(m)
  sdy <- as.numeric(attr(mf[["scale(year)"]], "scaled:scale"))

  fe <- coef(m); V <- as.matrix(vcov(m)); se <- sqrt(diag(V)); z <- qnorm(0.975)
  nm <- names(fe)

  ints_n <- nm[grepl("^holc_grade", nm) & !grepl(":", nm)]
  slps_n <- nm[grepl("^holc_grade", nm) & grepl(":.*scale\\(year\\)", nm)]
  base_slope_n <- nm[grepl("^scale\\(year\\)$", nm)]

  lvls <- levels(mf$holc_grade)

  lvl_from <- function(x) {
    x <- sub("^holc_grade[_]?", "", x)
    x <- sub(":.*$", "", x)
    if (x %in% lvls) return(x)
    # patterns like DA/DB/DC/DD with ref first
    if (nchar(x) == 2 && substr(x,1,1) == lvls[1]) {
      y <- substr(x,2,2)
      if (y %in% lvls) return(y)
      if (x == paste0(lvls[1], lvls[1])) return(lvls[1])  # DD -> D
    }
    x
  }

  ints <- tibble::tibble(
    type       = "intercept",
    holc_grade = vapply(ints_n, lvl_from, character(1)),
    estimate   = fe[ints_n],
    std.error  = se[ints_n],
    conf.low   = estimate - z * std.error,
    conf.high  = estimate + z * std.error
  )

  slps_core <- tibble::tibble(
    type       = "slope_per_SDyear",
    holc_grade = vapply(sub(":.*$", "", slps_n), lvl_from, character(1)),
    estimate   = fe[slps_n],
    std.error  = se[slps_n],
    conf.low   = estimate - z * std.error,
    conf.high  = estimate + z * std.error
  )

  ref_level <- lvls[1]
  if (length(base_slope_n) == 1) {
    base_row <- tibble::tibble(
      type       = "slope_per_SDyear",
      holc_grade = ref_level,
      estimate   = fe[base_slope_n],
      std.error  = se[base_slope_n],
      conf.low   = fe[base_slope_n] - z * se[base_slope_n],
      conf.high  = fe[base_slope_n] + z * se[base_slope_n]
    )
    slps <- dplyr::bind_rows(slps_core, base_row)
  } else {
    slps <- slps_core
  }

  slps <- slps |>
    dplyr::mutate(
      estimate_per_year  = estimate / sdy,
      conf.low_per_year  = conf.low / sdy,
      conf.high_per_year = conf.high / sdy
    ) |>
    dplyr::arrange(factor(holc_grade, lvls))

  dplyr::bind_rows(
    ints |> dplyr::arrange(factor(holc_grade, lvls)),
    slps
  )
}

# function to extract fixed effects (vs. HOLC D) and year slopes (per 1 SD of year)
# works for: lmer/glmer/glmer.nb (lme4) AND glmmTMB (nbinom2/1 etc.)
ext_fixef_D <- function(m) {
  # --- 1) grab fixed effects and VCOV depending on class ---
  if (inherits(m, "glmmTMB")) {
    fe <- fixef(m)$cond
    V  <- as.matrix(vcov(m)$cond)
    mf <- model.frame(m)              # model frame (has scale(year))
  } else {
    fe <- lme4::fixef(m)
    V  <- as.matrix(stats::vcov(m))
    mf <- stats::model.frame(m)
  }

  # --- 2) SD of 'year' used inside scale(year) ---
  # Try to read the 'scaled:scale' attribute first; fall back to sd(year)
  sdy <- NA_real_
  if ("scale(year)" %in% names(mf)) {
    sdy <- suppressWarnings(as.numeric(attr(mf[["scale(year)"]], "scaled:scale")))
  }
  if (!is.finite(sdy)) {
    if ("year" %in% names(mf)) {
      sdy <- stats::sd(mf[["year"]], na.rm = TRUE)
    } else {
      stop("Cannot determine SD of year: no 'scale(year)' attribute and 'year' not in model.frame().")
    }
  }

  # --- 3) pick the coefficients we need ---
  nm <- names(fe)

  # intercept (differences vs D) terms: holc_grade_D*
  ints_n <- grep("^holc_grade_D[^:]+$", nm, perl = TRUE, value = TRUE)

  # slope interaction terms with scale(year)
  slps_n <- grep(
    "^(?:holc_grade_D[^:]+:scale\\(year\\)|scale\\(year\\):holc_grade_D[^:]+)$",
    nm, perl = TRUE, value = TRUE
  )

  # helper: extract the grade letter after 'holc_grade_D'
  lev <- function(term) sub(".*holc_grade_D([^:]+).*", "\\1", term)

  z <- stats::qnorm(0.975)

  # --- 4) build tidy tibbles (handle empty sets gracefully) ---
  make_ints <- function() {
    if (length(ints_n) == 0) {
      return(tibble::tibble(
        type = character(), holc_grade = character(),
        estimate = numeric(), std.error = numeric(),
        conf.low = numeric(), conf.high = numeric()
      ))
    }
    se <- sqrt(diag(V))[ints_n]
    tibble::tibble(
      type = "intercept",
      holc_grade = lev(ints_n),
      estimate = unname(fe[ints_n]),
      std.error = unname(se),
      conf.low = estimate - z * std.error,
      conf.high = estimate + z * std.error
    )
  }

  make_slps <- function() {
    if (length(slps_n) == 0) {
      return(tibble::tibble(
        type = character(), holc_grade = character(),
        estimate = numeric(), std.error = numeric(),
        conf.low = numeric(), conf.high = numeric(),
        estimate_per_year = numeric(),
        conf.low_per_year = numeric(),
        conf.high_per_year = numeric()
      ))
    }
    se <- sqrt(diag(V))[slps_n]
    out <- tibble::tibble(
      type = "slope_per_SDyear",
      holc_grade = lev(slps_n),
      estimate = unname(fe[slps_n]),
      std.error = unname(se),
      conf.low = estimate - z * std.error,
      conf.high = estimate + z * std.error
    )
    dplyr::mutate(
      out,
      estimate_per_year = estimate / sdy,
      conf.low_per_year = conf.low / sdy,
      conf.high_per_year = conf.high / sdy
    )
  }

  dplyr::bind_rows(make_ints(), make_slps())
}

# function to extract fixed effects from lm objects (D baseline; matches lmer extractor)
ext_fixef_D_lm <- function(m) {
  mf  <- model.frame(m)
  sdy <- as.numeric(attr(mf[["scale(year)"]], "scaled:scale"))
  fe  <- coef(m); V <- as.matrix(vcov(m)); se <- sqrt(diag(V)); z <- qnorm(0.975)
  nm  <- names(fe)
  ints_n <- grep("^holc_grade_D[^:]+$", nm, perl = TRUE, value = TRUE)
  slps_n <- grep("^(?:holc_grade_D[^:]+:scale\\(year\\)|scale\\(year\\):holc_grade_D[^:]+)$",
                 nm, perl = TRUE, value = TRUE)
  lev <- function(term) sub(".*holc_grade_D([^:]+).*", "\\1", term)

  ints <- tibble(
    type = "intercept", holc_grade = lev(ints_n),
    estimate = fe[ints_n], std.error = se[ints_n]
  ) |>
    dplyr::mutate(conf.low = estimate - z*std.error,
           conf.high = estimate + z*std.error)

  slps <- tibble(
    type = "slope_per_SDyear", holc_grade = lev(slps_n),
    estimate = fe[slps_n], std.error = se[slps_n]
  ) |>
    dplyr::mutate(conf.low = estimate - z*std.error,
           conf.high = estimate + z*std.error,
           estimate_per_year = estimate / sdy,
           conf.low_per_year = conf.low / sdy,
           conf.high_per_year= conf.high / sdy)

  bind_rows(ints, slps)
}

# composite tick labels: multiplicative interpretation on tick labels
lab_log_rr <- function(x) sprintf("%0.2f\n(×%0.2f)", x, exp(x))

# composite tick labels: first line = log, second line = % change
lab_log_plus_pct <- function(x) sprintf("%.2f\n(%+d%%)", x, round((exp(x)-1)*100))

# model assumption function
m_ass = function(
    name = 'diagnostics', # define model name (used as file name and in a title)
    mo = your_model, # mo: model
    dat = your_data,  # dat: data used in the model
    accumulate = FALSE,   # collect panels (TRUE) or draw immediately (FALSE)

    # lists of variables to visualize against residuals
    cont = NULL, # vector of variable names used as continues fixed effects
    categ = NULL, # vector of variable names used as categorical fixed effects
    trans = "none", # vector of transformations used for each fixed effect
    #nested = FALSE, # indicate whether some of the random intercepts are nested,
    # plots to show
    offset = FALSE, # TRUE if offset was used
    show_base     = TRUE,   # fitted-vs-resid, sqrt|res|
    show_binned    = FALSE,  # observed vs fitted (binned fitted and means of observed)
    show_reqq     = TRUE,      # random-effects Q–Q
    show_temporal = TRUE, # PACF(residuals)
    show_temporal_grouped = NULL, # PACF on means of temporal variable (e.g. year)
    show_spatial = TRUE, # PACF(residuals)
    lat_var = 'lat',
    lon_var = 'lon',
    max_points = 10000, # max number of points to plot; if n higher, the data will be sampled to get 5000
    PNG = TRUE,  width_ = 10, height_ = 2,
    n_col = 6, n_row = NULL, # number of columns and rows if automatic calculation not desirable
    colour_pt = rgb(0,0,0,0.15),
    wrap_title = FALSE, wrap_width = 100, 
    outdir = 'Output/Model_ass'
  ){ #output directory

  ## example ##
   #m_ass(name = "Table S1a - full a", mo = mhs, dat = dh, fixed = c("SD", "FlockSize", "BodyMass", "rad", "rad", "Temp", "Human"), trans = c("log", "log", "log", "sin", "cos", "", ""), outdir = here::here("Outputs/modelAss/")) 
  ## testing ##
  # name = "test1"; mo = d_ri; dat = hB_; accumulate = FALSE; cont = NULL; categ = 'holc_grade'; trans = 'none'; show_base = TRUE; show_binned = FALSE; show_reqq = TRUE; show_temporal = TRUE; show_temporal_grouped = NULL; show_spatial = TRUE; lat_var = 'lat'; lon_var = 'lon'; PNG = FALSE;  width_ = 10; height_ = 5; n_col = 6; n_row = NULL; wrap_title = FALSE; wrap_width = 100; outdir = 'Output/Model_ass/'
  
  ## model frame vs data — alignment check ##
   if (nrow(dat) != length(residuals(mo))) {
      stop(sprintf("m_ass ERROR: nrow(dat)=%d but model has %d rows.",
                  nrow(dat), length(residuals(mo))))
    }   

  ## functions and constants ##
    have     <- function(x) !is.null(x) && length(x) && all(x %in% names(dat))
    safe_num <- function(x) as.numeric(x)
    col_neg <- rgb(83, 95, 124, alpha = 100, maxColorValue = 255)
    col_pos <- rgb(253, 184, 19, alpha = 100, maxColorValue = 255)
    
    ### flatten (grp -> coef -> vector) into (grp:coef -> vector)
    flatten_re <- function(re_list) {
      out <- list()
      for (g in names(re_list)) {
        x <- re_list[[g]]
        if (is.list(x)) for (c in names(x)) out[[paste(g, c, sep=":")]] <- x[[c]] else out[[g]] <- x
      }
      out
    }

    ### extrat random effets (blups), if present; which`'all' get also slopes
    get_re_vals <- function(mo, which = "all") {
      pick_cols <- function(dd) {
        if (which == "all") {
          out <- lapply(names(dd), function(col) setNames(dd[[col]], rownames(dd)))
          names(out) <- names(dd); out
        } else if (which == "first") {
          list(setNames(dd[[1L]], rownames(dd)))
        } else { # "(Intercept)"
          if ("(Intercept)" %in% names(dd)) list(setNames(dd[["(Intercept)"]], rownames(dd)))
          else list(setNames(dd[[1L]], rownames(dd)))
        }
      }


      if (inherits(mo, "lmerMod") || inherits(mo, "merMod")) {
        re <- lme4::ranef(mo)
        out <- lapply(re, pick_cols)
        # flatten one level if 'which' picks single column
        if (which != "all") out <- lapply(out, `[[`, 1L)
        return(out)

      } else if (inherits(mo, "glmmTMB")) {
        re <- glmmTMB::ranef(mo)
        re <- if (!is.null(re$cond)) re$cond else re[[1L]]
        out <- lapply(re, pick_cols)
        if (which != "all") out <- lapply(out, `[[`, 1L)
        return(out)

      } else if (inherits(mo, "gam")) {  # mgcv::bam
        beta <- coef(mo); sm <- mo$smooth; out <- list()
        for (s in sm) if (isTRUE(s$bs %in% "re")) {
          idx <- s$first.para:s$last.para
          lev <- levels(mo$var.summary[[s$term]])
          vals <- beta[idx]; names(vals) <- lev[seq_along(vals)]
          out[[s$term]] <- vals
        }
        return(out)
      
      } else if (inherits(mo, "negbin") || inherits(mo, "glm") || inherits(mo, "lm")) {
      return(list())  
      } else stop("Unsupported model class.")
    }

    ### get RE Names only (uniform)
    get_re_names <- function(mo, which = "all") {
     re <- get_re_vals(mo, which = which)           # your function
     if (which == "all") names(flatten_re(re)) else names(re)
    }

    ### make qq plots for random effects, if present
    plot_re_qq <- function(mo, which="all") {
     re <- get_re_vals(mo, which = which)
     if (!length(re)) return(invisible(NULL))  
     # flatten if which="all": make names like "grp:coef"
     if (which == "all") {
      flat <- list()
      for (grp in names(re)) {
        if (is.list(re[[grp]])) {
          for (coefnm in names(re[[grp]])) {
            flat[[paste(grp, coefnm, sep=":")]] <- re[[grp]][[coefnm]]
          }
        } else flat[[grp]] <- re[[grp]]
      }
      re <- flat
     }

     for (nm in names(re)) {
      panels_add(local({
        x <- re[[nm]]         # capture vector
        ttl <- nm             # capture name
        function() {          # <-- single closure that actually draws
          qqnorm(x, main = ttl, col = colour_pt)
          qqline(x, col = "red")
        }
      }))
     }
    }

   ### panels (collector + immediate draw switch) ##
   panels <- list(); .i <- 0L
   panels_add <- function(fun) {
      if (accumulate) {
        .i <<- .i + 1L
        panels[[.i]] <<- fun
      } else {
        fun()
      }
      invisible(NULL)
   }
  
  ## get fit, res, and family ##
    y_obs <- tryCatch(
      stats::model.response(stats::model.frame(mo)),
      error = function(e) NULL
    )
    if (is.matrix(y_obs) && ncol(y_obs)==2L) y_obs <- y_obs[,1]/rowSums(y_obs) # for binomial response
    fit <- tryCatch(stats::fitted(mo, type="response"), error=function(e) stats::fitted(mo))
    res <- tryCatch(stats::residuals(mo, type="pearson"),  error=function(e) stats::residuals(mo))
    fit <- safe_num(fit); res <- safe_num(res); y_obs <- safe_num(y_obs)

    fam <- tryCatch(tolower(family(mo)$family), error = function(e) NA_character_)
    is_nb <- grepl("negative binomial|nbinom1|nbinom2", fam) |  inherits(mo, "negbin") # MASS::glm.nb etc.
    is_pois <- !is_nb && grepl("poisson", fam) # captures "poisson" and "quasipoisson"
    is_bin  <- !is_nb && !is_pois && grepl("binomial", fam) # captures "binomial", "quasibinomial" but excludes "negative binomial"

    # pretty family label (round NB theta if available)
      fam_txt <- if (!is.na(fam)) {
        if (inherits(mo, "negbin") && !is.null(mo$theta)) {
          paste0("negative binomial(theta: ", round(mo$theta, 2), ")")
        } else {
          fam
        }
      } else "n/a"

  ## stratified subsample for plotting ##
  N <- length(fit)
  idx_plot <- seq_len(N)

  if (!is.null(max_points) && is.finite(max_points) && N > max_points) {
    n_strata <- 50L
    brks <- unique(quantile(fit, probs = seq(0, 1, length.out = n_strata + 1),
                     na.rm = TRUE))
    
    if (length(brks) > 2L) {
      # standard stratified sampling across (length(brks)-1) non-empty bins
      bin <- cut(fit, brks, include.lowest = TRUE, labels = FALSE)

      n_eff_strata <- length(brks) - 1L
      per_bin <- ceiling(max_points / n_eff_strata)

      set.seed(5) 
      idx_plot <- sort(unique(unlist(tapply(idx_plot, bin, function(ii) {
      if (length(ii) <= per_bin) ii else sample(ii, per_bin)}))))

    } else {
      # fallback: too little variation in fitted values to stratify
      # -> simple random sample
      set.seed(5)
      idx_plot <- sort(sample(idx_plot, max_points))
    }
  }

  fit_s = fit[idx_plot]
  res_s = res[idx_plot]
  y_obs_s = y_obs[idx_plot]


  ## MAKE PANELS ##

   ### if drawing immediately, open device now  ###
    if (!accumulate) {
      
      base_plots <-
        (if (show_base) 4 else 0) +                # resid~fit, sqrt|res|, obs~fit, QQ 
        1 +                          # dispersion panel (always when available)
        #(if (show_base && !show_binned &&
        #     !is.null(mo$family) &&
        #     mo$family$family == "gaussian") 1 else 0) +  # normal QQ only for gaussian;  TODO: perhaps add optional/rough for Poisson/NB
        (if (show_binned) 2 else 0) + 
        (if (is_bin) -1 else 0) 

      has_re   <- length(get_re_names(mo)) > 0L
      rand_plots <- if (show_reqq && has_re) length(get_re_names(mo)) else 0L

      n <- base_plots + rand_plots + 
          length(cont) + length(categ) +
          (if (offset) 2 else 0) +
          (if (show_temporal) 1 else 0) +
          (if (!is.null(show_temporal_grouped)) 1 else 0) +
          (if (show_spatial) 5 else 0) # 3 maps + Moran's I panel + semivariogram

      if (n == 0) n <- 1
      if (is.null(n_row)) n_row <- ceiling(n / n_col)

      if (PNG) {
      png(paste0(here::here(outdir,name), ".png"), 
        width = width_, height = height_*n_row, 
        units="in", res=300
        ) #res = 150 ok for html
      par(mfrow = c(n_row, n_col), tcl = -0.08, cex = 0.5, 
        cex.main = 0.95, mar = c(2, 2, 2, 1), mgp=c(1,0,0),
        oma = c(1,1,4,1))
      } else {
      dev.new(width=width_,height=height_*n_row)
      par(mfrow = c(n_row, n_col), tcl = -0.08, cex = 0.5, 
      cex.main = 0.95, mar = c(2, 2, 2, 1), mgp=c(1,0,0), 
      oma = c(1,1,2,1))
      }
    }


  ### fitted vs residuals ###
    if (show_base) {
      # patterns/funnel shapes → hints at over/underdispersion or mean-variance misfit
      panels_add(function() {    
        plot(fit_s, res_s,
            xlab = "Fitted (response)",
            ylab = "Pearson residuals",
            main = "Pearson residuals vs fitted",
            pch  = 16, col  = colour_pt
          )#scatter.smooth(fit,res,col=colour_pt, xlab="Fitted (response)", ylab="Pearson residuals", main="Pearson residuals vs fitted")
          ok <- is.finite(fit) & is.finite(res)
          lines(lowess(fit[ok], res[ok]), lwd = 1.2) # smoother on full data
          abline(h=0, lty=2, col ='red')
        })

      # highlights non-constant residual spread with μ
      panels_add(function() {  
        plot(fit_s, sqrt(abs(res_s)),
            xlab = "Fitted (response)",
            ylab = "Sqrt(|Pearson res|)",
            main = "Sqrt(|res|) vs fitted",
            pch  = 16, col  = colour_pt
          ) # scatter.smooth(fit,sqrt(abs(res)), col=colour_pt, xlab="Fitted (response)", ylab="Sqrt(|Pearson res|)", main="Sqrt(|res|) vs fitted")
          ok <- is.finite(fit) & is.finite(res)
          lines(lowess(fit[ok], sqrt(abs(res[ok]))), lwd = 1.2) # smoother on full data
          abline(h=0, lty=2, col ='red')
        })  
    }

    if (show_base && !is_pois && !is_nb && !is_bin) {
      panels_add(function() {
        plot(fit_s, y_obs_s,
            xlab = "Fitted (response)",
            ylab = "Observed",
            main = "Observed vs fitted",
            pch  = 16, col = colour_pt)
        abline(0, 1, lty = 2, col = "red")
        ok <- is.finite(fit) & is.finite(y_obs)
        lines(lowess(fit[ok], y_obs[ok]), lwd = 1.2)
      })
    }  

    if (show_binned) {
      # interpratation: for Poisson/NB counts: systematic deviation from 1:1 or too-wide scatter suggests misspecification or extra structure; for interpretation of effects/predictions, use rates if that’s the scientific quantity

      # panel with equal-count bins for counts
      panels_add(function() {  

       if (is_bin) { # Binomial / quasibinomial

        # bins on fitted probabilities
        seq_ = seq(0.05, 0.95, by = 0.1)
        bins_ = cut(fit, seq(0, 1, by = 0.1), include.lowest = TRUE)

        # bin means & SE 
        means = tapply(y_obs, bins_, mean)
        se = tapply(y_obs, bins_, function(x) sd(x)/sqrt(length(x))) # binomial approx: n <- tapply(y, b, length); se <- sqrt(pmax(means * (1 - means) / n, 0)) #
        
        plot(fit_s, jitter(y_obs_s, amount=0.05), 
          xlab = "Fitted values", ylab= "Observed proportion", 
          col = colour_pt, main="Binned observed vs fitted")
          abline(0, 1, lty = 2, col = "red")
          points(seq_, means, pch = 16, col = "orange")
          segments(seq_, means-2*se, seq_, means+2*se, col = "orange", lwd = 2)
       
       } else if (is_pois || is_nb) { # Poisson / NB (incl. glmmTMB, bam NB, glm.nb)
                                      # For NB, large scatter but no clear bias is a norm; For negbin, large scatter but no clear bias is normal; consistent bias across bins signals poor mean structure.
 
        n_bins <- 10
        brks   <- unique(quantile(fit, probs = seq(0, 1, length.out = n_bins + 1),
                          na.rm = TRUE))

        if (length(brks) > 2L) {
         bins_  <- cut(fit, brks, include.lowest = TRUE)

         obs_mean <- tapply(y_obs, bins_, mean)
         fit_mean <- tapply(fit,   bins_, mean)
         se       <- tapply(y_obs, bins_, function(x)
                            sd(x, na.rm = TRUE) / sqrt(length(x)))

         plot(fit_mean, obs_mean,
            xlab = "Mean fitted count", ylab = "Mean observed count",
            main = "Binned observed vs fitted counts",
            pch = 16, col = "orange")
         abline(0, 1, lty = 2, col = "red")
         segments(fit_mean, obs_mean - 2 * se,
                fit_mean, obs_mean + 2 * se,
                col = "orange", lwd = 2)
        } else {
          ## Fallback: not enough unique fitted values for stable binning ##
          plot(fit_s, res_s,
               xlab = "Fitted", ylab = "Residual",
               main = "Binned plot skipped: low variation in fitted values",
               col  = colour_pt)
          abline(h = 0, lty = 2, col = "red")
        }

       } else {
       ## Fallback for other families ##
       plot(fit_s, res_s,
           xlab = "Fitted", ylab = "Residual",
           main = paste("Binned plot not defined for", fam_txt),
           col = colour_pt)
       abline(h = 0, lty = 2, col = "red")
       }
      })

      # zoomed binned plot 
      panels_add(function() {

        zoom_q <- 0.95  # use central 95% of fitted values

        if (is_bin) {

          q    <- quantile(fit, zoom_q, na.rm = TRUE)
          idx  <- fit <= q
          if (!any(idx, na.rm = TRUE)) { idx <- rep(TRUE, length(fit)) }

          # bins on truncated range
          seq_  <- seq(0.05, 0.95, by = 0.1)
          bins_ <- cut(fit[idx], seq(0, 1, by = 0.1), include.lowest = TRUE)

          means <- tapply(y_obs[idx], bins_, mean)
          se    <- tapply(y_obs[idx],  bins_, function(x) sd(x)/sqrt(length(x)))

          idx_s <- fit_s <= q

          plot(fit_s[idx_s], jitter(y_obs_s[idx_s], amount = 0.05),
               xlab = "Fitted values (zoomed)", ylab = "Observed proportion",
               col  = colour_pt,
               main = paste0("Zoomed binned (≤ ", zoom_q * 100, "% fitted)"))
          abline(0, 1, lty = 2, col = "red")
          points(seq_, means, pch = 16, col = "orange")
          segments(seq_,
                   means - 2 * se,
                   seq_,
                   means + 2 * se,
                   col = "orange", lwd = 2)

        } else if (is_pois || is_nb) {

          q    <- quantile(fit, zoom_q, na.rm = TRUE)
          idx  <- fit <= q
          if (!any(idx, na.rm = TRUE)) { idx <- rep(TRUE, length(fit)) }

          n_bins <- 10
          brks_z <- unique(quantile(fit[idx],
                             probs = seq(0, 1, length.out = n_bins + 1),
                             na.rm = TRUE))
          
          if (length(brks_z) > 2L) {
           bins_z <- cut(fit[idx], brks_z, include.lowest = TRUE)

           obs_mean_z <- tapply(y_obs[idx], bins_z, mean)
           fit_mean_z <- tapply(fit[idx],   bins_z, mean)
           se_z       <- tapply(y_obs[idx], bins_z, function(x)
                                 sd(x, na.rm = TRUE) / sqrt(length(x)))

           idx_s <- fit_s <= q

           plot(fit_s[idx_s], y_obs_s[idx_s],
               xlab = "Fitted count (zoomed)",
               ylab = "Observed count",
               main = paste0("Binned observed vs fitted (≤ ", zoom_q * 100, "% fitted)"),
               pch  = 16, col = colour_pt)
           abline(0, 1, lty = 2, col = "red")
           segments(fit_mean_z,
                   obs_mean_z - 2 * se_z,
                   fit_mean_z,
                   obs_mean_z + 2 * se_z,
                   col = "orange", lwd = 2)
           points(fit_mean_z, obs_mean_z,
                 pch = 16, col = "orange")
          } else {
            ## Fallback zoom
            idx_s <- fit_s <= q
            if (!any(idx_s, na.rm = TRUE)) idx_s <- rep(TRUE, length(fit_s))
            plot(fit_s[idx_s], res_s[idx_s],
                 xlab = "Fitted (zoomed)", ylab = "Residual",
                 main = paste0("Zoomed residuals vs fitted (", zoom_q * 100, "%)"),
                 col  = colour_pt)
            abline(h = 0, lty = 2, col = "red")
          }

        } else {

          ## Fallback zoom: residuals vs fitted in dense region
          q    <- quantile(fit, zoom_q, na.rm = TRUE)
          idx  <- fit_s <= q
          if (!any(idx, na.rm = TRUE)) { idx <- rep(TRUE, length(fit_s)) }

          plot(fit_s[idx], res_s[idx],
               xlab = "Fitted (zoomed)", ylab = "Residual",
               main = paste("Zoomed residuals vs fitted (", zoom_q * 100, "%)", sep = ""),
               col  = colour_pt)
          abline(h = 0, lty = 2, col = "red")
        }
      })

    }

     # TODO: observe the behaviour of the chunk below and perhaps remove QQ if bin model present (and do so likely using family info from mo)
     #if (show_base & !show_binned) { # noraml QQ use only for Gaussian
     if (show_base & !is_bin) { 
      panels_add(function() {  
         qqnorm(res, main=list("Normal Q-Q Plot: residuals", cex=0.8),col=colour_pt);qqline(res, col = 'red')
        })
    }

  ### offset ###
    if (offset) {
      off <- tryCatch(
        stats::model.offset(stats::model.frame(mo)),
        error = function(e) NULL
      )

      if (!is.null(off)) {

        # (1) Residuals vs offset (scale used in model)
        panels_add(local({
          v  <- as.numeric(off)
          r0 <- res
          function() {
            scatter.smooth(v, r0,
              xlab = "Offset (linear predictor scale)",
              ylab = "Pearson residuals",
              col  = colour_pt,
              main = "Residuals vs offset"
            )
            abline(h = 0, lty = 2, col = "red")
          }
        }))

        # (2) Leverage vs offset (if hatvalues exist); interpretation: points with high hatvalues (>2*#_of_parameters/n or >3*#_of_parameters/n) and extreme offset → those combinations can dominate estimation; if plot shows hatvalues (e.g. below 1) in a modest range and no crazy outliers, you’re fine. If most points are clustered and only a few are clearly higher, those few are the ones to inspect (especially if they also have extreme residuals or offset).

        hv <- tryCatch(stats::hatvalues(mo), error = function(e) NULL)
        if (!is.null(hv)) {
          panels_add(local({
            h <- hv
            v <- as.numeric(off)
            function() {
              plot(h, v,
                xlab = "Leverage (hatvalues)",
                ylab = "Offset (linear predictor scale)",
                main = "Leverage vs offset", col = colour_pt
              )
            }
          }))
        }
      }
    }
  
  ### random effects ###
    if (show_reqq) plot_re_qq(mo, which="all")
   
  ### fixed effects ###
   #### continuous
    if (any(!is.na(cont) & nzchar(trimws(cont)))) {
      cont  <- trimws(cont)
      trans <- tolower(if (length(trans)) rep_len(trans, length(cont)) else rep("none", length(cont)))

      for (i in seq_along(cont)) {
        vname <- cont[i]; if (!vname %in% names(dat)) next
        if (!is.numeric(dat[[vname]])) next

        v  <- dat[[vname]]
        tr <- trans[i]; lab <- vname
        
        if (tr %in% c("log","ln")) { v[v <= 0] <- NA; lab <- if (tr=="log") paste0("log10(",vname,")") else paste0("ln(",vname,")"); v <- if (tr=="log") log10(v) else log(v) }
        if (tr == "abs") { v <- abs(v); lab <- paste0("abs(",vname,")") }
        if (tr == "sin") { v <- sin(v); lab <- paste0("sin(",vname,")") }
        if (tr == "cos") { v <- cos(v); lab <- paste0("cos(",vname,")") }
        if (all(is.na(v))) next  

        panels_add(local({
          vx <- v[idx_plot]; xl <- lab; vn <- vname; r0 <- res_s; 
          function() {
            plot(vx, r0,
                xlab = xl, ylab = "Pearson residuals",
                main = paste("Residuals vs", vn),
                pch  = 16, col  = colour_pt
              )#scatter.smooth(vx, r0, xlab=xl, ylab="Pearson residuals", col=colour_pt, main=paste("Residuals vs", vn))
            ok <- is.finite(v) & is.finite(res)
          if (sum(ok) > 1L) lines(lowess(v[ok], res[ok]), lwd = 1.2) # smoother on full data
            abline(h=0, lty=2, col ='red')
          }
        }))
      }
    }

   ### categorical ###
   if (any(!is.na(categ) & nzchar(trimws(categ)))) {
    for (cat_var in categ) {
      if (!cat_var %in% names(dat)) next
      panels_add(local({  
        cv <- cat_var
        function() { boxplot(res ~ dat[[cv]], border="grey40", ylab="Pearson residuals", xlab = cv, main=paste("Residuals by", cv)); abline(h=0, lty=2, col="red")}
      }))
    }
   }

  ### autocorrelations ###
   if(show_temporal){
     panels_add(function() {  
      acf(res, type="p", ylab = "Partial series residual (PACF)", main="Temporal autocorrelation")
     })
   }

   if (!is.null(show_temporal_grouped)) { # emulates grouping lightly (can reveal group-level leftover structure); if PACF shows decay, but the group-level PACF is flat → the apparent decay comes from pooled residuals across groups, not true within-group autocorrelation
    g  <- dat[[show_temporal_grouped]]
    ok <- !is.na(g) & !is.na(res)         
    by <- aggregate(res[ok], list(grouping = g[ok]), mean) #  mean residual per grouping variable
    nn <- aggregate(res[ok], list(grouping = g[ok]), length)  # sizes

    o  <- order(by$grouping)
    m  <- by$x[o]
    n  <- nn$x[o]
    z  <- m * sqrt(n / max(n)) # variance-stabilized means

    panels_add(local({
      zz <- z; yl <- paste("PACF of", show_temporal_grouped, "mean residuals (weighted)")
      function() { pacf(zz, main="Temporal autocorrelation across groups", ylab = yl)}
    }))
   }

   if(show_spatial && have(c(lon_var, lat_var))) {  

      spdata <- data.table(resid = res_s, x = dat[[lon_var]][idx_plot], y = dat[[lat_var]][idx_plot])
      spdata[ , col := ifelse(resid < 0, col_neg, col_pos)]
      cex_vals <- c(1, 1.5, 2, 2.5, 3)
      spdata[, cex := as.numeric(cut(abs(resid), 5, labels = cex_vals))]
      
      x  <- dat[[lon_var]]
      y  <- dat[[lat_var]]
      ok <- is.finite(x) & is.finite(y) & is.finite(res)
      coords_ok <- cbind(x[ok], y[ok])

      # tiny jitter for exact duplicates to satisfy knearneigh / variogram
      dup <- duplicated(coords_ok)
      if (any(dup)) {
        eps <- sqrt(.Machine$double.eps)
        n_dup <- sum(dup)
        coords_ok[dup, ] <- coords_ok[dup, ] +
          matrix(runif(2L * n_dup, -eps, eps), ncol = 2L)
      }

      panels_add(function() { 
       plot(spdata$x, spdata$y, col = spdata$col, cex = spdata$cex, pch = 16, main = "Spatial distribution of residuals", xlab = "longitude", ylab = "latitude")
       legend("topleft", pch=16, cex=0.8, legend=c('<0','>=0'), col=c(col_neg,col_pos))
      })

      panels_add(function() { 
       spdata_neg = spdata[resid<0]  
       plot(spdata_neg$x, spdata_neg$y, col = spdata_neg$col, cex = spdata_neg$cex, pch = 16, main = "Spatial distribution of residuals (<0)", xlab = "longitude", ylab = "latitude")
      })
      
      panels_add(function() {    
       spdata_pos = spdata[resid>=0]  
       plot(spdata_pos$x, spdata_pos$y,col=spdata_pos$col, cex=spdata_pos$cex, pch= 16, main=list('Spatial distribution of residuals (>=0)', cex=0.8), xlab = "longitude", ylab = "latitude")
      })

      panels_add(function() {# Moran's I summary panel
          plot.new()
          box()
          title("Spatial autocorrelation (Moran's I)")
          
          if (!requireNamespace("spdep", quietly = TRUE)) {
            text(0.05, 0.8, adj = 0,
                 "Package 'spdep' not available.\nSkipping Moran's I.")
            return(invisible())
          }

          if (sum(ok) < 10L) {
            text(0.05, 0.8, adj = 0, 
              "Too few valid points for Moran's I.") 
            return(invisible())
          }
          
          # k-NN weights; robust to irregular sampling
          mm <- tryCatch({
            nb <- spdep::knearneigh(coords_ok, k = 4) |>
                  spdep::knn2nb()
            lw <- spdep::nb2listw(nb, style = "W")
            spdep::moran.test(res[ok], lw, na.action = na.exclude)
          }, error = function(e) NULL)
          
          if (is.null(mm)) {
            text(0.05, 0.8, adj = 0,
                "Moran's I failed (neighbors).\nCheck coordinates.")
            return(invisible())
          }
          
          Ival <- unname(mm$estimate[["Moran I statistic"]])
          pval <- mm$p.value
          
          text(0.05, 0.8, adj = 0, labels = sprintf("Moran's I: %.3f", Ival))
          text(0.05, 0.7, adj = 0, labels = sprintf("p-value:   %.3g", pval))
          text(0.05, 0.5, adj = 0, labels = "I ≈ 0 & ns:\nno strong global spatial autocorrelation")
          text(0.05, 0.3, adj = 0, labels = "|I| large & sig.:\n consider spatial structure")
      })
         
      panels_add(function() { ## Residual semivariogram
          if (!requireNamespace("gstat", quietly = TRUE) ||
              !requireNamespace("sp", quietly = TRUE)) {
            plot.new()
            box()
            title("Residual semivariogram")
            text(0.05, 0.8, adj = 0,
                 "Need 'gstat' + 'sp'.\nSkipping semivariogram.")
            return(invisible())
          }
          
          if (sum(ok) < 20L) {
            plot.new()
            box()
            title("Residual semivariogram")
            text(0.05, 0.8, adj = 0,
                 "Too few valid points\nfor stable semivariogram.")
            return(invisible())
          }
          
          sdat <- data.frame(lon = coords_ok[,1], lat = coords_ok[,2], res = res[ok])
          sp::coordinates(sdat) <- ~ lon + lat
          
          plot(gstat::variogram(res ~ 1, sdat), main = "Residual semivariogram", col = colour_pt, cex = 16)
      })
  
    }


  ### Pearson dispersion text panel ###
  # note: for standard lm/Gaussian GLM with estimated σ², the Pearson dispersion is essentially the same as the residual variance estimate, so ≈1 by construction
  disp <- NA_real_

   df_resid <- tryCatch(stats::df.residual(mo), error = function(e) NA_real_) 
   if (!is.null(res) &&
      is.finite(df_resid) &&
      df_resid > 0) {
    disp <- sum(res^2, na.rm = TRUE) / df_resid
   }

   if (!is.na(disp)) {
    panels_add(local({
      d   <- disp
      function() {
        plot.new()
        box()
        title("Dispersion summary")
        usr <- par("usr")  # x1 x2 y1 y2
        x0  <- usr[1] + 0.06 * (usr[2] - usr[1])
        y0  <- 0.80
        text(x0, y0, adj = 0, labels = paste0("Family: ", fam_txt))
        text(x0, y0 - 0.20, adj = 0,
             labels = paste0("Pearson dispersion: ", signif(d, 3)))
        text(x0, y0 - 0.30, adj = 0, labels = "\u22481: ok")
        text(x0, y0 - 0.35, adj = 0, labels = ">>1 (e.g. > 5–10): overdispersion") # means remaining overdispersion, suggesting missing structure or misfit even after modeling NB
        text(x0, y0 - 0.40, adj = 0, labels = "<<1: underdispersion")         
      }
    }))
  }
  
  ### title ###
     if (!accumulate) {
      mc <- tryCatch({if (isS4(mo)) slot(mo, "call") else mo$call},
        error = function(e) mo$call
      )
      
      link_txt <- tryCatch(family(mo)$link,    error = \(e) NA_character_)
      disp_txt <- tryCatch({cl <- getCall(mo)
        if (!is.null(cl$dispformula)) {
          paste("dispersion:", paste(deparse(cl$dispformula), collapse = " "))
        } else { NULL}
      }, error = function(e) NULL)
      
      off_txt <- if (offset) {
        tryCatch({
          mf  <- stats::model.frame(mo)
          off <- stats::model.offset(mf)
          if (!is.null(off)) {
            cl <- getCall(mo)
            off_expr <- NULL
            if (!is.null(cl$formula)) {
              f_chr <- paste(deparse(cl$formula), collapse = " ")
              if (grepl("offset\\(", f_chr)) {
                off_expr <- sub(".*offset\\(([^)]*)\\).*", "\\1", f_chr)
              }
            }
            paste0("offset: ", if (!is.null(off_expr)) paste0(off_expr,')') else "present")
          } else { NULL}
        }, error = function(e) NULL)
      } else {NULL}

      main_txt <- tryCatch(
       paste0(
        name, " model check:\n",
        deparse(mc[[1L]]),
        "(", paste(deparse(mc[[2L]]), collapse = " "), ")\n", 
        if (!is.na(fam_txt)) paste0('family: ', fam_txt) else "", 
        if (!is.na(link_txt)) sprintf(" (%s link)", link_txt) else "", 
        if (!is.null(off_txt)) paste0("; ", off_txt) else "", 
        if (!is.null(disp_txt)) paste0("; ", disp_txt) else ""
       ),
       error = function(e)  paste0(name, " model check")
      )

      if (wrap_title) {
        title_text <- strwrap(main_txt, width = wrap_width) 
        mtext(paste(title_text, collapse = "\n"), side = 3, line = 1, cex = 0.5, outer = TRUE, col = 'red2')
        } else {
          mtext(main_txt, side = 3, line = 1, cex = 0.5, outer = TRUE, col = 'red2')
        }
      if (PNG) dev.off()
     } # option for accumulate not prepared
}

# 3a. load authors' data for HOLC comparison (uses code from the authors' 05_paper_1_analyses_R4.Rmd)
  holc <- read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/soc_dem_max_2022_03_12 17_31_11.csv')
                   , col_select = c(id : area_holc_km2
                                    , holc_tot_pop
                                    , msa_GEOID : msa_total_popE
                                    , msa_gini))
  birds_records <- 
    read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_sum_bird_obs_by_holc_id_1933_2022.csv')) |> 
    dplyr::mutate(id = str_remove(id, '_Aves_all_observations')) |> 
    dplyr::select(id, records = N_samples)
  
  birds_completeness <-
    read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/bird_completeness_HOLC_cities_2022_R1.csv')) |> 
    dplyr::select(id, completeness = Completeness) |> 
    tidylog::mutate(id = ifelse(id == 'VA_Roanoke_B2\\r\\n2_B_9289', 'VA_Roanoke_B2\r\n2_B_9289', id))
  
  birds_source <- 
    read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_col_code_by_holc_id_2000_2020.csv')) |> 
    dplyr::mutate(id = str_remove(id, '_Aves_all_observations')) |> 
    dplyr::select(id, records = N_samples, type = Collection_code) |> 
    pivot_wider(id_cols = id, names_from = type, values_from = records)
  
  clim <- read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/climatic_data_cities.csv')
                   , col_select = c(  -'...1'
                                    , mean_temp_c = mean_temp
                                    , mean_precip_mm = mean_precip)
                   )
  green <- read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/NDVI_unique_ID_updated.csv')) %>% 
        dplyr::mutate(ndvi = ifelse(is.na(ndvi), median(.$ndvi, na.rm = TRUE), ndvi)) |> # interpolated missing
        dplyr::select(id, ndvi)
        
  pad <- read_csv(here::here('Data/provided/NDVI_PAD_unique_ID.csv')#'original_paper/Data/Biodiv_Greeness_Social/NDVI_PAD_unique_ID.csv' 
                 , col_select = c(id, pct_pa = percent_pa))
  # combine
  comb <- 
    left_join(holc, birds_records, by = 'id') |> # adds bird biodiversity                
    left_join(birds_completeness, by = 'id') |> # adds bird completeness
    left_join(birds_source, by = 'id') |> # adds in sampling by source
    left_join(clim, by = 'city') |> # adds temp and precip
    left_join(green, by = 'id') |> # adds green 
    left_join(pad, by = 'id') |> # adds protected areas
    dplyr::mutate(
        pop_per_km           = ifelse(is.na(holc_tot_pop), 0, holc_tot_pop / area_holc_km2)
        , sampling_density     = records / area_holc_km2
        , sampling_density_log = log(sampling_density)
        , completeness_log     = log(completeness)) |> 
    relocate(sampling_density, sampling_density_log, completeness, completeness_log, 
            .before = completeness) 

  h = data.table(comb)
  h=h[!holc_grade%in%'E']

  # additional variables
  h[, sample_binary := ifelse(is.na(sampling_density_log), 0, 1)]

  h[holc_grade%in%'A', holc_grade_num:=1]
  h[holc_grade%in%'C', holc_grade_num:=2]
  h[holc_grade%in%'B', holc_grade_num:=3]
  h[holc_grade%in%'D', holc_grade_num:=4]

  # add lat lon
  latlon = fread(here::here('Data/MaPe/DAT_obs_year_polygon.csv'), select = c("id", "lat", "lon"))|> unique() # ❗ we extracted polygon lat lon from RData files of observations; 943 polygons were missing lat/lon (had no observations) and the authors' shape files (holc_ad_data.shp) is missing these polygons (contains only polygons where birds were observed); as cities contain many polygons with known lat/lon, we used the mean of available polygon lat/lon within city_state as a reasonable fallback for missing polygons in the same city. It preserves city-level spatial context.
  h = latlon[h, on = "id"] 

# 3b. load authors' temporal data
t = fread(here::here('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_trend_by_time_holc_id_1933_2022.csv')) #tt = fread('Data/from_script_04/R1_biodiv_trend_by_time_holc_id_1933_2022.csv')
# names(temporal_trend) <- c('Year','holc_grade','Type','holc_polygon_id', 'Sum')
names(t) <- c('year','holc_grade', 'Sum')

t = t[holc_grade != 'E'] #d = data.table(temporal_trend)
tt = t[, .(n_obs = sum(Sum)), by = list(year, holc_grade)]
tt = tt[order(holc_grade,year)]

# add area per holc grade (as the authors used two ways to calculate this, we test both, but then use only the (a) as that has the correct overal area per holc grade
  # a)
  holc_a <- fread(here::here('original_paper/Data/Biodiv_Greeness_Social/soc_dem_max_2022_03_12 17_31_11.csv'))

  holc_area_sum_a = holc_a[, list(sum_area_holc_km2 = sum(area_holc_km2)), holc_grade]
  holc_area_sum_a_dt = data.table(holc_area_sum_a)  
    # gives
      #  holc_grade: chr [1:4] "A" "B" "C" "D"
      #  sum_area_holc_km2  : num [1:4] 1279 2712 5179 3280

  # b) copy of the L54-65 of 04_R4_uneven_biodiversity_data_2023.R, with MaPe changed folder path
  holc_b <- suppressWarnings(sf::st_read(here::here('Data/provided/holc_ad_data.shp'), quiet = TRUE) %>% #MaPe changed folder path 
      sf::st_cast('POLYGON') %>% # IMPORTANT
      dplyr::filter(!sf::st_is_empty(.)) %>% 
      sf::st_make_valid(.) %>% 
      tibble::rowid_to_column() %>% 
      dplyr::mutate(  id = paste(state, city, holc_id, holc_grade, rowid, sep = '_')
                      , city_state = paste0(city, ', ', state)
                      , area_holc_km2 = as.double(sf::st_area(.) / 1e+6)) %>% 
      dplyr::select(id, state, city, holc_id, holc_grade, city_state, area_holc_km2) )

    # Calculate the total area of holc polygons
    holc_area_sum_b <-  holc_b %>% dplyr::select(city, holc_grade, area_holc_km2) %>% dplyr::group_by(holc_grade) %>% dplyr::summarise(sum_area_holc_km2 = sum(area_holc_km2)) %>% dplyr::filter(holc_grade != 'E')  %>% as_tibble() %>% dplyr::select(-geometry)    
    # gives
      #  holc_grade: chr [1:4] "A" "B" "C" "D"
      #  sum_area_holc_km2  : num [1:4] 1282 2948 4365 2689
   holc_area_sum_b_dt = data.table(holc_area_sum_b)  

tta = merge(tt,holc_area_sum_a_dt, all.x = TRUE)
ttb = merge(tt,holc_area_sum_b_dt, all.x = TRUE)

# sampling density
tta[, sampling_density := n_obs/sum_area_holc_km2]
ttb[, sampling_density := n_obs/sum_area_holc_km2]
tta[, sampling_density_b :=ttb$sampling_density]

#ggplot(tta, aes(x = sampling_density, y = sampling_density_b)) + 
#  geom_abline(slope = 1, intercept = 0, linetype = "dotted", col = 'red') + 
#  geom_point() + 
#  facet_wrap(~holc_grade) +
#  coord_equal(expand = FALSE)  

# adjust contrasts - ensure treatment coding with D as baseline
options(contrasts = c("contr.treatment", "contr.poly"))
tta[, holc_grade_D := factor(holc_grade, levels = c("D","B","C","A"))]
tt00 = tta[year >= 2000 & year <= 2020]
tt10 = tt00[year >= 2010]

# estimate 2000 - 2020 A/D disparity
#dispar = round((((ttb[year%in%c(2020) &  holc_grade%in%c('A'), sampling_density] / ttb[year%in%c(2020) &  holc_grade%in%c('D'), sampling_density])/(ttb[year%in%c(2000) &  holc_grade%in%c('A'), sampling_density]/ ttb[year%in%c(2000) &  holc_grade%in%c('D'), sampling_density]))-1)*100, 1) # gives 39.5

dispar = round((((tta[year%in%c(2020) &  holc_grade%in%c('A'), sampling_density] / tta[year%in%c(2020) &  holc_grade%in%c('D'), sampling_density])/(ttb[year%in%c(2000) &  holc_grade%in%c('A'), sampling_density]/ tta[year%in%c(2000) &  holc_grade%in%c('D'), sampling_density]))-1)*100, 1) # gives 39.8 while using correct area

# 4. load temporal data for year, HOLC grade, neighbourhood (polygon) generated by us (contains only A, B, C, D)
if(recreate_data==TRUE){
  source(here::here('R/rev_Dat_temporal_trend_v2.R'))
}else{
  d = fread(here::here('Data/MaPe/DAT_obs_year_polygon.csv'))
}
# Ensure treatment coding with D as baseline (IMPORTANT)
options(contrasts = c("contr.treatment", "contr.poly"))
d[, holc_grade_D := factor(holc_grade, levels = c("D","B","C","A"))]

# sampling density
d[, sampling_density:=sum_bird_obs/area_holc_km2]

# shorten the city-state names
d[, state_city := paste(state, substr(city, 1, 7), sep =', ')]
d[nchar(state_city)>11, state_city:=paste0(state_city, '.') ]
d = d[ order(state,city)]

# offset sampling density to allow for log of zero values
d[, sampling_density_shifted := sampling_density + 0.1]

# subset
d00 = d[year >= 2000 & year <= 2020]
d10 = d[year >= 2010 & year <= 2020] # save(file = 'Data/Dat_temporal-trend.Rdata', d00, d10)

# create median  and mean per year and HOLC
d_m = d[, .(sampling_density_med = median(sampling_density), sampling_density_mean = mean(sampling_density)), by = .(year, holc_grade)]
#ggplot(d_med, aes(x = sampling_density_med)) +  geom_density()

# aggregate temporal data per year and HOLC (as the authors have done)
dd = d[, .(sum_bird_obs = sum(sum_bird_obs)), by = .(year, holc_grade)] # we have initially used the sum_km2 = sum(area_holc_km2) as well, but because our data are missing neighbourhoods that had no ID to link them with neighbourhood area and hence has fewer sampled neighbourhoods and hence smaller area, for consistency with the authors' values, we use their overall area per HOLC grade.

dd = merge(dd,holc_area_sum_a_dt, all.x = TRUE)

# sampling density
dd[, sampling_density := sum_bird_obs/sum_area_holc_km2]

# disparity in ration (meaning of it unclear given the similarity in 2000)
dispar2 = round((((dd[year%in%c(2020) &  holc_grade%in%c('A'), sampling_density] / dd[year%in%c(2020) &  holc_grade%in%c('D'), sampling_density])/(dd[year%in%c(2000) &  holc_grade%in%c('A'), sampling_density]/
 dd[year%in%c(2000) &  holc_grade%in%c('D'), sampling_density]))-1)*100, 1) #Using the dataset per year, HOLC grade and neighbourhoods generated yet a different percentage.
#' 
#' ***
#' 
#' <br>
#' 
#' # Summary of the authors' paper
#' 
#' ## Research questions
#' The authors claim to have investigated: “after controlling for greenspace and climate… …(1) how does sampling effort of bird biodiversity [neighbourhood sampled (yes or no), sampling density and completeness] vary with socioeconomic conditions (historic housing segregation policies, such as HOLC-graded neighbourhoods), biophysical predictors across urban environments and by data source; (2) whether the least-sampled areas for bird biodiversity (biodiversity coldspots hereafter) are consistently located in regions that were segregated by redlining; and (3) whether when considering temporal trends, biodiversity sampling is becoming more even or uneven over time in the age of increasing digital and citizen-science data collection?” [@ellis-soto_historical_2023, main text, p. 1870]. 
#' 
#' ## Results
#' Authors found that A rated areas have more than double the sampling density of D rated areas (1,592 vs. 712 records per km²; descriptive statistics), and even the chance of being sampled at all differs: adjusted odds are 36% lower in D than A neighbourhoods (OR = 0.64; authors' Table 1). Relative to A, sampling-density odds are 37% lower in B (OR = 0.63), 63% lower in C (0.37), and 74% lower in D (0.26; authors' Table 1). Survey completeness was also highest in A-graded neighbourhoods (~65%) and lowest in D-graded areas (~59%; authors' Table 1), implying greater uncertainty about species assemblages where the historical disinvestment was the greatest. Finally, the disparity is not shrinking with the rise of digital citizen science but it is increasing. Authors calculated that the A:D ratio in cumulative sampling density rose from 2.021 in 2000 to 2.74 in 2020 - an increase of 35.6%, although it is unclear whether these represent descriptive statistics or model predictions. 
#' 
#' Based on their results, the authors then claim that "historically redlined neighbourhoods remain the most undersampled urban areas for bird biodiversity today, potentially impacting conservation priorities and propagating urban environmental inequities" (main text, p. 1870), and that this disparity in sampling “increased by 35.6% over the past 20 years” [@ellis-soto_historical_2023, abstract, p. 1869]. 
#'  
#' ## Data  
#' The authors obtained 12,297,506 georeferenced observations of birds (class Aves) occurring within 9,851 HOLC-defined neighbourhoods in 202 cities. Note that in the paper authors reported 195 cities because they accidentally considered the same cities those having the same-name but occurring in different-state as the same cities. The bird observations were retrieved from [Global Biodiversity Information Facility](https://www.gbif.org/), spanning observations collected after HOLC’s categories were mapped (1933 up to October 2022), with major contributions from two citizen science databases [eBird](https://ebird.org/home) and [iNaturalist](https://www.inaturalist.org/). Historical neighbourhood polygons came from the [Mapping Inequality project](https://dsl.richmond.edu/panorama/redlining/). The authors then created three key response variables: (1) Neighbourhood sampled at all (yes or no), (2) sampling density (neighbourhood's density of bird observations per km²), and (3) survey completeness (% of observed species richness relative to the expected species richness. 
#' 
#' The authors also extract the following present-day covariates for each neighborhood: population density (km²) derived by areal interpolation from the 2013–2017 American Community Survey [@goodchild1993], area covered by parks (%, based on PAD-US and as a proxy for shared open scape) [@usgs2021], a composite Landsat Normalized Difference Vegetation Index (2014–2019) built in [Google Earth Engine](https://earthengine.google.com/) (as a proxy for greenness). The authors also calculated mean annual precipitation and temperature for each city using monthly averages from 1979 to 2013 at 1-km² resolution [@karger2017].
#' 
#' ***
#' 
#' <br>
#' 
#' # Computation reproducibility
#' ## No initial computational reproducibility
#' We used the code and data provided by the authors [@ellis-soto_SI_2023] along with their original publication [@ellis-soto_historical_2023]. Their Zenodo repository contained:  (1) codes to download, clean and prepare the data, hereafter `Cleaning code` (`01_gbif_biodiversity_by_HOLC_singlepolygon_2023.R`; `02_MSA_CBG_Links_equity_singlepart.Rmd`; `03_sum_area_per_city_grade.R`; and partially `04_R4_uneven_biodiversity_data_2023.R` [parts 1-6]), as well as (2) codes to produce main results, hereafter `Analysis code` (`05_paper_1_analyses_R4.R`; partially `04_R4_uneven_biodiversity_data_2023.R` [parts 7-11]). However, the deposited data were insufficient to initially execute the codes (our Table S0).
#' 
#+ tS0, out.width = "60%"
knitr::include_graphics(here::here("Output", "Table_S0.png"))
#' 
#' ## No computational reproducibility from raw data
#' We were initially unable to run the provided Cleaning codes because of missing files. Upon the request from the Institute for Replication, the authors provided us with some of the requested data, specifically on HOLC-grading (directly accessible via [Google Drive](https://drive.google.com/drive/u/0/folders/1ka97OdGV-sU3MTL4NUFM7_fDoElmTyEd)), and provided us the link and instructions to download the mean annual precipitation and temperature from [Chelsa Climatology platform](https://chelsa-climate.org/).
#' 
#' Using the additional data and instructions, we still failed to regenerate the data because, alongside several minor issues, (a) the GBIF dataset reported in the paper (https://doi.org/10.15468/dd.ha9ksv) could not be accessed/recreated: the DOI of authors dataset returned “doi not found”; moreover, the authors’ Cleaning script requires GBIF credentials that were not provided, and our own credentials failed at authentication; and (b) the CHELSA climate inputs were inconsistent with authors values: we were typically not able to match names of most sites with those provided by authors and we found mismatched temperature/precipitations values for matched records. Fixing issues in data generation would take disproportionate effort with unclear results, and hence we focused on computational reproducibility using the `Analysis codes` and data provided by authors.
#' 
#' ## Partial computational reproducibility from 'Analysis code and data'  
#' Finding the relevant script to generate paper results and outputs was not always easy because scripts behind display items were not consistently labelled. However, when the authors provided some missing data files (`holc_ad_data.shp`; `NDVI_PAD_unique_ID.csv`), we fixed minor code issues (see our Table S1 for details), and used the deposited, already prepared data, we reproduced most of the main-text and supplementary outputs, including Fig. 2, Fig. 3, Fig. S1, Table S1 - S3.  
#' 
#' ***
#' 
#' <a name="T_S1">
#' **Table S1 | Summary of computation reproducibility of authors' claims.** </a>
tab_s1 = data.table(readxl::read_excel(here::here("Output", "Table_S1_comp_rep.xlsx")))
tab_s1$ID = NULL
tab_s1[, (names(tab_s1)) := lapply(.SD, function(x) replace(x, is.na(x), ""))]

colnames(tab_s1) <- c("Main text claims", "Display item", "Generated by", "Script lines", 'Replicated', 'Note')

tab_s1 %>%
  kableExtra::kbl(align = c("l", "l","l","l", "l", "l"), linesep = "") %>%
  kableExtra::kable_paper(c("striped", "condensed"), full_width = F, position = "left") %>%
  column_spec(1, width = "20em") %>%   # Main text info
  column_spec(2, width = "4em") %>%   # Display
  column_spec(2, width = "15em") %>%   # Generated by
  column_spec(4, width = "5em") %>%   # Script lines column
  column_spec(6, width = "10em") %>%   # Note column (increase as needed)
  kableExtra::scroll_box(width = "100%", height = "550px")
#'  
#' <br>
#' 
#' We could not reproduce Fig. 1 and the key claim about 35.6% increase in disparity between grade A and D over 20 years as the authors did not describe how they generated these outputs, nor did we find the code to do so.
#' 
#' We reproduced Table S2 only partially, specifically its first panel, mislabelled as log(sampling density) whereas non-transformed sampling density is shown (our Table [S2](#F_S2)). We could not reproduce the second panel (also labelled as log(sampling density)). Note that according to the Table S2 description, three panels were expected, for sampling, density of sampling and completeness of sampling, but it is unclear which sampling and completeness models should have been depicted. 
#' 
#' <br>
#' 
#' ***
#' 
#' <a name="T_S2">
#' **Table S2 | Replicated Table S2, with correct sub-panel label.** </a>  

samp_d_binary_holc_rirs = lme4::glmer(sample_binary ~ holc_grade + 
        (holc_grade | msa_NAME),
        data = h, 
        family = binomial(link = "logit"))

d_fe_rirs <- lme4::lmer(log(sampling_density) ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            (1 + holc_grade| msa_NAME), 
            data =  h[!is.na(sampling_density)]) 

c_fe_rirs <- lme4::lmer(completeness ~ holc_grade + 
    scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
    (1 + holc_grade| msa_NAME), 
    data =  h[!is.na(completeness)])

#tab_model(samp_d_binary_holc_rirs,d_fe_rirs, c_fe_rirs)
tab_model(d_fe_rirs, transform = 'exp')

#'  
#' <br>
#'  
#' ***
#' 
#' We initially could not fully reproduce (i) Table 1 because the best model was indicated to be shown, but the model behind the plot was not the best one according to AIC-selection found in the script, and (ii) Table S4 because the model description did not match the script and the underlying data were accidentally multiplied (our Table [S3](#T_S3a), see ‘d’ for output matching authors’ Table S4). (iii) The latter obscured the reproducibility also of Fig. 4 (see next). 
#' 
#' ## Coding error
#' The major coding error that we found in the authors' script preparing temporal trend data (`04_R4_uneven_biodiversity_data_2023.R`, section *[7] Plot temporal trends 1933-2022 and 2000-2020*) unintentionally multiplied the dataset (our Fig. [S1](#F_S1)). Hence, Fig. 4 and Table S4, and thus likely the 35.6% claim, did not depict the actual data (see our Fig. [S2](#F_S2) and Table [S3](#T_S3a)). 
#' 
#' Initially, we struggled to reproduce Fig. 4. The output that resembles Fig. 4 is located in two scripts, each yielding different results. 
#'
#' (a) According to the authors' [README](original_paper/Code/README_code.md), the code `05_paper_1_analyses_R4_check.Rmd` should contain all key analyses for the paper. We did find a script with the heading “6 trends”, which generates trend lines for all four HOLC categories, but instead for sampling density (authors' Fig. 4), it depicts the number of HOLC polygons that were sampled (our Fig. [R1](#F_R1) A). To complete the picture, we also plotted the trends for the number of sampling observations (our Fig. [R1](#F_R1) B).
#' 
#' ***
#'  
#+ f_r1_obs, fig.width=.75*20*0.393701,fig.height=.75*8*0.393701 

# Here we show the authors original code with our adjustments indicate by MaPe

# a) prepare panel A and B

suppressWarnings({ #MaPe added, as well as package specs below
  counts_grade_year <- 
    readr::read_csv(here::here('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_trend_by_time_holc_id_1933_2022.csv'),#MaPe changed folder path from read_csv('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_trend_by_time_holc_id_1933_2022.csv')
    show_col_types = FALSE   # MaPe added to suppress readr column spec message
    ) |>
    dplyr::filter(holc_grade != 'E') |> 
    dplyr::arrange(year, holc_grade) |> 
    dplyr::group_by(year, holc_grade) |>
    dplyr::count() |> 
    dplyr::summarise(cumsum = cumsum(n), .groups = "drop") # MaPe added ', .groups = "drop"' to suppress information messages
  })

tag_pos <- c(0.045, 0.94)   # x, y in panel [0..1]  

# MaPe added color mapping
r_f4_A =
counts_grade_year |>  
  dplyr::filter(year >= 2000 & year <= 2020) |> 
  ggplot(aes(year, cumsum, col = forcats::fct_rev(holc_grade))) +  #MaPe changed group (group=holc_grade) to col to label the holc_grade lines
  geom_point() + #MaPe added to aid visualisation
  geom_line(linewidth = 1) +
  scale_color_manual(values = holc_pal, guide = guide_legend(reverse = TRUE), name = 'HOLC grade') + #MaPe added for consistent coloring
  circle_at_tag() + 
  labs(y = '# of sampled HOLC polygons', tag='A') + #MaPe added
  theme_light() +  #MaPe added for consistency
  theme(legend.position = "right", plot.title = element_text(size = 10))+
  offset_tag #MaPe added 


# MaPe added # of observations per year and HOLC 
r_f4_B =
  ggplot(tt[year >= 2000 & year <= 2020], aes(year, n_obs, col = forcats::fct_rev(holc_grade))) +  #MaPe changed group (group=holc_grade) to col to label the holc_grade lines
  geom_point() + #MaPe added to aid visualisation
  geom_line(size = 1) +
  scale_color_manual(values = holc_pal, guide = guide_legend(reverse = TRUE), name = 'HOLC grade') + #MaPe added for consistent coloring
  circle_at_tag() + 
  labs(y = '# of observations', tag='B') + #MaPe added
  theme_light() +#MaPe added for consistency
  theme(legend.position = "right", plot.title = element_text(size = 10))+
  offset_tag #MaPe added 
    

(r_f4_A + r_f4_B) +
plot_layout(guides = "collect", axis_titles = 'collect') #; ggsave('Output/Fig_r4_count_v2.jpg', width= 20, height = 9, units ='cm')  

#' <a name="F_R1">
#' **Figure R1</a> | Change in # of sampled polygons (A) and # of observations (B) according to HOLC grade over time.** Each point represents the sum per year. Lines aid the visualisation by connecting the points. Colour indicates HOLC grade category.
#' 
#' ***
#' 
#' (b) We then found that the code `04_R4_uneven_biodiversity_data_2023.R` contains section *[7] Plot temporal trends 1933-2022 and 2000-2020*. The script was not initially running due to absolute folder assignments that were unintuitive regarding the location of the files. We searched for the required files (`R1_biodiv_col_code_by_holc_id_2000_2020.csv` and `R1_biodiv_trend_by_time_holc_id_1933_2022.csv`) among the folders provided by the authors and respecified the folder paths. In addition, the original script loaded packages with conflicting functions. To smoothly reproduce the code, we thus made a new script where we loaded only the relevant packages and only the relevant data. Then the script, thought to generate Fig. 4, run without issues. However, none of the outputs (our Fig. [S2](#F_S2)) resembles the one from the authors' Fig. 4.<br><br>We later found out that the complex coding, along with the conflicting R-packages multiplied the number of observations (our Fig. [S1](#F_S1) below) and only such data produced the authors' Fig. 4. Specifically, the convoluted chunk of code in `04_R4_uneven_biodiversity_data_2023.R` (L410-20) produces the correct dataset, only if `plyr` R-package is not loaded. When the `plyr` package is loaded in R, the code multiplies the number of observations so that summing of all observation per HOLC grade and year (cumsum_n_obs) gives about 7 times more observations than the actual number of observations and hence sampling densities per km² are high. By fixing the error, we regenerated temporal trends, including Fig. 4 (corresponding to Fig. [S2](#F_S2) C and D), in a way that reflects the underlying data. Note that in the initial Fig. 4, the plotted lines just connected the data points (our Fig. [S2](#F_S2) A), but fitting a line through the data  produces a more realistic picture (our Fig. [S2](#F_S2) B; albeit here using only locally estimated scatterplot smoothing, while using model predictions would be more fitting - see later).
#'  

#+ F_S1, fig.width=1.1*20*0.393701,fig.height=1.1*8*0.393701 

holc_area = copy(holc_area_sum_b)
setnames(holc_area, 'sum_area_holc_km2', 'area_sum')

# prepare the multiplied dataset (chunk of code from `04_R4_uneven_biodiversity_data_2023.R` (L410-20))
require(plyr) # the package causeing the multiplications
temporal_trend = read.table(here::here("original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_trend_by_time_holc_id_1933_2022.csv"),
                            header = TRUE,sep = ",")
# names(temporal_trend) <- c('Year','holc_grade','Type','holc_polygon_id', 'Sum')
names(temporal_trend) <- c('Year','holc_grade', 'Sum')
temporal_trend = temporal_trend %>% dplyr::filter(holc_grade != 'E')

temporal_all_data = ddply(temporal_trend, 'holc_grade', function(x){
  ddply(x, 'Year', function(z){
    
    data.frame(
      Year = unique(z$Year),
      holc_grade = unique(z$holc_grade),
      n_obs = sum(z$Sum,na.rm=T)
      #    n_obs_cum = cumsum(z$Sum)
    )
    
  })
})

tmpppp = temporal_all_data %>% group_by(holc_grade, Year) # %>% mutate(cumsum = cumsum(n_obs))

trend_A = tmpppp %>% filter(holc_grade == 'A') %>% plyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% plyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_B  = tmpppp %>% filter(holc_grade == 'B') %>% plyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% plyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_C  = tmpppp %>% filter(holc_grade == 'C') %>% plyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% plyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_D  = tmpppp %>% filter(holc_grade == 'D') %>% plyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% plyr::mutate(sampling_density = cumsum_n_obs /area_sum )

mul = data.table(rbind(trend_A,trend_B,trend_C,trend_D))

# load the correct dataset (note that for consistency we use the same code as above, but without the plyr package. Note, the code can be drastically simplified by removing the redundant parts and using data.table as we do in the above section [Code to load tools and data](Code to load tools and data).

detach("package:plyr", unload = TRUE, character.only = TRUE) #unload plyr package to recieve a correct picture

# authors' code (L412-20); note that the following code that multiplied the datase is unnecessary as tmppp already contains number of observations and hence only area needed to be joined
trend_A = tmpppp %>% filter(holc_grade == 'A') %>% dplyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% dplyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_B  = tmpppp %>% filter(holc_grade == 'B') %>% dplyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% dplyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_C  = tmpppp %>% filter(holc_grade == 'C') %>% dplyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% dplyr::mutate(sampling_density = cumsum_n_obs /area_sum )

trend_D  = tmpppp %>% filter(holc_grade == 'D') %>% dplyr::mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% dplyr::mutate(sampling_density = cumsum_n_obs /area_sum )

ok = data.table(rbind(trend_A,trend_B,trend_C,trend_D))

ok[, cumsum_n_obs_mul:=mul$cumsum_n_obs]
ok[, sampling_density_ok := cumsum_n_obs/area_sum]
ok[, sampling_density_mul := cumsum_n_obs_mul/area_sum]

# prepare for plotting
cor_a = 
ggplot(ok, aes(x = cumsum_n_obs, y = cumsum_n_obs_mul, col = holc_grade)) + 
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", col = 'red') +
  labs(x = "# of observations\n[true]", y = "# of observations\n[accidentally multiplied]")+
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  theme_light() +
  circle_at_tag() + labs(tag = "A") + offset_tag

cor_b = 
ggplot(ok, aes(x = sampling_density_ok, y = sampling_density_mul, col = holc_grade))  +
  geom_point() + 
  geom_abline(slope = 1, intercept = 0, linetype = "dotted", col = 'red') +
  labs(x = "Sampling density per km²\n[true]", y = "Sampling density per km²\n[based on accidentally multiplied observations]") +
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  theme_light() +
  circle_at_tag() + labs(tag = "B") + offset_tag

(cor_a | cor_b) + plot_layout(guides = "collect")#ggsave('Output/Fig_check_wrong_data.jpg', width= 23, height = 10, units ='cm') 

#' <a name="F_S1">
#' **Figure S1</a> | Relationship between the correct dataset and authors' accidentally multiplied one.** Each point represents # of observations (A) and sampling density in km² (B) per HOLC grade and year, point colour indicates HOLC grade. Dotted line indicates unity - same value for true and authors' data. 
#'  
#+ F_S2, fig.width=20/2.5,fig.height=15/2.5 
# b) 
# prepare panels A and B with the original figure 
r_f4_A =
ggplot(mul[Year >= 2000 & Year <= 2020], aes(x = Year, y = sampling_density), fill = holc_grade) +
geom_point(aes(color = holc_grade)) + # MaPe added to aid visualisation
geom_line(aes(color = holc_grade), size = 1, show.legend = FALSE) +
#stat_smooth(aes(color = holc_grade), size = 1) +
#coord_cartesian(ylim = c(0, 300)) + scale_y_continuous(breaks = seq(0, 300, by = 100)) + # MaPe added
scale_color_manual(values = holc_pal, name = 'HOLC grade') +
labs(title = "Multiplied # of observation", subtitle = 'Lines connect points', tag='A', y = 'Sampling density\n[bird observations per km²]') + # MaPe added subtitle
theme_light()+
#theme(legend.position = 'none') + # MaPe hashtagged out to aid visualisation
theme( plot.subtitle = element_text(size = 10, colour = "grey40"), plot.title = element_text(size = 10))

r_f4_B =
ggplot(mul[Year >= 2000 & Year <= 2020], aes(x = Year, y = sampling_density), fill = holc_grade) +
stat_smooth(aes(color = holc_grade), size = 1, show.legend = FALSE) +
geom_point(aes(color = holc_grade)) + 
#stat_smooth(aes(color = holc_grade), size = 1) +
#coord_cartesian(ylim = c(0, 300)) + scale_y_continuous(breaks = seq(0, 300, by = 100)) + # MaPe added
scale_color_manual(values = holc_pal, name = 'HOLC grade') +
labs(title = "", subtitle = 'Lines represent LOESS, shading its 95%CIs', tag='B', y = 'Sampling density\n [bird observations per 1km²]') + # MaPe added subtitle
theme_light()+
#theme(legend.position = 'none') + # MaPe hashtagged out to aid visualisation
theme(legend.position = 'none', plot.subtitle = element_text(size = 10, colour = "grey40"), plot.title = element_text(size = 10))

# prepare panels D and C with the corrected data

# copy of the L390-430 of 04_R4_uneven_biodiversity_data_2023.R, with changed folder path
# Load 1933-2022 data
temporal_trend = read.table(here::here('original_paper/Data/Biodiv_Greeness_Social/R1_biodiv_trend_by_time_holc_id_1933_2022.csv'), header= T,sep=',')
# names(temporal_trend) <- c('Year','holc_grade','Type','holc_polygon_id', 'Sum')
names(temporal_trend) <- c('Year','holc_grade', 'Sum')
temporal_trend = temporal_trend %>% dplyr::filter(holc_grade != 'E')
# MaPe hashtagged out as it needed other data, not relevant for the current case: sum(temporal_2000_2020$Sum,na.rm=T) / sum(temporal_trend$Sum,na.rm=T) # 77.8 % of biodiversity data collected in last 20 years ! 

temporal_all_data = plyr::ddply(temporal_trend, 'holc_grade', function(x){
  plyr::ddply(x, 'Year', function(z){
    data.frame(
      Year = unique(z$Year),
      holc_grade = unique(z$holc_grade),
      n_obs = sum(z$Sum,na.rm=T)
      #    n_obs_cum = cumsum(z$Sum)
    )
    
  })
})

tmpppp = temporal_all_data %>% group_by(holc_grade, Year) # %>% mutate(cumsum = cumsum(n_obs))

trend_A = tmpppp %>% filter(holc_grade == 'A') %>% mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% mutate(sampling_density = cumsum_n_obs /area_sum )

trend_B  = tmpppp %>% filter(holc_grade == 'B') %>% mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% mutate(sampling_density = cumsum_n_obs /area_sum )

trend_C  = tmpppp %>% filter(holc_grade == 'C') %>% mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% mutate(sampling_density = cumsum_n_obs /area_sum )

trend_D  = tmpppp %>% filter(holc_grade == 'D') %>% mutate(cumsum_n_obs = cumsum(n_obs)) %>% left_join(holc_area) %>% mutate(sampling_density = cumsum_n_obs /area_sum )

temporal_all_data = rbind(trend_A,trend_B,trend_C,trend_D) 

# Plot temporal trend: 2000-2020; same as in the script, but showing legend
r_f4_C =
temporal_all_data %>%
filter(Year >= 2000 & Year <= 2020) %>%
#filter(Year >= 2010 & Year <= 2019) %>%
#filter(Year <= 2020) %>%
ggplot(aes(x = Year, y = sampling_density), fill = holc_grade) +
geom_point(aes(color = holc_grade)) + # MaPe added to aid visualisation
geom_line(aes(color = holc_grade), size = 1, show.legend = FALSE) +
#stat_smooth(aes(color = holc_grade), size = 1) +
coord_cartesian(ylim = c(0, 300)) + scale_y_continuous(breaks = seq(0, 300, by = 100)) + # MaPe added
scale_color_manual(values = holc_pal, name = 'HOLC grade') +
labs(title = "Non-duplicated data", subtitle = 'Lines connect points', tag='C', y = 'Sampling density\n [bird observations per km²]') + # MaPe added subtitle
theme_light()+
#theme(legend.position = 'none') + # MaPe hashtagged out to aid visualisation
theme( plot.subtitle = element_text(size = 10, colour = "grey40"), plot.title = element_text(size = 10))

# ggsave('Output/Fig_r4_density.jpg', width= 15, height = 15, units ='cm') 

# MaPe - LOESS smoothed
r_f4_D =
temporal_all_data %>%
filter(Year >= 2000 & Year <= 2020) %>%
#filter(Year >= 2010 & Year <= 2019) %>%
#filter(Year <= 2020) %>%
ggplot(aes(x = Year, y = sampling_density), fill = holc_grade) +
stat_smooth(aes(color = holc_grade), size = 1, show.legend = FALSE) +
geom_point(aes(color = holc_grade)) + # MaPe added to aid visualisation
#geom_line(aes(color = holc_grade), size = 1) +
coord_cartesian(ylim = c(0, 300)) + scale_y_continuous(breaks = seq(0, 300, by = 100)) + # MaPe added
scale_color_manual(values = holc_pal, name = 'HOLC grade') +
labs(subtitle = 'Lines represent LOESS; shading its 95%CIs', tag='D', y = 'Sampling density\n [bird observations per km²]') + # MaPe added subtitle
theme_light()+
#theme(legend.position = 'none') + # MaPe hashtagged out to aid visualisation
theme( legend.position = 'none', plot.subtitle = element_text(size = 10, colour = "grey40"), plot.title = element_text(size = 10))
#g2; ggsave('Output/Fig_r4_density_LOESS.jpg', width= 15, height = 15, units ='cm') 

# COMBINE ALL
#(r_f4_C + r_f4_D) +
#plot_layout(guides = "collect", axis_titles = 'collect') 
#ggsave('Output/Fig_r4_AB.jpg', width= 20, height = 11, units ='cm')

#((r_f4_A + r_f4_B) / (r_f4_C + r_f4_D)) + plot_layout(guides = "collect", axis_titles = 'collect')

r_f4_A_ = r_f4_A +  theme(axis.title = element_blank(),  axis.text.x  = element_blank()) + circle_at_tag() + labs(tag = "A") + offset_tag
r_f4_B_ = r_f4_B + theme(axis.title = element_blank(),axis.text  = element_blank()) + circle_at_tag() + labs(tag = "B") + offset_tag
r_f4_C_ = r_f4_C + theme(axis.title = element_blank()) + circle_at_tag() + labs(tag = "C") + offset_tag
r_f4_D_ = r_f4_D + theme(axis.title = element_blank(), axis.text.y  = element_blank()) + circle_at_tag() + labs(tag = "D") + offset_tag 

pp = ((r_f4_A_ + r_f4_B_) / (r_f4_C_ + r_f4_D_))+ plot_layout(guides = "collect") #+ plot_layout(guides = "collect", axis_titles = 'collect')

ggdraw(xlim = c(-0.03, 1), ylim = c(-0.03, 1)) +
  draw_plot(pp)+
  draw_label("Year", x = 0.5, y = -0.03, vjust = -1, size = 11) +
  draw_label('Sampling density\n [bird observations per 1km²]', x = -0.04, y = 0.5, angle = 90, vjust = 1.5, size = 11)#; ggsave('Output/Fig_rFig4_abcd_circle.jpg', width= 20, height = 15, units ='cm') 

#' <a name="F_S2">
#' **Figure S2</a> | Change in sampling density between HOLC grades over time.** Each point represents sampling density per HOLC grade and year (sum of all observations divided by the total area for the given grade). Lines in the left panels connect the points, in the right panels represent local regression non-parametric smoothing and shaded areas its 95% confidence intervals. Top row represents the accidentaly multiplied observations used by the author's, bottom row the correct number of observations.  
#' 
#' ***
#' 
#' The corrected figures, we have generated, shows that (i) sampling for all categories was similar until ±2010, then (ii) the differences in sampling between categories likely increased, but later (iii) leveled off. Note however, that such differences are easier to see if we plot the sampling densities of two categories against each other (our Fig. [S3](#F_S3)) or relative and absolute difference (see section two on *Temporal trends* below). Also note that the plots (our Fig. [S3](#F_S3)) are senstitive to the aggregation metric.
#' 
#' ***
#'  
#+ F_S3, fig.width=21*0.393701,fig.height=14*0.393701
# all pairwise combinations you want
pairs <- data.table(
  x = c("D","D","D","C","C","B"),
  y = c("A","B","C","A","B","A")
)

# join to create plotting dataset
plotdat <- pairs[, .(x, y)][
  , merge(ttb[holc_grade==x], ttb[holc_grade==y], by="year", suffixes=c(".x",".y")), 
  by=.(x,y)
][, pair := paste0(y, " ~ ", x)]

# plot
ggplot(plotdat[year >= 2000 & year <= 2020], aes(sampling_density.x, sampling_density.y)) +
  geom_abline(slope=1, intercept=0, linetype="dotted", color="black") +
  geom_smooth(col = 'red') +
  geom_point(aes(col=year), show.legend = FALSE) +
  geom_text(aes(label=year), hjust=-0.2, size=2.5, check_overlap=TRUE) +
  scale_x_continuous(breaks=seq(0,300,50), expand = c(0,0)) +
  scale_y_continuous(breaks=seq(0,300,50), expand = c(0,0)) +
  coord_cartesian(xlim=c(0,300), ylim=c(0,300)) +  
  facet_wrap(~pair, scales="fixed") +
  labs(x="Sampling density for x [km²]", y="Sampling density for y [km²]")+
  theme_light()+
  theme(
    strip.background = element_blank(),       # remove grey panel background
    strip.text = element_text(color = "black") # make labels black
  ) #; ggsave('Output/Fig_r4_alternative_new.jpg', width= 21, height = 14, units ='cm')  

#' <a name="F_S3">
#' **Figure S3</a> | Change in sampling density between HOLC grades over time.** Each point represents sampling density per HOLC grade and year (sum of all observations per divided by the total a for the given grade) with the year indicated for some of those. Red lines with shaded area represent local regression non-parametric smoothing with its 95% confidence intervals. Dotted line indicates unity, i.e. point on the line highlights the same sampling density, above the line higher sampling for y and below the line higher sampling for x.
#'
#' ***
#'   
#' These figures highlight the effects discussed by the authors. The figures show that 2020 might be an off year (e.g. due to COVID-19-restrictions driven increase in human presence in urban parks), an increase around 2010 (e.g. due to introduction of smarthpones)and that disparity remained (did not increase much after). However, these results depend on the aggregation metric (our Fig. [4](#F_4)) and same as the author's intended Fig. 4, the aggregations do not account for spatial and temporal non-independence of data points and/or confounding factors. 
#' 
#' We also struggled with generating the results found in the authors' Table S4. The authors' scripts contained models where sampling density was not log-transformed, whereas the description of the authors' Table S4 indicated log-transformation (throughout log refers to ln). Using log-transformed sampling density and multiplied dataset did produce the outcome found in authors' Table S4 (see below Table [S3](T_S3a)). It is however unclear why the generalised additive model was used as no wave form was fitted (i.e. simple Gaussian model was used). 
#' 
#' <br>
#'    
#' <a name="T_S3a"> 
#' **Table S3a | Change in sampling density across time in relations to HOLC grade**</a>

m_density_mul = glm((sampling_density) ~ Year * holc_grade, data = mul[Year %in% c(2000:2020)])
m_density_ok = glm((sampling_density) ~ Year * holc_grade, data = ok[Year %in% c(2000:2020)])

if(recompute_diag){
m_ass(name = 'Table_S3a_multiplied_data', mo = m_density_mul, dat = mul[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  

m_ass(name = 'Table_S3a_correct_data', mo = m_density_ok, dat = ok[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  
}

tab_model(m_density_mul, m_density_ok,  auto.label = T, string.ci='95%CI', title = "Generalized linear model", dv.labels = c("Multiplied # of observations", "Actual # of observations")) # m = lm(log(sampling_density) ~ Year * holc_grade, data = ok[Year %in% c(2000:2020)]); summary(glht(m)) # gives even less clear relationship
#' ***
#' 
#' <br>
#' 
#' <a name="T_S3b"> 
#' **Table S3b | Change in sampling density across time in relations to HOLC grade**</a>

m_density_mul_ln = glm(log(sampling_density) ~ Year * holc_grade, data = mul[Year %in% c(2000:2020)])
m_density_ok_ln = glm(log(sampling_density) ~ Year * holc_grade, data = ok[Year %in% c(2000:2020)])

if(recompute_diag){
m_ass(name = 'Table_S3b_multiplied_data', mo = m_density_mul_ln, dat = mul[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  

m_ass(name = 'Table_S3b_correct_data', mo = m_density_ok_ln, dat = ok[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE) 
}

tab_model(m_density_mul_ln, m_density_ok_ln,  auto.label = T, string.ci='95%CI', title = "Generalized linear model on ln(sampling density)", dv.labels = c("Multiplied # of observations", "Actual # of observations"))
#' ***
#' 
#' <br>
#'  
#' <a name="T_S3c"> 
#' **Table S3c | Change in sampling density across time in relations to HOLC grade**</a>

gam_density_mul = gam(sampling_density ~ Year * holc_grade, data = mul[Year %in% c(2000:2020)])

gam_density_ok = gam(sampling_density ~ Year * holc_grade, data = ok[Year %in% c(2000:2020)])

if(recompute_diag){
m_ass(name = 'Table_S3c_multiplied_data', mo = gam_density_mul, dat = mul[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  

m_ass(name = 'Table_S3c_correct_data', mo = gam_density_ok, dat = ok[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)        
}

tab_model(gam_density_mul, gam_density_ok, auto.label = T, string.ci='95%CI', title = "Generalized additive model", dv.labels = c("Multiplied # of observations", "Actual # of observations"))
#' ***
#' 
#' <br>
#' 
#' <a name="T_S3d"> 
#' **Table S3d | Change in sampling density across time in relations to HOLC grade**</a>

gam_density_mul_ln = gam(log(sampling_density) ~ Year * holc_grade, data = mul[Year %in% c(2000:2020)])

gam_density_ok_ln = gam(log(sampling_density) ~ Year * holc_grade, data = ok[Year %in% c(2000:2020)])

if(recompute_diag){
m_ass(name = 'Table_S3d_multiplied_data', mo = gam_density_mul_ln, dat = mul[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  

m_ass(name = 'Table_S3d_correct_data', mo = gam_density_ok_ln, dat = ok[Year %in% c(2000:2020)], 
      categ = 'holc_grade',
      cont = c('Year'), 
      PNG = TRUE)  
}

tab_model(gam_density_mul_ln, gam_density_ok_ln, auto.label = T, string.ci='95%CI', title = "Generalized additive model on ln(sampling density)", dv.labels = c("Multiplied # of observations", "Actual # of observations")) #summary(glht(gam_density_ok_ln)) corrected p-values
#' 
#' Table S3d matches the authors' output in Table S4.
#' ***
#' 
#' <br>
#' 
#' Note, models with sampling density ln-transformed fit the data better (see [Model assumptions](#Model_ass)). 
#' 
#' <br>
#' 
#' ***
#' 
#' <br>
#' 
#' # Robustness reproducibility
#'   
#'   
#' ## 1. HOLC grade differences
#' The authors investigated how HOLC grades differ in bird sampling using three response variables:
#'    
#' (A) A neighbourhood sampled or not (binary outcome).  
#' (B) Sampling density (km²).    
#' (C) Completeness of sampling.  
#'    
#' Accordingly, the authors fitted three sets of models. However, the authors description in the Methods is ambiguous about (i) used variables, and (ii) the number of models fitted (i.e. whether various predictors were added one by one or at once). Moreover, the described model structures do not seem to always fit with those presented in the script `05_paper_1_analyses_R4.Rmd`. Specifically, the authors note “The simplest null model (intercept only) was fit and fixed effects for HOLC (primary variable of interest) were added. Next, the following terms were added: metropolitan statistical area (MSA) as a random intercept, HOLC grade as a random slope, and fixed effects for Normalized Difference Vegetation Index (NDVI), percent open space and population density as random slopes within HOLC-defined city for mean temperature and precipitation (climate) interaction.” [@ellis-soto_historical_2023, p. 1875]. This description does not clarify what variables were fitted as fixed or random slopes and whether the terms were added sequentially or into a single complex model. Importantly, the authors do not describe whether they fitted a different set of models for each response. Here, we thus assumed that they fitted the same model structure to all three response variables.    
#' <br>
#' **First**, in the below list we show our interpretation of the above model specification with fixed effects without parentheses and random effects enclosed in parentheses (the term in front of `|` represents random slope, the term after `|` representing random intercept, with explicitly nested terms connected by `/` and interactions by `:`:  
#' 
#' (1) ~ 1 (intercept only model)  
#' (2) ~ HOLC grade  
#' (3) ~ HOLC grade + (1 | metropolitan area)  
#' (4) ~ HOLC grade + (HOLC grade | metropolitan area)  
#' (5) ~ HOLC grade + other predictors + (HOLC grade | metropolitan area)  
#' (6) ~ HOLC grade + other predictors + (HOLC grade | metropolitan area) + (HOLC grade | metropolitan area) + (temperature : precipitation | city )  
#'  
#' Models (5) and (6) possibly represent multiple models where each predictor (NDVI, open space (%), population density (per km²), and perhaps mean temperature and precipitation) was added separately or sequentially one by one.  
#'  
#' Moreover, the script revealed that, e.g. for neighbourhoods sampled (binary response), three models seemed to have been fitted: model 3 and 4 described above and a complex model, not described in the Methods:  
#' 
#' 7. ~ HOLC grade + NDVI + open space + population density + (1 + HOLC grade + msa_gini | metropolitan area) + (1 + temperature : precipitation | city)  
#'  
#' This model resembles a model described in the Methods, i.e. model (6) above, but with essential nuance: two random slopes are fitted within the metropolitan area, one of which, `msa_gini`, is nowhere defined. We speculate that `msa_gini` represents the Gini index of household income inequality at the level of metropolitan area. Given that `msa_gini` has the same number of levels as the metropolitan area, `msa_gini` cannot be fitted as a random slope within the metropolitan area, i.e. the model is misspecified. The model sets for the other two responses suffer from similar issues (e.g. `msa_medhhincE` random slope is not defined in the main text and has lower number of unique values than the metropolitan area within which it was fitted; similarly, interaction of temperature and precipitation as random slope within city has only within city-specific values (combinations)).
#'  
#' The script contains 10-12 models for sampling density and 10 models for sampling completeness. The authors nowhere justify the need for specifying their complex random structures and comparing so many models [@whittingham2006; @forstmeier2011; @cade2015].
#' <br>
#' **Second**, the authors compared  models within each set (i.e. for each response) and selected the best one using Akaike information criterion (AIC) [@Akaike2025], a method that is the most robust only if the compared models have the same random effect structure [@verbyla2019]. However, the authors compared models differing in both fixed and random effects, a practice advised against [@zuur2009; @greven2010; @vaida2005].
#'  
#' We believe that the authors should have fitted one complex model that is controlled for all discussed variables as well as non-independence of data points. Their data with n around 9,000 neighbourhoods and clear data structure allows that. If in doubt, fitting a simpler model and comparing its estimates with the complex one would suffice. Such practice reduces the risk of reporting spurious effects [@forstmeier2011; @whittingham2006].  
#' <br>
#' **Finally**, the authors do not describe in their Methods, nor provide in their scripts, any checks of model assumptions. This is problematic as some of their models fit poorly the data, do not sufficiently control for non-independence of data points (spatial and/or temporal), or suffer from heteroscedasticity [@zuur2010; @ives2021; @knief2021].  
#' <br>
#' **Here**, we respecified the model set for each response, checked the model assumptions (and if needed respecified the model/s) and compared the model estimates. We include plausible models from the authors' set and included further two models either with HOLC grade as a single fixed effect (simple models) or including also all other continues control variables (discussed by the authors) as fixed effects (NDVI, open space (%), population density (per km²), mean temperature and mean precipitation; full model). For our simple and full models, we varied the random structure (i) to mirror the authors' logic, but using only discussed and meaningful variables (e.g. excluding random slopes with the same number of levels as the corresponding random intercept), and to show whether estimates and their clarity change if we (ii) account for non-independence of data points, and (iii) if needed model the heteroscedasticity. 
#' 
#' (1) ~ HOLC grade + (1 | metropolitan area)     
#' (2) ~ HOLC grade + (1 | city)  
#' (3) ~ HOLC grade + (1 + HOLC grade | metropolitan area)  
#' (4) ~ HOLC grade + (HOLC grade | city)  
#' (5) ~ HOLC grade + (HOLC grade numeric | city)    
#' (6) ~ HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)  
#' (7) ~ HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)  
#' (8) ~ HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)  
#' (9) ~ HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)  
#' 
#' The models with random intercept of `metropoly` (metropolitan area) indicate the authors' specified models, and those with random intercept of `city` our models. Note that we use the city (unique across the dataset), instead of the metropolitan area (as the city has more levels). Moreover, for the neighbourhood  sampled (yes/no) authors did not use the model (6) and hence we have not specified it. Moreover, for responses neighbourhood sampled (yes/no) and completeness of sampling, we improved the model fit by controlling for the neighbourhood area (natural-log(km²)), a biologically and statistically meaningful control because it directly affects the probability of detections.
#' 
#' We have also attempted a model with random intercepts of state, metropoly and city, but those models converged poorly (similar levels for metropoly and city). 
#'  
#' In the coming sections A, B, C for each response variable we compare the HOLC grade estimates from the various models.
#'  
#' <br> 
#' 
#' ### A. Sampled or not
#' 
#' ***
#'
#+ F_S4, fig.width = 25/2.5, fig.height = 8/2.5
  # ggplot(h, aes(x = area_holc_km2, y = pct_pa)) + geom_point() + stat_smooth() # correlation check
  # binary models

    # specified by authors (second one fails to converge)
    samp_d_binary_holc = lme4::glmer(sample_binary ~ holc_grade + 
        (1 | msa_NAME), 
        data = h, 
        family = binomial(link = "logit")) 

    samp_d_binary_holc_rirs = lme4::glmer(sample_binary ~ holc_grade + 
        (holc_grade | msa_NAME),
        data = h, 
        family = binomial(link = "logit"))

    # specified by us
    m0 = lme4::glmer(sample_binary ~ holc_grade + 
        (1 | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )       

    m1= lme4::glmer(sample_binary ~ holc_grade + 
        (holc_grade | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )
    m1b= lme4::glmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        (holc_grade | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )

    m2= lme4::glmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        (holc_grade_num | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )

    m1p= lme4::glmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (holc_grade | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )

    m2p= lme4::glmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (holc_grade_num | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5)))  

    m3p= lme4::glmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (1|state) + (holc_grade_num | city_state), 
        data = h, 
        family = binomial(link = "logit"),
        control = glmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5)))   

  # model ass
  if(recompute_diag){
  m_ass(name = 'Fig_S4', mo = samp_d_binary_holc, dat = h,  
      categ = 'holc_grade', show_binned = TRUE, PNG = TRUE) # Panels 1 and 4 together show systematic structure (not random scatter), implying that although the dispersion summary (~0.87) suggests no overdispersion, the mean–variance relationship isn’t fully captured—possibly due to missing covariates or inadequate functional form (e.g. nonlinear effect of holc_grade).

  m_ass(name = 'Fig_S4b', mo = m0, dat = h, 
        categ = 'holc_grade', show_binned = TRUE, PNG = TRUE)  

  m_ass(name = 'Fig_S4c', mo = samp_d_binary_holc_rirs, dat = h, 
        categ = 'holc_grade', show_binned = TRUE, PNG = TRUE) 

  m_ass(name = 'Fig_S4d', mo = m1, dat = h, 
        categ = 'holc_grade', show_binned = TRUE, PNG = TRUE) 

  m_ass(name = 'Fig_S4e', mo = m1b, dat = h, 
        categ = 'holc_grade', cont = 'area_holc_km2', trans = 'log', show_binned = TRUE, PNG = TRUE) 
        # spatial smoother (s(lon, lat, bs = "gp"))) in glmmTMB refit did not improve the residuals suggesting  that whatever clustering remains is weak or idiosyncratic rather than a systematic spatial process (random structure already absorbs most spatial correlation); 
        # library(spdep); nb <- knearneigh(coords, k = 4) |> knn2nb(); lw <- nb2listw(nb, style = "W"); moran.test(resid(m1b), lw); I ≈ −0.0095, p ≈ 0.96 → no positive spatial autocorrelation; if anything a tiny (practically irrelevant) negative signal. 
        #library(gstat); library(sp); h_sp <- h; coordinates(h_sp) <- ~lon + lat; vgm_mod <- variogram(resid(m1b) ~ 1, h_sp); plot(vgm_mod, main = "Residual semivariogram"); flat over distance; no sharp rise near 0, no clear sill/range structure = spatial clustering is not an issue

  m_ass(name = 'Fig_S4f', mo = m2, dat = h, 
        categ = 'holc_grade', cont = 'area_holc_km2', trans = 'log', show_binned = TRUE, PNG = TRUE) 

  m_ass(name = 'Fig_S4g', mo = m1p, dat = h, 
        categ = 'holc_grade', cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), trans = c('log','none','none','none','none','none'), show_binned = TRUE, PNG = TRUE) 

  m_ass(name = 'Fig_S4h', mo = m2p, dat = h, 
        categ = 'holc_grade',
        cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'),
        trans = c('log','none','none','none','none','none'), 
        show_binned = TRUE, PNG = TRUE)   

  m_ass(name = 'Fig_S4i', mo = m3p, dat = h, 
        categ = 'holc_grade', 
        cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'),
        trans = c('log','none','none','none','none','none'), 
        show_binned = TRUE, PNG = TRUE) 
  }

  # PLOT for logit
    # add models to a list
    models_A<- list(
    samp_d_binary_holc      = samp_d_binary_holc,
    samp_d_binary_holc_rirs = samp_d_binary_holc_rirs,
    m0       = m0,
    m1       = m1,
    m1b       = m1b,
    m2       = m2,
    m1p      = m1p,
    m2p      = m2p,
    m3p      = m3p
    )

    # model labels
    model_labels_A <- c(
    samp_d_binary_holc      = "HOLC grade + (1 | metropoly)",
    samp_d_binary_holc_rirs    = "HOLC grade + (1 + HOLC grade | metropoly)",
    m0       = "HOLC grade + (1 | city)",
    m1       = "HOLC grade + (HOLC grade | city)",
    m1b       = "HOLC grade + area km² + (HOLC grade | city)",
    m2       = "HOLC grade + area km² + (HOLC grade numeric | city)",
    m1p      = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    m2p      = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    m3p   = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # sort models
    models_A_order <- c(
        "HOLC grade + (1 | metropoly)",
        "HOLC grade + (1 | city)",
        "HOLC grade + (1 + HOLC grade | metropoly)",
        "HOLC grade + (HOLC grade | city)",
        "HOLC grade + area km² + (HOLC grade | city)",
        "HOLC grade + area km² + (HOLC grade numeric | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )


    # extract
    coef_df_A <- purrr::imap_dfr(models_A, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

    coef_df_A <- coef_df_A %>%
    dplyr::mutate(model_label = factor(model_labels_A[model], levels = models_A_order)) %>% data.table()

    # distinquish original models from our alternative ones
    coef_df_A <- coef_df_A %>%
    dplyr::mutate(
        model_group = ifelse(grepl("^samp", model), "Authors' original", "Our new"),
        model_label = model_labels_A[model]
    )

    coef_df_A <- coef_df_A %>%
    dplyr::mutate(model_label = factor(model_label, levels = models_A_order)) #fwrite(file = 'Data/freeze/Fig_1.csv',coef_df_A )

    red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
    blue_ =  "#46B8DAFF"

    # plot
    A1 = 
    ggplot(coef_df_A, aes(x = estimate, y = fct_rev(model_label))) +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
        color = model_group),
        position = position_dodge(width = 0.6), height = 0) +
    geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_wrap(~ holc_grade) +
    scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
    scale_fill_manual(values = c("white","#46B8DAFF")) +
    #scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
    theme_minimal(base_size = 8) +
    labs(
        x = "",
        y = "Model structure",
        color = NULL,
        fill = NULL,
        subtitle = 'logit scale; binomial model'
    ) +
    theme(
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        plot.subtitle = element_text(size = 7, colour = "grey40", margin = margin(b=-14))
        )#; ggsave('Output/Fig_r1_Sampled01_v2.jpg', width = 25, height = 5, units = 'cm')
  
  # gaussian
    # as specified abvoe by the authors 
    samp_m0_g = lmer(sample_binary ~ holc_grade + 
        (1 | msa_NAME), 
        data = h)  

    samp_m1_g= lmer(sample_binary ~ holc_grade + 
        (holc_grade | msa_NAME), 
        data = h,
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))) 

    # specified by us
    m0_g = lmer(sample_binary ~ holc_grade + 
        (1 | city_state), 
        data = h)

    m1_g= lmer(sample_binary ~ holc_grade + 
        (holc_grade | city_state), 
        data = h,
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))) 

    m1b_g= lmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        (holc_grade | city_state), 
        data = h,
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))) 

    m2_g= lmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        (holc_grade_num | city_state), 
        data = h, 
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5)))   

    m1p_g= lmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (holc_grade | city_state), 
        data = h,
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )   

    m2p_g= lmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (holc_grade_num | city_state), 
        data = h, 
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )
    
    m3p_g= lmer(sample_binary ~ holc_grade + 
        log(area_holc_km2) +        
        scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + scale(mean_temp_c)*scale(mean_precip_mm) + 
        (1|state) + (holc_grade_num | city_state), 
        data = h, 
        control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
        )   
  # model ass
  if(recompute_diag){
  m_ass(name = 'gaus_Fig_S4a', mo = samp_m0_g, dat = h,  
      categ = 'holc_grade',  PNG = TRUE) 

  m_ass(name = 'gaus_Fig_S4b', mo = m0_g, dat = h, 
        categ = 'holc_grade',  PNG = TRUE)  

  m_ass(name = 'gaus_Fig_S4c', mo = samp_m1_g, dat = h, 
        categ = 'holc_grade',  PNG = TRUE) 

  m_ass(name = 'gaus_Fig_S4d', mo = m1_g, dat = h, 
        categ = 'holc_grade', PNG = TRUE) 

  m_ass(name = 'gaus_Fig_S4e', mo = m1b_g, dat = h, 
        categ = 'holc_grade', cont = 'area_holc_km2', trans = 'log', 
        PNG = TRUE) 
        
  m_ass(name = 'gaus_Fig_S4f', mo = m2_g, dat = h, 
        categ = 'holc_grade', cont = 'area_holc_km2', trans = 'log', 
        PNG = TRUE) 

  m_ass(name = 'gaus_Fig_S4g', mo = m1p_g, dat = h, 
        categ = 'holc_grade', cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), trans = c('log','none','none','none','none','none'),  PNG = TRUE) 

  m_ass(name = 'gaus_Fig_S4h', mo = m2p_g, dat = h, 
        categ = 'holc_grade',
        cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'),
        trans = c('log','none','none','none','none','none'), 
        PNG = TRUE)   

  m_ass(name = 'gaus_Fig_S4i', mo = m3p_g, dat = h, 
        categ = 'holc_grade', 
        cont = c('area_holc_km2','ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'),
        trans = c('log','none','none','none','none','none'), 
      PNG = TRUE)
  }

  # PLOT for gaussian
    # add models to a list
    models_A_g<- list(
        samp_m0_g      = samp_m0_g,
        samp_m1_g = samp_m1_g,
        m0_g      = m0_g,
        m1_g      = m1_g,
        m1b_g      = m1b_g,
        m2_g       = m2_g,
        m1p_g     = m1p_g,
        m2p_g      = m2p_g,
        m3p_g      = m3p_g
        )

    # model labels
    model_labels_A_g <- c(
        samp_m0_g    = "HOLC grade + (1 | metropoly)",
        samp_m1_g    = "HOLC grade + (1 + HOLC grade | metropoly)",
        m0_g         = "HOLC grade + (1 | city)",
        m1_g         = "HOLC grade + (HOLC grade | city)",
        m1b_g        = "HOLC grade + area km² + (HOLC grade | city)",
        m2_g         = "HOLC grade + area km² + (HOLC grade numeric | city)",
        m1p_g        = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        m2p_g        = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        m3p_g        = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
        )

    # sort models
    models_A_order_g <- c(
        "HOLC grade + (1 | metropoly)",
        "HOLC grade + (1 | city)",
        "HOLC grade + (1 + HOLC grade | metropoly)",
        "HOLC grade + (HOLC grade | city)",
        "HOLC grade + area km² + (HOLC grade | city)",
        "HOLC grade + area km² + (HOLC grade numeric | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )


    # extract
    coef_df_A_g <- purrr::imap_dfr(models_A_g, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

    coef_df_A_g <- coef_df_A_g %>%
    dplyr::mutate(model_label = factor(model_labels_A_g[model], levels = models_A_order_g)) %>% data.table()

    # distinquish original models from our alternative ones
    coef_df_A_g <- coef_df_A_g %>%
    dplyr::mutate(
        model_group = ifelse(grepl("^samp", model), "Authors' original", "Our new"),
        model_label = model_labels_A_g[model]
    )

    coef_df_A_g <- coef_df_A_g %>%
    dplyr::mutate(model_label = factor(model_label, levels = models_A_order_g))#fwrite(file = 'Data/freeze/Fig_1_gaussian.csv',coef_df_A_g )

    red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
    blue_ =  "#46B8DAFF"

    # plot
    A2 = 
    ggplot(coef_df_A_g, aes(x = estimate, y = fct_rev(model_label))) +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
        color = model_group),
        position = position_dodge(width = 0.6), height = 0) +
    geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_wrap(~ holc_grade) +
    scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
    scale_fill_manual(values = c("white","#46B8DAFF")) +
    #scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
    theme_minimal(base_size = 8) +
    labs(
        x = "Estimates of 'sampled or not' relative to HOLC grade A",
        y = "Model structure",
        color = NULL,
        fill = NULL,
        subtitle = "original scale; Gaussian model"
    ) +
    theme(
        legend.position = "none",
        #legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        strip.text = element_blank(),
        plot.subtitle = element_text(size = 7, colour = "grey40", margin = margin(b=-2))
        )#; ggsave('Output/Fig_r1_Sampled01_v2.jpg', width = 25, height = 5, units = 'cm')
  
  # combine
  (A1 / A2) + plot_layout(axis_titles = "collect")#; ggsave('Output/Fig_S4.jpg', width = 25, height = 8, units = 'cm') 

#' <a name="F_S4">
#' **Figure S4</a> | Differences in estimated sampling presence between HOLC grades across modelling approaches.** Points represent fixed-effect contrasts for HOLC grades B-D (columns), relative to HOLC grade A, in the probability that a HOLC polygon was ever sampled (binary "sampled or not" outcome). Horizontal lines are 95% Wald confidence intervals, and the vertical dashed line indicates zero difference. Red open circles indicate models specified by the original authors; blue filled circles those specified by us. The y-axis lists model structure: terms outside parentheses are fixed effects (`*` denotes interactions), terms inside parentheses denote random effects (variables left of `|` are random slopes, variables right of `|` are random intercepts). The **top row** shows binomial logit-link models of sampling presence, the **bottom row** Gaussian identity-link models fitted to the same binary outcome on the original 0–1 scale. All models are based on n = `r nrow(h)` HOLC polygons (neighbourhoods).
#'
#' ***
#' 
#' Across all model structures (from simplest to full controls), our estimates (blue) consistently reproduce the expected negative relationship between HOLC grades (B–D vs. A) and the probability of being sampled (Fig. [S4](#F_S4)). Binomial and Gaussian results align closely, confirming robustness to link-function choice and distributional assumptions.
#'  
#' Adding only a neighbourhood-area control reduced the apparent A–D difference, indicating that part of the original HOLC effect reflected varying polygon sizes rather than true sampling bias. However, when we also included covariates (NDVI, protected area %, population density, climate), the A–D contrast strengthened again, suggesting that ecological context had masked some of the patterns in simpler models. Both the area control and covariates stabilize estimates while preserving their direction and approximate magnitude.
#'  
#' In the Gaussian models, the authors’ original results (red) appear less extreme (closer to zero), suggesting that their omission of control variables conflated spatial or ecological bias with HOLC grade category.
#'
#' <br> 
#' 
#' ### B. Sampling density
#' 
#' ***
#'
#+ F_S5, fig.width = 25/2.5, fig.height = 12/2.5
  
  # non zero data
    hB_ = h[!is.na(sampling_density)] # remove NAs (zeros)
    
    # authors'
    d_ri <- lme4::lmer(log(sampling_density) ~ holc_grade + 
                    (1 | msa_NAME), 
                    data = hB_)   

    d_rirs <- lme4::lmer(log(sampling_density) ~ holc_grade + 
                (1 + holc_grade|msa_NAME),
                data = hB_)    

    d_fe_rirs <- lme4::lmer(log(sampling_density) ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            (1 + holc_grade| msa_NAME), 
            data = hB_)          
    
    # us
    mB0 = lmer(log(sampling_density) ~  holc_grade + 
            (1 | city_state), 
            data = hB_
            ) 

    mB1 = lmer(log(sampling_density) ~ holc_grade + 
            (holc_grade | city_state), 
            data = hB_,
            control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
            )

    mB2 = lmer(log(sampling_density) ~ holc_grade + 
            (scale(holc_grade_num) | city_state), 
            data = hB_,
            control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
            )
    
    mB1p = lmer(log(sampling_density) ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (holc_grade | city_state), 
            data = hB_, 
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            )  

    mB2p = lmer(log(sampling_density) ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (scale(holc_grade_num) | city_state), 
            data = hB_, 
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            ) 
     
    mB3p = lmer(log(sampling_density) ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
             (1|state) + (holc_grade | city_state), 
            data = hB_, 
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            )         
    
    # model ass
    if(recompute_diag){
      m_ass(name = 'Fig_S5a', mo = d_ri, dat = hB_,  
            categ = 'holc_grade',  PNG = TRUE) 

      m_ass(name = 'Fig_S5b', mo = mB0, dat = hB_, 
            categ = 'holc_grade',  PNG = TRUE)  

      m_ass(name = 'Fig_S5c', mo = d_rirs, dat = hB_, 
            categ = 'holc_grade',  PNG = TRUE) 

      m_ass(name = 'Fig_S5d', mo = mB1, dat = hB_, 
            categ = 'holc_grade', PNG = TRUE) 

      m_ass(name = 'Fig_S5e', mo = mB2, dat = hB_, 
            categ = 'holc_grade', PNG = TRUE) 

      m_ass(name = 'Fig_S5f', mo = d_fe_rirs, dat = hB_, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km'), 
            PNG = TRUE) 
            
      m_ass(name = 'Fig_S5g', mo = mB1p, dat = hB_, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), 
            PNG = TRUE) 

      m_ass(name = 'Fig_S5h', mo = mB2p, dat = hB_, 
            categ = 'holc_grade', cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), 
            PNG = TRUE) 

      m_ass(name = 'Fig_S5i', mo = mB3p, dat = hB_, 
            categ = 'holc_grade',
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), 
            PNG = TRUE)
    }

    # add models to a list
    models_B <- list(
    d_ri      = d_ri,
    d_rirs    = d_rirs,
    d_fe_rirs = d_fe_rirs,
    mB0       = mB0,
    mB1       = mB1,
    mB2       = mB2,
    mB1p      = mB1p,
    mB2p      = mB2p,    
    mB3p      = mB3p
    )

    # labels 
    model_labels_B <- c(
    d_ri      = "HOLC grade + (1 | metropoly)", 
    d_rirs    = "HOLC grade + (1 + HOLC grade | metropoly)", 
    d_fe_rirs = "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
    mB0       = "HOLC grade + (1 | city)", 
    mB1       = "HOLC grade + (HOLC grade | city)", 
    mB2       = "HOLC grade + (HOLC grade numeric | city)",
    mB1p      = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    mB2p      = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    mB3p      = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # sort models
    models_B_order <- c(
        "HOLC grade + (1 | metropoly)",
        "HOLC grade + (1 | city)",
        "HOLC grade + (1 + HOLC grade | metropoly)",
        "HOLC grade + (HOLC grade | city)",
        "HOLC grade + (HOLC grade numeric | city)", 
        "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # extract
    coef_df_B <- purrr::imap_dfr(models_B, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

    coef_df_B <- coef_df_B %>%
    dplyr::mutate(model_label = factor(model_labels_B[model], levels = models_B_order)) %>% data.table()

    # distinquish original models from our alternative ones
    coef_df_B <- coef_df_B %>%
    dplyr::mutate(
        model_group = ifelse(grepl("^d_", model), "Authors' original", "Our new"),
        model_label = model_labels_B[model]
    )

    coef_df_B <- coef_df_B %>%
    dplyr::mutate(model_label = factor(model_label, levels = models_B_order))#fwrite(file = 'Data/freeze/Fig_2_non-zero.csv',coef_df_B )

    red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
    blue_ =  "#46B8DAFF"

    # plot
    B1 = 
    ggplot(coef_df_B, aes(x = estimate, y = fct_rev(model_label))) +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
        color = model_group),
        position = position_dodge(width = 0.6), height = 0) +
    geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_wrap(~ holc_grade) +
    scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
    scale_fill_manual(values = c("white","#46B8DAFF")) +
    #scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
    theme_minimal(base_size = 8) +
    labs(
        x = "Estimates of sampling density relative to HOLC grade A",
        y = "Model structure",
        color = NULL,
        fill = NULL,
        subtitle = 'non-zero data; log(sampling density) response; Gaussian family\n(n = 8,904)'
    ) +
    theme(
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        plot.subtitle = element_text(size = 6, colour = "grey40", margin = margin(b=-22))
        )#; ggsave('Output/Fig_r1_Sampled01_v2.jpg', width = 25, height = 5, units = 'cm')

  # including zero data (with a small offset to allow log)
    hB = copy(h)
    hB = hB[is.na(sampling_density), sampling_density := 0] # zeros were NAs, making them zeros 
    
    # offset function for log on zeros
    c_off <- function(x) {
        nz <- x[x > 0]
        if (!length(nz)) stop("all zeros")
        max(min(nz, na.rm=TRUE)/2, 1e-6)
        }
    
    hB = hB[, sd_ln := log(sampling_density + c_off(sampling_density))] # ~ offset of 0.1257409

    # authors'
    d_ri_a  <- lme4::lmer(sd_ln ~ holc_grade + 
                    (1 | msa_NAME), 
                    data = hB)

    d_rirs_a  <- lme4::lmer(sd_ln ~ holc_grade + 
                (1 + holc_grade|msa_NAME),
                data = hB)    

    d_fe_rirs_a  <- lme4::lmer(sd_ln ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            (1 + holc_grade| msa_NAME), 
            data = hB)          
    
    # us
    mB0_a = lmer(sd_ln ~  holc_grade + 
            (1 | city_state), 
            data = hB
            ) 

    mB1_a  = lmer(sd_ln ~ holc_grade + 
            (holc_grade | city_state), 
            data = hB,
            control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
            )

    mB2_a  = lmer(sd_ln ~ holc_grade + 
            (holc_grade_num | city_state), 
            data = hB,
            control = lmerControl(optimizer = "bobyqa",
                            optCtrl = list(maxfun = 2e5))
            )

    mB1p_a  = lmer(sd_ln ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (holc_grade | city_state), 
            data = hB, 
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            )  

    mB2p_a  = lmer(sd_ln ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (holc_grade_num | city_state), 
            data = hB, 
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            ) 

    mB3p_a  = lmer(sd_ln ~ holc_grade + 
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
             (1|state) + (holc_grade | city_state), 
            data = hB,
            control = lmerControl(optimizer = "bobyqa",
                                optCtrl = list(maxfun = 2e5))
            )  # Model showed a boundary (singular) fit due to limited within-city grade variation, meaning that effective RE structure is simpler; results were robust to using numeric HOLC grade as slope, which run wihtout issues and produces similar results. For consistency with the rest, we keep the current model.  
    
    # model ass
    if(recompute_diag){
      m_ass(name = 'a_Fig_S5a', mo = d_ri_a, dat = hB,  
            categ = 'holc_grade',  PNG = TRUE) 

      m_ass(name = 'a_Fig_S5b', mo = mB0_a, dat = hB, 
            categ = 'holc_grade',  PNG = TRUE)  

      m_ass(name = 'a_Fig_S5c', mo = d_rirs_a, dat = hB, 
            categ = 'holc_grade',  PNG = TRUE) 

      m_ass(name = 'a_Fig_S5d', mo = mB1_a, dat = hB, 
            categ = 'holc_grade', PNG = TRUE) 

      m_ass(name = 'a_Fig_S5e', mo = mB2_a, dat = hB, 
            categ = 'holc_grade', PNG = TRUE) 

      m_ass(name = 'a_Fig_S5f', mo = d_fe_rirs_a, dat = hB, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km'), 
            PNG = TRUE) 
            
      m_ass(name = 'a_Fig_S5g', mo = mB1p_a, dat = hB, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), 
            PNG = TRUE) 

      m_ass(name = 'a_Fig_S5h', mo = mB2p_a, dat = hB, 
            categ = 'holc_grade', cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), offset = TRUE,
            PNG = TRUE) 

      m_ass(name = 'a_Fig_S5i', mo = mB3p_a, dat = hB, 
            categ = 'holc_grade',
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm'), 
            PNG = TRUE)   
    }

    # add models to a list
    models_B_a  <- list(
    d_ri_a       = d_ri_a ,
    d_rirs_a     = d_rirs_a ,
    d_fe_rirs_a  = d_fe_rirs_a ,
    mB0_a       = mB0_a ,
    mB1_a        = mB1_a ,
    mB2_a        = mB2_a ,
    mB1p_a       = mB1p_a ,
    mB2p_a       = mB2p_a,    
    mB3p_a      = mB3p_a 
    )

    # labels 
    model_labels_B_a  <- c(
    d_ri_a      = "HOLC grade + (1 | metropoly)", 
    d_rirs_a     = "HOLC grade + (1 + HOLC grade | metropoly)", 
    d_fe_rirs_a  = "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
    mB0_a        = "HOLC grade + (1 | city)", 
    mB1_a        = "HOLC grade + (HOLC grade | city)", 
    mB2_a        = "HOLC grade + (HOLC grade numeric | city)",
    mB1p_a       = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    mB2p_a       = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    mB3p_a      = "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # sort models
    models_B_order_a  <- c(
        "HOLC grade + (1 | metropoly)",
        "HOLC grade + (1 | city)",
        "HOLC grade + (1 + HOLC grade | metropoly)",
        "HOLC grade + (HOLC grade | city)",
        "HOLC grade + (HOLC grade numeric | city)", 
        "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        "HOLC grade + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # extract
    coef_df_B_a  <- purrr::imap_dfr(models_B_a , ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

    coef_df_B_a  <- coef_df_B_a  %>%
    dplyr::mutate(model_label = factor(model_labels_B_a [model], levels = models_B_order_a )) %>% data.table()

    # distinquish original models from our alternative ones
    coef_df_B_a  <- coef_df_B_a  %>%
    dplyr::mutate(
        model_group = ifelse(grepl("^d_", model), "Authors' original", "Our new"),
        model_label = model_labels_B_a [model]
    )

    coef_df_B_a  <- coef_df_B_a  %>%
    dplyr::mutate(model_label = factor(model_label, levels = models_B_order_a ))#fwrite(file = 'Data/freeze/Fig_2_all.csv',coef_df_B )

    red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
    blue_ =  "#46B8DAFF"

    # plot
    B2 = 
    ggplot(coef_df_B_a , aes(x = estimate, y = fct_rev(model_label))) +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
        color = model_group),
        position = position_dodge(width = 0.6), height = 0) +
    geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_wrap(~ holc_grade) +
    scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
    scale_fill_manual(values = c("white","#46B8DAFF")) +
    #scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
    theme_minimal(base_size = 8) +
    labs(
        x = "Estimates of sampling density relative to HOLC grade A",
        y = "Model structure",
        color = NULL,
        fill = NULL,
        subtitle = 'all data; log(sampling density) response; Gaussian family\n(n = 9,847)'
    ) +
    theme(
        legend.position = "none",
        #legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        strip.text = element_text(colour = NA),  # reserves strip space
        strip.background = element_rect(fill = NA, colour = NA),
        plot.subtitle = element_text(size = 6, colour = "grey40", margin = margin(b=-22))
        )#; ggsave('Output/Fig_r1_Sampled01_v2.jpg', width = 25, height = 5, units = 'cm')

  # RATES
    hB[is.na(records), records := 0]
    
    # authors'
    d_ri_r  <- glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
                    (1 | msa_NAME), 
                    family = nbinom2(),
                    data = hB)

    d_rirs_r  <- glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
                (1 + holc_grade|msa_NAME), 
                family = nbinom2(),
                data = hB)    

    d_fe_rirs_r  <- glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            (1 + holc_grade| msa_NAME), 
            family = nbinom2(),
            data = hB)          
    
    # us
    mB0_r = glmmTMB(records ~  holc_grade + offset(log(area_holc_km2)) +
            (1 | city_state), 
            family = nbinom2(), 
            data = hB
            ) 

    mB1_r  = glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            (holc_grade | city_state), family = nbinom2(), 
            data = hB
            )

    mB2_r  = glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            (holc_grade_num | city_state), 
            family = nbinom2(), 
            data = hB
            )

    mB1p_r  = glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (holc_grade | city_state), 
            family = nbinom2(), 
            data = hB
            )  

    mB2p_r  = glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
            (holc_grade_num | city_state), 
            family = nbinom2(), 
            data = hB
            )

    mB3p_r  = glmmTMB(records ~ holc_grade + offset(log(area_holc_km2)) +
            scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
            scale(mean_temp_c)*scale(mean_precip_mm) + 
             (1|state) + (holc_grade | city_state),  
            family = nbinom2(),
            data = hB
            )  
    # model ass
    if(recompute_diag){
      m_ass(name = 'r_Fig_S5a', mo = d_ri_r, dat = hB,  
        categ = 'holc_grade', cont = 'area_holc_km2', offset = TRUE, show_binned = TRUE,      
        PNG = TRUE) 

      m_ass(name = 'r_Fig_S5b', mo = mB0_r, dat = hB, 
            categ = 'holc_grade', cont = 'area_holc_km2', offset = TRUE, show_binned = TRUE, PNG = TRUE)  

      m_ass(name = 'r_Fig_S5c', mo = d_rirs_r, dat = hB, 
            categ = 'holc_grade', cont = 'area_holc_km2', offset = TRUE, show_binned = TRUE, PNG = TRUE) 

      m_ass(name = 'r_Fig_S5d', mo = mB1_r, dat = hB, 
            categ = 'holc_grade', cont = 'area_holc_km2', offset = TRUE, show_binned = TRUE,  PNG = TRUE) 

      m_ass(name = 'r_Fig_S5e', mo = mB2_r, dat = hB, 
            categ = 'holc_grade', cont = 'area_holc_km2', offset = TRUE, show_binned = TRUE, PNG = TRUE) 

      m_ass(name = 'r_Fig_S5f', mo = d_fe_rirs_r, dat = hB, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km'), 
            offset = TRUE, show_binned = TRUE, PNG = TRUE) 
            
      m_ass(name = 'r_Fig_S5g', mo = mB1p_r, dat = hB, 
            categ = 'holc_grade', 
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'), 
            offset = TRUE, show_binned = TRUE, PNG = TRUE) 

      m_ass(name = 'r_Fig_S5h', mo = mB2p_r, dat = hB, 
            categ = 'holc_grade', cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'),
            offset = TRUE, show_binned = TRUE, PNG = TRUE) 

      m_ass(name = 'r_Fig_S5i', mo = mB3p_r, dat = hB, 
            categ = 'holc_grade',
            cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'), 
            offset = TRUE, show_binned = TRUE, PNG = TRUE) 
    }

    # add models to a list
    models_B_r  <- list(
    d_ri_r       = d_ri_r ,
    d_rirs_r     = d_rirs_r ,
    d_fe_rirs_r  = d_fe_rirs_r ,
    mB0_r       = mB0_r ,
    mB1_r        = mB1_r ,
    mB2_r        = mB2_r ,
    mB1p_r       = mB1p_r ,
    mB2p_r       = mB2p_r,    
    mB3p_r      = mB3p_r 
    )

    # labels 
    model_labels_B_r  <- c(
    d_ri_r       = "HOLC grade + log area km² offset + (1 | metropoly)", 
    d_rirs_r     = "HOLC grade + log area km² offset + (1 + HOLC grade | metropoly)", 
    d_fe_rirs_r  = "HOLC grade + log area km² offset + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
    mB0_r        = "HOLC grade + log area km² offset + (1 | city)", 
    mB1_r        = "HOLC grade + log area km² offset + (HOLC grade | city)", 
    mB2_r        = "HOLC grade + log area km² offset + (HOLC grade numeric | city)",
    mB1p_r       = "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    mB2p_r       = "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    mB3p_r       = "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # sort models
    models_B_order_r  <- c(
        "HOLC grade + log area km² offset + (1 | metropoly)",
        "HOLC grade + log area km² offset + (1 | city)",
        "HOLC grade + log area km² offset + (1 + HOLC grade | metropoly)",
        "HOLC grade + log area km² offset + (HOLC grade | city)",
        "HOLC grade + log area km² offset + (HOLC grade numeric | city)", 
        "HOLC grade + log area km² offset + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
        "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
        "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
        "HOLC grade + log area km² offset + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

    # extract 
    coef_df_B_r  <- purrr::imap_dfr(models_B_r , ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

    coef_df_B_r  <- coef_df_B_r  %>%
    dplyr::mutate(model_label = factor(model_labels_B_r [model], levels = models_B_order_r )) %>% data.table()

    # distinquish original models from our alternative ones
    coef_df_B_r  <- coef_df_B_r  %>%
    dplyr::mutate(
        model_group = ifelse(grepl("^d_", model), "Authors' original", "Our new"),
        model_label = model_labels_B_r [model]
    )

    coef_df_B_r <- coef_df_B_r  %>%
    dplyr::mutate(model_label = factor(model_label, levels = models_B_order_r )); #fwrite(file = 'Data/freeze/Fig_2_rates.csv',coef_df_B_r )

    red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
    blue_ =  "#46B8DAFF"

    # plot
    B3 = 
    ggplot(coef_df_B_r , aes(x = estimate, y = fct_rev(model_label))) +
    geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
        color = model_group),
        position = position_dodge(width = 0.6), height = 0) +
    geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_wrap(~ holc_grade) +
    scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
    scale_fill_manual(values = c("white","#46B8DAFF")) +
    #scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
    theme_minimal(base_size = 8) +
    labs(
        x = "Estimates of sampling density relative to HOLC grade A",
        y = "Model structure",
        color = NULL,
        fill = NULL,
        subtitle = 'all data; # of observations response with log(area km²) offset; negative binomial family\n(n = 9,847)'
    ) +
    theme(
        legend.position = "none",
        #legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        strip.text = element_text(colour = NA),  # reserves strip space
        strip.background = element_rect(fill = NA, colour = NA),
        plot.subtitle = element_text(size = 6, colour = "grey40", margin = margin(b=-22))
        )#; ggsave('Output/Fig_r1_Sampled01_v2.jpg', width = 25, height = 5, units = 'cm')
    
   # combine
    lims <- range(c(coef_df_B$conf.low, coef_df_B$conf.high,
                coef_df_B_a$conf.low, coef_df_B_a$conf.high,
                coef_df_B_r$conf.low, coef_df_B_r$conf.high), na.rm = TRUE)

    B1 = B1 + scale_x_continuous(limits = lims, breaks = pretty(lims, n = 4))
    B2 = B2 + scale_x_continuous(limits = lims, breaks = pretty(lims, n = 4))
    B3 = B3 + scale_x_continuous(limits = lims, breaks = pretty(lims, n = 4))

    (B1 / B2 / B3) + plot_layout(axis_titles = "collect") #ggsave('Output/Fig_S5.jpg', width = 25, height = 12, units = 'cm') 

#' <a name="F_S5">
#' **Figure S5</a> | Differences in estimated sampling density between HOLC grades across modelling approaches.** Points represent fixed-effect contrasts for HOLC grades B-D (columns), relative to HOLC grade A, in sampling density of bird observations (per km²), while horizontal lines are 95% Wald confidence intervals, expressed on the model’s link scale. The **top row** shows Gaussian identity-link models fitted to log-transformed sampling density using only polygons with non-zero sampling density; the **middle row** shows Gaussian identity-link models fitted to all polygons (including zeros) by adding a small data-derived constant (0.125) to the `sampling density` to allow log transformation (`log(sampling density + 0.125)`); the **bottom row** shows negative-binomial log-link models of bird observation counts with an `log(area (km²))` offset, modelling counts per unit area and thus representing differences in sampling density. Sample sizes for each row are given in the subtitles. The vertical dashed line indicates zero difference. Red open circles indicate models specified by the original authors; blue filled circles those specified by us. The y-axis lists model structure: terms outside parentheses are fixed effects (`*` denotes interactions), terms inside parentheses denote random effects (variables left of `|` are random slopes, variables right of `|` are random intercepts).  
#'
#' The original authors modelled log(sampling density) using Gaussian models, either excluding zeros or adding a small constant. Our modelling of both these variants reproduced the qualitative pattern of reduced sampling in lower HOLC grades, yet the absolute differences were sensitive to data transformation and model structure (Fig. [S5](#F_S5)). When re-analysed as count data with an offset for polygon area and a negative-binomial family, the direction of effects remained consistent, but estimates became slightly less extreme and more stable across model specifications. This stability reflects that the count-offset formulation properly accounts for unequal polygon areas, includes zeros without arbitrary transformation, and models overdispersion explicitly rather than absorbing it into residual variance.
#'  
#' Across all variants, the conservative random-effects structure (city_state) yielded comparable estimates while better controlling for local clustering of observations. Together, these results confirm that the apparent HOLC-grade pattern is robust in direction but that Gaussian log-rate models exaggerate its magnitude by ignoring area and count-process properties.
#' 
#' <br>
#'   
#' ### C. Completeness of sampling
#' 
#' ***
#' 
#+ F_S6, fig.width = 25/2.5, fig.height = 4/2.5

hC = h[!is.na(completeness)]

# specified by the authors
c_ri <- lme4::lmer(completeness ~ holc_grade + 
    (1 | msa_NAME), 
    data = hC)

c_rirs    <- lme4::lmer(completeness ~ holc_grade + 
    (1 + holc_grade | msa_NAME), 
    data = hC)

c_fe_rirs <- lme4::lmer(completeness ~ holc_grade + 
    scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
    (1 + holc_grade| msa_NAME), 
    data = hC) # the authors' winnder

# specified by us
mC0 = lmer(completeness ~  holc_grade + 
    (1 | city), 
    data = hC
    ) 

mC1= lmer(completeness ~ holc_grade + 
    (holc_grade | city), 
    data = hC)

mC1b= lmer(completeness ~ holc_grade + log(area_holc_km2) +
    (holc_grade | city), 
    data = hC)    

mC2= lmer(completeness ~ holc_grade + log(area_holc_km2) +
    (holc_grade_num | city), 
    data = hC)

mC1p= lmer(completeness ~ holc_grade + log(area_holc_km2) + 
    scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
    scale(mean_temp_c)*scale(mean_precip_mm) + 
    (holc_grade | city), 
    data = hC, 
    control = lmerControl(optimizer = "bobyqa",
                         optCtrl = list(maxfun = 2e5))
    )  

mC2p= lmer(completeness~ holc_grade + log(area_holc_km2) + 
    scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
    scale(mean_temp_c)*scale(mean_precip_mm) + 
    (holc_grade_num | city), 
    data = hC, 
    control = lmerControl(optimizer = "bobyqa",
                         optCtrl = list(maxfun = 2e5))
    )  

mC3p= lmer(completeness~ holc_grade + log(area_holc_km2) + 
    scale(ndvi) + scale(pct_pa) + scale(pop_per_km) + 
    scale(mean_temp_c)*scale(mean_precip_mm) + 
    (1 | state) + (holc_grade_num | city), 
    data = hC, 
    control = lmerControl(optimizer = "bobyqa",
                         optCtrl = list(maxfun = 2e5))
    )  

# model ass
if(recompute_diag){
m_ass(name = 'Fig_S6a', mo = c_ri, dat = hC,  
      categ = 'holc_grade',    
      PNG = TRUE) 

m_ass(name = 'Fig_S6b', mo = mC0, dat = hC, 
      categ = 'holc_grade', PNG = TRUE)  

m_ass(name = 'Fig_S6c', mo = c_rirs, dat = hC, 
      categ = 'holc_grade', cont = 'area_holc_km2', PNG = TRUE) 

m_ass(name = 'Fig_S6d', mo = mC1, dat = hC, 
      categ = 'holc_grade', cont = 'area_holc_km2', PNG = TRUE) 

m_ass(name = 'Fig_S6e', mo = mC2, dat = hC, 
      categ = 'holc_grade', cont = 'area_holc_km2',PNG = TRUE) 

m_ass(name = 'Fig_S6f', mo = c_fe_rirs, dat = hC, 
      categ = 'holc_grade', 
      cont = c('ndvi','pct_pa','pop_per_km'), 
      PNG = TRUE) 
      
m_ass(name = 'Fig_S6g', mo = mC1p, dat = hC, 
      categ = 'holc_grade', 
      cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'), 
      PNG = TRUE) 

m_ass(name = 'Fig_S6h', mo = mC2p, dat = hC, 
      categ = 'holc_grade', cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'), PNG = TRUE) 

m_ass(name = 'Fig_S6i', mo = mC3p, dat = hC, 
      categ = 'holc_grade',
      cont = c('ndvi','pct_pa','pop_per_km','mean_temp_c','mean_precip_mm','area_holc_km2'), 
      PNG = TRUE)  
}

# add models to a list
models_C <- list(
  c_ri      = c_ri,
  c_rirs    = c_rirs,
  c_fe_rirs = c_fe_rirs,
  mC0       = mC0,
  mC1       = mC1,
  mC1b       = mC1b,
  mC2       = mC2,
  mC1p      = mC1p,
  mC2p      = mC2p,
  mC3p      = mC3p
)

# labels 
model_labels_C <- c(
    c_ri      = "HOLC grade + (1 | metropoly)", 
    c_rirs    = "HOLC grade + (1 + HOLC grade | metropoly)", 
    c_fe_rirs = "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
    mC0       = "HOLC grade + (1 | city)", 
    mC1       = "HOLC grade + (HOLC grade | city)", 
    mC1b      = "HOLC grade + area km² + (HOLC grade | city)", 
    mC2       = "HOLC grade + area km² + (HOLC grade numeric | city)",
    mC1p      = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    mC2p      = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    mC3p      = "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
    )

# sort models
models_C_order <- c(
    "HOLC grade + (1 | metropoly)",
    "HOLC grade + (1 | city)",
    "HOLC grade + (1 + HOLC grade | metropoly)",
    "HOLC grade + (HOLC grade | city)",
    "HOLC grade + area km² + (HOLC grade | city)",
    "HOLC grade + area km² + (HOLC grade numeric | city)", 
    "HOLC grade + NDVI + protected area % + population density + (1 + HOLC grade | metropoly)",
    "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade | city)",
    "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (HOLC grade numeric | city)",
    "HOLC grade + area km² + NDVI + protected area % + population density + temperature * precipitation + (1 | state) + (HOLC grade | city)"
)

# extract
coef_df_C <- purrr::imap_dfr(models_C, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

coef_df_C <- coef_df_C %>%
dplyr::mutate(model_label = factor(model_labels_C[model], levels = models_C_order)) %>% data.table()

# distinquish original models from our alternative ones
coef_df_C <- coef_df_C %>%
dplyr::mutate(
    model_group = ifelse(grepl("^c_", model), "Authors' original", "Our new"),
    model_label = model_labels_C[model]
)

coef_df_C <- coef_df_C %>%
dplyr::mutate(model_label = factor(model_label, levels = models_C_order)); #fwrite(file = 'Data/freeze/Fig_3.csv',coef_df_C)

red_ = "#D43F3AFF" # ggsci::pal_locuszoom()(5)    
blue_ =  "#46B8DAFF"

# plot
ggplot(coef_df_C, aes(x = estimate, y = fct_rev(model_label))) +
geom_errorbar(aes(xmin = conf.low, xmax = conf.high, 
    color = model_group),
    position = position_dodge(width = 0.6), height = 0) +
geom_point(aes(color = model_group, fill = model_group), position = position_dodge(width = 0.6), size = 1.5, shape =21) +
geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
facet_wrap(~ holc_grade) +
scale_color_manual(values = c("#D43F3AFF","#46B8DAFF")) +
scale_fill_manual(values = c("white","#46B8DAFF")) +
#scale_shape_manual(values = c(21, 16), guide = "none") +  # shapes fixed, no shape legend
theme_minimal(base_size = 8) +
labs(
    x = "Estimates of sampling completeness relative to HOLC grade A",
    y = "Model structure",
    color = NULL,
    fill = NULL
) +
theme(
    legend.key.height = unit(0.25, "cm")  # reduce vertical spacing between items 
    #plot.subtitle = element_text(size = 7, colour = "grey40", margin = margin(b=-25))
    )#; ggsave('Output/Fig_S6.jpg', width = 25, height = 5, units = 'cm')

#' <a name="F_S6">
#' **Figure S6</a> | Differences in estimated sampling completeness between HOLC grades across modelling approaches.** Points represent fixed-effect contrasts for HOLC grades B-D (columns), relative to HOLC grade A, in completeness of sampling (index), while horizontal lines are 95% confidence intervals. The vertical dashed line indicates zero difference. Red open circles indicate models specified by the original authors; blue filled circles those specified by us. The y-axis lists model structure: terms outside parentheses are fixed effects (`*` denotes interactions), terms inside parentheses denote random effects (variables left of `|` are random slopes, variables right of `|` are random intercepts). All models are based on n = `r nrow(hC)` HOLC polygons (neighbourhoods).
#' 
#' ***
#' 
#' 
#' Our replication supports the same qualitative trends but quantifies a larger disparity for A-D, and demonstrates that robustness improves with model complexity and adequate random-effect specification (Fig. [S6](#F_S6)).
#'
#' ***
#'
#' <br>
#' 
#' ## 2. Temporal trends  
#' 
#' The authors' results on temporal trends contain three key outputs:
#'   
#' (i) Claim about 35.6% increase in relative disparity between HOLC grade A and D from 2000 to 2020.   
#' (ii) Visualisation of temporal trends for each HOLC grade (Fig. 4).   
#' (iii) Formal analyses of HOLC grade temporal trend differences (Table S4).    
#'
#' In what follows we provide alternative approaches to each.
#' 
#' ### i. & ii. Disparity claim and trend visualisation
#' 
#' We did not find the code generating the 35.6% claim (Abstract p. 1869, Results p. 1871), hence could only speculate how this was calculated. We suspect that the authors used the yearly sampling density (total observations per year divided by total HOLC-grade area; data behind our Fig. S2 C and D) as that is the only temporal data we found. 
#' 
#' Using the yearly sampling density metric and calculating the ratio between grade A and D (A/D) for 2000 and for 2020 generated a different result (`r dispar`%). This result seems to reflect the one (~40%) from the authors' Fig. 4 legend. Importantly, plotting such disparity reveals non-linear temporal trends, with zero or even opposite disparities (our Fig. [1](#F_1)). Moreover, the choice of metric matters because different metrics reflect different underlying sampling mechanisms.
#' 
#' Disparity between HOLC grades A and D can be decomposed into two mechanisms: (1) the probability that a polygon is sampled at all (“coverage”), and (2) the intensity of sampling within sampled polygons. Metrics based on means or totals (especially arithmetic mean sampling density and total observations per HOLC-grade area) combine these components and are therefore sensitive to extreme high-density events (‘hotspots’) and to city-specific variation. Geometric-mean sampling density isolates the typical polygon-year by down-weighing extreme values, whereas arithmetic means emphasise rare high-density polygons. Trimming extreme hotspots (top 0.5% of polygons) visualises these issues (our Fig. [1](#F_1)). 
#'  
#' Across 2000–2020, alternative metrics that represent different sampling mechanisms reveal a different picture from the original 35.6% claim (our Fig. [1](#F_1)). The proportion of polygons sampled shows a long-term decline in A–D disparity, while intensity among sampled polygons increases only modestly before stabilising after ~2015, far less than the increases implied by hotspot-sensitive metrics. In contrast, hotspot-sensitive metrics produce the largest apparent rise. Note that some heavily sampled neighbourhoods lack extensive green spaces or contain museums or universities; thus, ideally their validity should be verified, which was beyond the scope of our replication. After trimming extreme hotspots, the pronounced increase in mean-based disparities largely disappears, demonstrating that the originally reported 35.6% rise in disparity is not a stable temporal signal but an artefact of a metric disproportionately driven by a small subset of heavily sampled polygons (primarily after 2010) and by inter-annual sampling peaks, features that inflate total-area averages and occur irrespective of HOLC grade. Importantly, note the lack of absolute differences in sampling density of A and D grades until about 2010 Importantly, the raw sampling densities of A and D polygons were nearly indistinguishable until about 2010 (our Fig. [1](#F_1)). 
#'  
#' Such results question the use of arbitrary, two-point (2000 and 2020) comparisons. Including pre-2000 years reveals an even more complex temporal picture, although these early data are likely less reliable because online platforms were not yet in use and earlier observations may have been added retrospectively.
#' 
#+ F_1, fig.width = 31/2.5, fig.height = 25/2.5

# prepare authors' data
a = tta[holc_grade%in%c('A','D')]
aw <- a[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "sampling_density")]
aw[, dispar := 100*((A/D)-1)] #aw[, dispar := A/D]

# prepare our data
dd_ = dd[holc_grade%in%c('A','D')]
w <- dd_[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "sampling_density")]
w[, dispar := 100*((A/D)-1)] #w[, dispar := A/D]


# check whether our and author aggregation gives same answers it does
  g1a_ = ggplot(aw[year>1999 & year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = 'red') + labs(subtitle = "Author's aggregation; sum per year") + theme_light() 
  g1b_ = ggplot(aw, aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = 'red')+ labs(subtitle = "") + theme_light()
    #(g1a|g1b) + plot_layout(axis_titles = "collect")

  g2a_ = ggplot(w[year>1999 & year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = 'red')+ labs(subtitle = "Our aggregation; sum per year") + theme_light()
  g2b_ = ggplot(w, aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = 'red')+ labs(subtitle = "") + theme_light()
  #(g2a|g2b) + plot_layout(axis_titles = "collect")

#((g1a_|g1b_) / (g2a_|g2b_))+ plot_layout(axis_titles = "collect")

# PLOT

# TREND - year aggregates 
trim_to = quantile(d[sampling_density > 0 & year>1999 & year<2021, sampling_density],  probs = .995)
col_all = 'black'
col_trim = 'red'
# OVERALL DENSITY - sums/area - as authors 
sums <- d[holc_grade%in%c('A','D'), 
              .(sum_bird_obs = sum(sum_bird_obs)), 
              by = .(year, holc_grade)]
dens =  holc_area_sum_a_dt[sums, on = "holc_grade"] 
dens[, sampling_density := sum_bird_obs/sum_area_holc_km2]
dens_w <- dens[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "sampling_density")]

## relative
dens_w[, dispar := 100*((A/D)-1)]

g1a = ggplot(dens_w[year>1999 & year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5) + labs(subtitle = "Overall sampling density", y ='Disparity in A relative to D [%]') + theme_light()
g1b = ggplot(dens_w[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

x_pos_AD <- 2002

g1c = ggplot() + 
  geom_point(data = dens_w[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  geom_point(data = dens_w[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 
  annotate("text", x = x_pos_AD, y = Inf, 
           label = "A", 
           hjust = 1, vjust = 2.5, 
           color = holc_pal[1], size = 3.2) +
  annotate("text", x = x_pos_AD, y = Inf, 
           label = "B", 
           hjust = 1, vjust = 4, 
           color = holc_pal[4], size = 3.2) + 
  labs(subtitle = "", y = 'Raw aggregated values') + theme_light()

## absolute
dens_w[, diff := A - D]

g1a_ = ggplot(dens_w[year>1999 & year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light()
g1b_ = ggplot(dens_w[year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

#((g1a_|g1b_) / (g1a|g1b))+ plot_layout(axis_titles = "collect")

# MEAN DENSITY - sums/area - as authors 
## relative
d_mean = d[holc_grade%in%c('A','D'), 
              .(sampling_density_mean = mean(sampling_density)), 
              by = .(year, holc_grade)]
w_mean <- d_mean[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "sampling_density_mean")]
w_mean[, dispar := 100*((A/D)-1)]

d_mean_trim = d[sampling_density < trim_to & holc_grade%in%c('A','D'), 
              .(sampling_density_mean = mean(sampling_density)), 
              by = .(year, holc_grade)]
w_mean_trim <- d_mean_trim[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "sampling_density_mean")]
w_mean_trim[, dispar := 100*((A/D)-1)]

x_pos <- max(w_mean$year[w_mean$year < 2021]) - 0.1

g2a = ggplot(w_mean[year>1999 & year<2021], aes(x = year, y = dispar)) + 
  stat_smooth(col = col_all, lwd = 0.5)+ 
  stat_smooth(data =  w_mean_trim[year>1999 & year<2021], aes(x = year, y = dispar), 
    col = 'red', fill = 'red', lty = 3, lwd = 0.5)+ 
  geom_point() + 
  geom_point(data = w_mean_trim[year>1999 & year<2021], 
    aes(x = year, y = dispar), col = 'red', cex = 0.5) +  
  labs(subtitle = "Mean sampling density", y ='Disparity in A relative to D [%]') + 
  theme_light() +
  annotate("text", x = x_pos, y = Inf, 
           label = "All data", 
           hjust = 1, vjust = 2.5, 
           color = col_all, size = 3.2) +
  annotate("text", x = x_pos, y = Inf, 
           label = "Top 0.5% trimmed", 
           hjust = 1, vjust = 14, 
           color = "red", size = 3.2)

g2b = ggplot(w_mean[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

x_pos_leg <- 2000
y_pos_leg <- 300
g2c = ggplot() + 
  geom_point(data = w_mean[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  #stat_smooth(data = w_mean, aes(x = year, y = A), col = holc_pal[1], fill = holc_pal[1], lwd = 0.5)+ 
  geom_point(data = w_mean[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 
  #stat_smooth(data = w_mean, aes(x = year, y = D), col = holc_pal[4], fill = holc_pal[4], lwd = 0.5)+ 
  geom_point(data = w_mean_trim[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], cex = 0.25) + 
  geom_point(data = w_mean_trim[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], cex = 0.25) + 

  annotate("point", x = x_pos_leg, y = y_pos_leg, size = 1.5, alpha = 0.5) +
  annotate("text",  label = "All data used", 
          x = x_pos_leg+1, y = y_pos_leg, hjust = 0, size = 3.2) +
  
  annotate("point", x = x_pos_leg, y = y_pos_leg-35, size = 0.5, alpha = 0.5) +
  annotate("text",  label = "Top 0.5% trimmed", 
          x = x_pos_leg+1, y = y_pos_leg-30, hjust = 0, size = 3.2) +
  labs(subtitle = "", y = 'Raw aggregated values') +  theme_light()

## absolute
w_mean[, diff_density := A - D]
g2a_ = ggplot(w_mean[year>1999 & year<2021], aes(x = year, y = diff_density)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5) + labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light() 
g2b_ = ggplot(w_mean[year<2021], aes(x = year, y = diff_density)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light() 

# In early years the A–D contrast was ~100% but corresponded to only ~10 vs 20 observations (or tiny density differences), whereas in recent years the relative contrast is similar but absolute sampling is orders of magnitude higher

# GEOMETRIC MEAN
# mean(log(sampling_intensity)) for non zero data; reduces the influence of extreme high values (typical sampling density among sampled polygons, ignoring extreme outliers)
## relative
int_log <- d[sampling_density > 0,
             .(mean_log_density = mean(log(sampling_density))),
             by = .(year, holc_grade)]
int_log[, geomean_density := exp(mean_log_density)]
w_int_log <- int_log[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "geomean_density")]

w_int_log[, dispar := 100*((A/D)-1)]

int_log_trim <- d[sampling_density > 0 & sampling_density < trim_to,
             .(mean_log_density = mean(log(sampling_density))),
             by = .(year, holc_grade)]
int_log_trim[, geomean_density := exp(mean_log_density)]
w_int_log_trim <- int_log_trim[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "geomean_density")]

w_int_log_trim[, dispar := 100*((A/D)-1)] # not used ad nearly identical

g3a = ggplot(w_int_log[year>1999 & year<2021], aes(x = year, y = dispar)) + 
  stat_smooth(col = col_all, lwd = 0.5)+ 
  #stat_smooth(data =  w_int_log_trim[year>1999 & year<2021], aes(x = year, y = dispar), col = 'red', fill = 'red', lty = 3, lwd = 0.5)+ 
  geom_point() + 
  #geom_point(data = w_int_log_trim[year>1999 & year<2021], aes(x = year, y = dispar), col = 'red', cex = 0.5) +   
  labs(subtitle = "Geometric mean sampling density of sampled polygons", y ='Disparity in A relative to D [%]') + 
  theme_light() #TODO decide which heading to use "Relative geometric-mean sampling density (A/D)"

g3b = ggplot(w_int_log[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

g3c = ggplot() + 
  geom_point(data = w_int_log[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  geom_point(data = w_int_log[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 
  labs(subtitle = "", y = 'Raw aggregated values') +  theme_light()

## absolute
w_int_log[, diff_density := A - D]
g3a_ = ggplot(w_int_log[year>1999 & year<2021], aes(x = year, y = diff_density)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light() #TODO decide which heading to use "Relative geometric-mean sampling density (A/D)"
g3b_ = ggplot(w_int_log[year<2021], aes(x = year, y = diff_density)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

#((g6a|g6b) / (g6a_|g6b_))+ plot_layout(axis_titles = "collect")

# COVERAGE
coverage <- d[holc_grade%in%c('A','D'), 
              .(prop_sampled = mean(sampling_density > 0)), 
              by = .(year, holc_grade)]
         
w_coverage <- coverage[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "prop_sampled")]

## relative
w_coverage[, dispar := 100*((A/D)-1)]

g4a = ggplot(w_coverage[year>1999 & year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "Proportion of sampled polygons", y ='Disparity in A relative to D [%]') + theme_light() 
g4b = ggplot(w_coverage[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()
g4c = ggplot() + 
  geom_point(data = w_coverage[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  geom_point(data = w_coverage[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 
  labs(subtitle = "", y = 'Raw aggregated values') +  theme_light()

## absolute
w_coverage[, diff := A -D]

g4a_ = ggplot(w_coverage[year>1999 & year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light() 
g4b_ = ggplot(w_coverage[year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

# INTENSITY
intensity <- d[sampling_density > 0 & holc_grade%in%c('A','D'), 
               .(mean_density_nonzero = mean(sampling_density)), 
               by = .(year, holc_grade)]

w_intensity <- intensity[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "mean_density_nonzero")]

intensity_trim <- d[sampling_density > 0 & sampling_density < trim_to & holc_grade%in%c('A','D'), 
               .(mean_density_nonzero = mean(sampling_density)), 
               by = .(year, holc_grade)]

w_intensity_trim <- intensity_trim[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "mean_density_nonzero")]

## relative
w_intensity[, dispar := 100*((A/D)-1)]
w_intensity_trim[, dispar := 100*((A/D)-1)]

g5a = ggplot(w_intensity[year>1999 & year<2021], aes(x = year, y = dispar)) + 
  stat_smooth(col = col_all, lwd = 0.5)+ 
  stat_smooth(data =  w_intensity_trim[year>1999 & year<2021], aes(x = year, y = dispar), 
    col = 'red', fill = 'red', lty = 3, lwd = 0.5)+ 
  geom_point() + 
  geom_point(data = w_intensity_trim[year>1999 & year<2021], 
    aes(x = year, y = dispar), col = 'red', cex = 0.5) +  
  labs(subtitle = "Mean sampling density across sampled polygons", y ='Disparity in A relative to D [%]') + 
  theme_light() 
g5b = ggplot(w_intensity[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y ="") + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()
g5c = ggplot() + 
  geom_point(data = w_intensity[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  geom_point(data = w_intensity[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 

  geom_point(data = w_intensity_trim[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], cex = 0.25) + 
  geom_point(data = w_intensity_trim[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], cex = 0.25) + 

  labs(subtitle = "", y = 'Raw aggregated values') +  theme_light()

## absolute
w_intensity[, diff := A -D]

g5a_ = ggplot(w_intensity[year>1999 & year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light() 
g5b_ = ggplot(w_coverage[year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

# EFFECTIVE SAMPLING - proove or principle - not plotted (same as mean)
sum_eff <- merge(coverage, intensity, by = c("year","holc_grade"))
sum_eff[, effective_density := prop_sampled * mean_density_nonzero]

w_sum_eff <- sum_eff[order(year),
            data.table::dcast(.SD, year ~ holc_grade, value.var = "effective_density")]

## relative
w_sum_eff[, dispar := 100*((A/D)-1)]

g6a = ggplot(w_sum_eff[year>1999 & year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "Effective sampling density across sampled polygons", y ='Disparity in A relative to D [%]') + theme_light() 
g6b = ggplot(w_sum_eff[year<2021], aes(x = year, y = dispar)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()
g6c = ggplot() + 
  geom_point(data = w_sum_eff[year>1999 & year<2021], aes(x = year, y = A), col = holc_pal[1], alpha = 0.5) + 
  geom_point(data = w_sum_eff[year>1999 & year<2021], aes(x = year, y = D), col = holc_pal[4], alpha = 0.5) + 
  labs(subtitle = "", y = 'Raw aggregated values') +  theme_light()

## absolute
w_sum_eff[, diff := A -D]
g6a_ = ggplot(w_sum_eff[year>1999 & year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+  labs(subtitle = "", y = 'Absolute disparity A minus D [observations/km²]') + theme_light() 
g6b_ = ggplot(w_sum_eff[year<2021], aes(x = year, y = diff)) + geom_point() + stat_smooth(col = col_all, lwd = 0.5)+ labs(subtitle = "", y = '') + 
scale_x_continuous(breaks = seq(1940,2020, length.out = 5)) + theme_light()

# combine
left <- (
  g1a | g1b |
  g2a | g2b |
  g4a | g4b |
  g5a | g5b |
  g3a | g3b 
) + 
  plot_layout(ncol = 2, nrow = 5,
              axis_titles = "collect")  
middle <- (
  g1a_ | g1b_ |
  g2a_ | g2b_ |
  g4a_ | g4b_ |
  g5a_ | g5b_ |
  g3a_ | g3b_ 
) +
  plot_layout(ncol = 2,nrow = 5,
              axis_titles = "collect") 

right <- (
  g1c | 
  g2c |
  g4c | 
  g5c | 
  g3c 
) +
  plot_layout(ncol = 1,nrow = 5,
              axis_titles = "collect") 

Fig_1 = (left | plot_spacer() | middle | plot_spacer() | right) +
  plot_layout(
    widths      = c(1, 0.1, 1, 0.1, 0.4),
    axis_titles = "collect_x"   # or "collect" if you want y merged when possible
  )
Fig_1; #ggsave('Output/Fig_1.png', Fig_1, units = 'cm', width = 31, height = 25)

#' <a name="F_1">
#' **Figure 1</a> | Change in relative disparity in bird sampling density between HOLC grade A and D over time.** Each point represents relative percentage difference (two left columns), absolute difference (two middle columns) in sampling density of A given D (with D being a baseline) based on overall sampling density (i.e. sum of all A or D observation divided by the total area of A or D; **first row**), mean sampling density per HOLC grade and year (**second row**), proportion of sampled polygons (**third row**), mean sampling density across sampled polygons (i.e. excluding non-sampled ones; **fourth row**) and geometric mean in sampling density (**fifth row**). The right column shows the actual values for A and D HOLC grades.  Dots represent yearly values (for all data: large dots; for data with top 0.5% observations trimmed: small dots . Lines represent local regression non-parametric smoothing and shaded areas 95% confidence intervals. Colour in the right column indicates all data (black) or data with top 0.5% trimmed (red), in the left column the HOLC grade category (A in green, D in red). The **top row** represents the aggregation likely used by the authors to support their claim about 35.6% increase, the **other rows** the aggregation done by us. Note that the authors' dataset did not contain area per year and HOLC grade; hence, we were unable to compute the median sampling density for their dataset.
#' 
#' ***
#' 
#' Importantly, the HOLC grade trends are not only volatile across methods, but also across cities (our Extended Data Fig. [1](#F_E1) and [2](#F_E2)). To illustrate typical within-city sampling patterns, we classified cities as *A-skewed*, *D-skewed*, and *mixed* using the following data-driven procedure. The goal was to identify cities with clear and stable differences between HOLC grades while avoiding pathological cases dominated by single-year hotspots. For each city and year we computed the geometric mean sampling density for HOLC grades A and D separately. The geometric mean was used because it reflects the typical sampling density of polygons in a given year while down-weighting single-year outliers. For each city, we then calculated the median over years of the annual A/D ratio and used along with the temporal consistency to classify the cities:  
#' - **A-skewed:** median(A/D) > 1.5 and A > D in ≥70% of years.  
#' - **D-skewed:** median(A/D) < 2/3 and D > A in ≥70% of years.  
#' - **Mixed:** 0.75 ≤ median(A/D) ≤ 1.33 and at least one sign change (A overtakes D at least once or vice versa).  
#' - **Unclassified:** Cities not meeting any above stability criterion.  
#'
#' We then selected the representative examples for each category, having all years of data (Extended Data Fig. [2](#F_E2)). Note that plots of all cities are in our Extended Data Fig. [1](#F_E1). 
#'
#' ***
#'
#+ F_E1a, fig.width = 20/2.5, fig.height = 22/2.5
# n col for plotting
x = unique(d$state_city)
ncol = 10

## batch a
di = d[state_city%in%x[1:94]] 
tr_des = 
 ggplot(di, aes(x = year, y = sampling_density_shifted, col = holc_grade)) + 
    geom_jitter(size = 0.5, alpha = 0.2) + 
    stat_smooth(se = FALSE, na.rm = TRUE) + 
    facet_wrap(~state_city, ncol = ncol) + 
    coord_cartesian(xlim=c(2000, 2020), ylim=c(.1, 100000))+
    scale_y_log10(
        name   = "Sampling density [km²]",
        breaks = c(0.1, 1, 10, 100, 1000, 10000, 100000),
        minor_breaks = minor_breaks_log10,      # many minor lines
        labels = c("0", "1", "10", "100", "1 000", "10 000", "100 000")#labels = scales::label_number(drop0trailing = TRUE)
    ) +
    scale_x_continuous(breaks = c(2000, 2010, 2020), name = 'Year')  +
    scale_color_manual(values = holc_pal, name = 'HOLC grade') +
    labs(subtitle = 'Part a') + 
    theme_minimal(base_size = 8) +
    theme(
        plot.subtitle = element_text(size = 10, colour = "grey40"),
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        strip.background = element_blank()
    )  

# remove axis labels for every second panel
g_tr_des <- ggplotGrob(tr_des)  # view with #g_tr_des$layout$name 
g_tr_des <- gtable_filter_remove(g_tr_des, name = c("axis-b-2-10", "axis-b-4-10", "axis-b-6-9", "axis-b-8-9" , "axis-b-10-9"), trim = FALSE) 
grid.draw(g_tr_des)#; ggsave('Output/Fig_E1a.png', g_tr_des, units = 'cm', width = 20, height = 25)

#+ F_E1b, fig.width = 20/2.5, fig.height = 22/2.5
## batch b
dib = d[state_city%in%x[95:188]]  
tr_des_b = 
  ggplot(dib, aes(x = year, y = sampling_density_shifted, col = holc_grade)) + 
    geom_jitter(size = 0.5, alpha = 0.2) + 
    stat_smooth(se = FALSE, na.rm = TRUE) + 
    facet_wrap(~state_city, ncol = 10) + 
    coord_cartesian(xlim=c(2000, 2020), ylim=c(.1, 100000))+
    scale_y_log10(
        name   = "Sampling density [km²]",
        breaks = c(0.1, 1, 10, 100, 1000, 10000, 100000),
        minor_breaks = minor_breaks_log10,      # many minor lines
        labels = c("0", "1", "10", "100", "1 000", "10 000", "100 000") #labels = scales::label_number(drop0trailing = TRUE)
    ) +
    scale_x_continuous(breaks = c(2000, 2010, 2020), name = 'Year')  +
    scale_color_manual(values = holc_pal, name = 'HOLC grade') +
    labs(subtitle = 'Part b') + 
    theme_minimal(base_size = 8) +
    theme(
        plot.subtitle = element_text(size = 10, colour = "grey40"),
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        strip.background = element_blank()
    )  

# remove axis labels for every second panel
g_tr_des_b <- ggplotGrob(tr_des_b)  # view with #g_tr_des$layout$name =
g_tr_des_b <- gtable_filter_remove(g_tr_des_b, name = c("axis-b-2-10", "axis-b-4-10", "axis-b-6-9", "axis-b-8-9" , "axis-b-10-9"), trim = FALSE) 

grid.draw(g_tr_des_b)#;ggsave('Output/Fig_E1b.png', g_tr_des_b, units = 'cm', width = 20, height = 25)

#' <a name="F_E1">
#' **Extended Data Figure 1</a> | Change in HOLC grade sampling density per km² over time across cities.** Lines represent locally estimated scatterplot smoothing or predictions from generalised additive models (generated by the *stat_smooth* function from the *ggplo2* R-package). Line colour indicates the HOLC grade.
#' 
#' ***
#' 
#+ F_E2, fig.width = .75*20/2.5, fig.height = .75*12.5/2.5
# helper for geometric mean (ignores non-finite and non-positive values)
geo_mean <- function(x) {
  x <- x[is.finite(x) & x > 0]
  if (!length(x)) return(NA_real_)
  exp(mean(log(x)))
}

# 1) geometric mean sampling density per city-year-grade
city_ratio <- d10[, .(ratio_AD = 
                  ((geo_mean(sampling_density[holc_grade=="A"]) /
                    geo_mean(sampling_density[holc_grade=="D"])
                    ))),
                 by = .(state_city, year)
                 ]

city_ratio = city_ratio[!is.na(ratio_AD)& is.finite(ratio_AD)]

# 2) city-level summaries
city_summ <- city_ratio[order(state_city, year),
  {
    r <- ratio_AD

    # sign relative to 1 (A>D vs D>A)
    s <- ifelse(r > 1,  1L,
         ifelse(r < 1, -1L, 0L))
    s <- s[s != 0L]  # drop exact ties if any

    n_switch <- if (length(s) < 2L) 0L else sum(diff(s) != 0L)

    .(
      n_years    = .N,                             # # of usable years
      mean_ratio = mean(r, na.rm = TRUE),
      median_ratio = median(r, na.rm = TRUE),
      p_A_gt_D  = mean(r > 1, na.rm = TRUE),      # proportion of years A>D
      p_D_gt_A  = mean(r < 1, na.rm = TRUE),      # proportion of years D>A
      n_switch  = n_switch
    )
  },
  by = state_city
]

city_summ[, class := fifelse(
  n_years < 5,  
  "unclassified",  # < 5 usable years or doesn’t meet any of the below
  fifelse(
    mean_ratio > 1.3 & p_A_gt_D >= 0.7, "A-skewed", # mean(A/D) > 1.3 and A>D in ≥ 70% of years
    fifelse(
      mean_ratio < 1 / 1.3 & p_D_gt_A >= 0.7, "D-skewed", # A-skewed: mean(A/D) > 1.3 and A>D in ≥ 70% of years
      fifelse(
        n_switch >= 2 | (mean_ratio >= 1 / 1.3 & mean_ratio <= 1.3),
        "mixed", # at least 2 sign switches or 0.77 < mean(A/D) < 1.3
        "unclassified"
      )
    )
  )
)]
g_geo = 
ggplot(city_summ, aes(x = class, fill = class)) + geom_bar() + 
  coord_cartesian(ylim = c(0,90)) + 
  scale_fill_manual(values = c("#92BC6B", "#E47D67", 'grey30', 'grey60'), guide = 'none') +
  scale_y_continuous(breaks = seq(0,90, by = 30), expan = c(0.0)) + 
  labs(x ="Sampling density", y = "", subtitle = 'Based on geometric mean') + 
  theme_minimal(base_size = 8)
#ggsave('Output/within-city-skew.png', width = 8, height = 8, units = 'cm')


# ARITHMETIC mean alternative
# 1) geometric mean sampling density per city-year-grade
city_ratio_ari <- d10[ , .(ratio_AD = 
                  ((mean(sampling_density[holc_grade=="A"]) /
                    mean(sampling_density[holc_grade=="D"])
                    ))),
                 by = .(state_city, year)
                 ]

city_ratio_ari = city_ratio_ari[!is.na(ratio_AD)& is.finite(ratio_AD)]

# 2) city-level summaries
city_summ_ari <- city_ratio_ari[order(state_city, year),
  {
    r <- ratio_AD

    # sign relative to 1 (A>D vs D>A)
    s <- ifelse(r > 1,  1L,
         ifelse(r < 1, -1L, 0L))
    s <- s[s != 0L]  # drop exact ties if any

    n_switch <- if (length(s) < 2L) 0L else sum(diff(s) != 0L)

    .(
      n_years    = .N,                             # # of usable years
      mean_ratio = mean(r, na.rm = TRUE),
      median_ratio = median(r, na.rm = TRUE),
      p_A_gt_D  = mean(r > 1, na.rm = TRUE),      # proportion of years A>D
      p_D_gt_A  = mean(r < 1, na.rm = TRUE),      # proportion of years D>A
      n_switch  = n_switch
    )
  },
  by = state_city
]

city_summ_ari[, class := fifelse(
  n_years < 5,  
  "unclassified",  # < 5 usable years or doesn’t meet any of the below
  fifelse(
    mean_ratio > 1.3 & p_A_gt_D >= 0.7, "A-skewed", # mean(A/D) > 1.3 and A>D in ≥ 70% of years
    fifelse(
      mean_ratio < 1 / 1.3 & p_D_gt_A >= 0.7, "D-skewed", # A-skewed: mean(A/D) > 1.3 and A>D in ≥ 70% of years
      fifelse(
        n_switch >= 2 | (mean_ratio >= 1 / 1.3 & mean_ratio <= 1.3),
        "mixed", # at least 2 sign switches or 0.77 < mean(A/D) < 1.3
        "unclassified"
      )
    )
  )
)]

g_ari =
ggplot(city_summ_ari, aes(x = class, fill = class)) + geom_bar() + 
  coord_cartesian(ylim = c(0,90)) + 
  scale_fill_manual(values = c("#92BC6B", "#E47D67", 'grey30', 'grey60'), guide = 'none') +
  scale_y_continuous(breaks = seq(0,90, by = 30), expan = c(0.0)) + 
  labs(x ="Sampling density", y = "# of cities", subtitle = 'Based on arithmetic mean') + 
  theme_minimal(base_size = 8)
#ggsave('Output/within-city-skew_arithmetic.png', width = 8, height = 8, units = 'cm')

# COMBINE into 1st panel
city_summ_ari[, method := 'Based on arithmetic mean']
city_summ[, method := 'Based on geometric mean']
cc = rbind(city_summ_ari,city_summ )

g_cc =
ggplot(cc, aes(x = class, fill = class)) + geom_bar() + 
  scale_fill_manual(values = c("#92BC6B", "#E47D67", 'grey30', 'grey60'), guide = 'none') +
  facet_wrap(~method) + 
  labs(x ="Sampling density", y = "# of cities") + 
  theme_light()

# SECOND/THIRD panel
city_summ_ari[,ar_based := class]
city_summ_ari[,ari_mean_ratio := mean_ratio]

city_summ[,geo_based := class]
city_summ[,geo_mean_ratio := mean_ratio]

cit = merge(city_summ, city_summ_ari[,.(state_city, ar_based, ari_mean_ratio)])

g_comp = 
ggplot(cit, aes(x = ari_mean_ratio, y = geo_mean_ratio)) + 
  geom_abline(slope = 1, intercept = 0, lty = 3, col = 'red') + 
  geom_point(alpha = 0.5) +
  coord_cartesian(xlim = c(0.004, 1300), ylim = c(0.004, 1300)) + 
  scale_x_log10(breaks = c(0.1, 1, 10, 100, 1000), name = 'A/D ratio of arithmetic means', labels = scales::label_number(drop0trailing = TRUE), minor_breaks = minor_breaks_log10) + 
  scale_y_log10(breaks = c(0.1, 1, 10, 100, 1000), name = 'A/D ratio of geometric means', labels = scales::label_number(drop0trailing = TRUE), minor_breaks = minor_breaks_log10) + 
  theme_minimal(base_size = 8)

#ggsave('Output/mean_city_compar.png', width = 7.2, height = 7, units = 'cm')  # Urban sampling bias classification depends on the metric;   

# BOTTOM example row

#cit[n_years == 23 & median_ratio>1.5]
di_A = d[state_city%in% c("MI, Detroit")] 
ex_A = 
 ggplot(di_A, aes(x = year, y = sampling_density_shifted, col = holc_grade)) + 
    geom_jitter(size = 0.5, alpha = 0.2) + 
    stat_smooth(se = FALSE, na.rm = TRUE) + 
    coord_cartesian(xlim=c(2000, 2020), ylim=c(.1, 10000))+
    scale_y_log10(
        name   = "Sampling density [km²]",
        breaks = c(0.1, 1, 10, 100, 1000, 10000),
        minor_breaks = minor_breaks_log10,      # many minor lines
        labels = c("0", "1", "10", "100", "1 000", "10 000")#labels = scales::label_number(drop0trailing = TRUE)
    ) +
    scale_x_continuous(breaks = c(2000, 2010, 2020), name = 'Year')  +
    scale_color_manual(values = holc_pal, name = 'HOLC grade') +
    labs(subtitle = 'A-skewed') + 
    theme_minimal(base_size = 8) +
    theme(
        #plot.subtitle = element_text(size = 10, colour = "grey40"),
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        legend.position=c(.85,.8),
        strip.background = element_blank()
    )  

#cit[n_years == 23 & median_ratio<2/3]
di_D = d[state_city%in% c("TX, Galvest")] 
ex_D = 
 ggplot(di_D, aes(x = year, y = sampling_density_shifted, col = holc_grade)) + 
    geom_jitter(size = 0.5, alpha = 0.2) + 
    stat_smooth(se = FALSE, na.rm = TRUE) + 
    coord_cartesian(xlim=c(2000, 2020), ylim=c(.1, 10000))+
    scale_y_log10(
        name   = "",
        breaks = c(0.1, 1, 10, 100, 1000, 10000),
        minor_breaks = minor_breaks_log10,      # many minor lines
        labels = c("0", "1", "10", "100", "1 000", "10 000")#labels = scales::label_number(drop0trailing = TRUE)
    ) +
    scale_x_continuous(breaks = c(2000, 2010, 2020), name = 'Year')  +
    scale_color_manual(values = holc_pal, guide = 'none') +
    labs(subtitle = 'D-skewed') + 
    theme_minimal(base_size = 8) +
    theme(
        #plot.subtitle = element_text(size = 10, colour = "grey40"),
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        #legend.position=c(.85,.85),
        strip.background = element_blank()
    )  

#cit[n_years == 23 & median_ratio>.75 & median_ratio<1.33]
di_m = d[state_city%in% c("MD, Baltimo")] 
ex_mix = 
 ggplot(di_m, aes(x = year, y = sampling_density_shifted, col = holc_grade)) + 
    geom_jitter(size = 0.5, alpha = 0.2) + 
    stat_smooth(se = FALSE, na.rm = TRUE) + 
    coord_cartesian(xlim=c(2000, 2020), ylim=c(.1, 10000))+
    scale_y_log10(
        name   = "",
        breaks = c(0.1, 1, 10, 100, 1000, 10000),
        minor_breaks = minor_breaks_log10,      # many minor lines
        labels = c("0", "1", "10", "100", "1 000", "10 000")#labels = scales::label_number(drop0trailing = TRUE)
    ) +
    scale_x_continuous(breaks = c(2000, 2010, 2020), name = 'Year')  +
    scale_color_manual(values = holc_pal, guide = 'none') +
    labs(subtitle = 'Mixed') + 
    theme_minimal(base_size = 8) +
    theme(
        #plot.subtitle = element_text(size = 8, colour = "grey40"),
        legend.key.height = unit(0.25, "cm"),  # reduce vertical spacing between items 
        #legend.position=c(.85,.85),
        strip.background = element_blank()
    )  

# EXPORT
top = ((g_ari | g_geo) + plot_layout(axis_titles = "collect") ) | g_comp
bottom = (ex_A | ex_mix | ex_D) + plot_layout(axis_titles = "collect_x")  
#ggsave('Output/Fig_5.png', top / bottom,  width = 20, height = 12.5, units = 'cm')
top / bottom

#' <a name="F_E2">
#' **Extended Data Figure 2</a> | City-level variation in sampling density and HOLC-grade skew.** **Top row**, number of cities classified as A-skewed, D-skewed, mixed, or unclassified based on arithmetic-mean (left) or geometric-mean (middle) sampling density ratios between HOLC A and D polygons. Comparison of city-level A/D ratios from geometric vs arithmetic means (right); points above the 1:1 line indicate cities where arithmetic means underestimate relative sampling advantage of A over D (A-skewed classification; typically due to strong D-grade hotspots raising the arithmetic mean for D), while points below the line indicate cities where arithmetic means overestimate A-skew (typically due to strong A-grade hotspots raising the arithmetic mean for A). Arithmetic means are highly sensitive to rare but extreme polygons (“hotspots”), and therefore reflect occasional seasonal survey bursts (e.g.  localised campaigns such as Christmas Bird Count) rather than the persistent, long-term differences in sampling between HOLC grades. Geometric means minimise outliers and capture the “typical” polygon in the “typical” year. The wide scatter around the 1:1 line shows that no single aggregation metric yields a stable across-metric classification, because hotspot influence differs strongly between metrics, thus cities frequently flip between A-skewed, mixed, and D-skewed depending on whether hotspots are emphasised (arithmetic) or down-weighted (geometric). **Bottom row**, representative example cities: A-skewed city (left), where A-grade polygons are consistently sampled more densely than D-grade polygons, mixed city (middle) with no persistent ordering between A and D grades across years, and D-skewed city (right), where D-grade polygons receive higher sampling density. Panels show raw polygon-level sampling densities (points) with LOESS trends per HOLC grade (solid lines). Cities were selected using a data-driven procedure based on the geometric mean A/D ratio (see description above). These examples illustrate that within-city patterns vary substantially and help interpret the aggregate national-level A/D disparity trends shown in Fig. [1](#F_1)). The plots for each city are in our Extended Data Fig. [1](#F_E1). 
#'
#' ***
#' 
#' <br>
#' 
#' ### iii. Formal analyses of HOLC grade temporal trend differences

#' To formally investigate temporal differences among HOLC grades, authors fitted  yearly sampling density per HOLC grade (log-transformed) in a generalised linear model (essentially linear trend, as no non-liner temrporal structure was included). However, using sampling density as a response variable collapses area, sampling coverage, and sampling intensity into a single quantity, and hence does not control for differences in the amount of area sampled across grades. We thus re-analysed the authors’ year-by-HOLC-grade aggregates using a negative binomial generalised linear model of counts with log(area(km²)) offset (our Table [S4](#T_S4), Fig. [S7](#F_S7)).
#' 
#' 
#' <a name="T_S4"> 
#' **Table S4 | Change in sampling density across time in relations to HOLC grade based on negative binomial model with offset**</a>
sum_m_nb  <- MASS::glm.nb(
  n_obs ~ holc_grade_D * scale(year) + offset(log(sum_area_holc_km2)),
  tt00
)
sum_m_10_nb  <- MASS::glm.nb(
  n_obs ~ holc_grade_D * scale(year) + offset(log(sum_area_holc_km2)),
  tt10
)

if(recompute_diag){
m_ass(name = 'Table_S4_all_years', mo = sum_m_nb, dat = tt00, 
      categ = 'holc_grade_D',
      cont = c('year'), 
      PNG = TRUE)  

m_ass(name = 'Table_S4_2010-2020_years', mo = sum_m_10_nb, dat = tt10, 
      categ = 'holc_grade_D',
      cont = c('year'), 
      PNG = TRUE)  
}

tab_model(sum_m_nb, sum_m_10_nb, auto.label = T, string.ci='95%CI', title = "Negative binomial model on sampling observations with log(area) offset", dv.labels = c("2000 - 2020", "2010 - 2020"))
#' 
#' ***
#'
#+ F_S7, fig.width=15*0.393701,fig.height=10*0.393701 
# plot predictions 2000 - 2020
# newdata grid
newD <- CJ(year = 2000:2020, holc_grade_D = unique(tt00$holc_grade))
newD[, sum_area_holc_km2 := 1] # predictions are on a per-km² rate scale

# predictions + 95%CI on log scale
pr <- predict(sum_m_nb, newdata = newD, se.fit = TRUE,  type    = "link")
newD[, `:=`(fit_link = pr$fit, se_link = pr$se.fit)]
newD[, `:=`(
  lwr_link = fit_link - 1.96 * se_link, 
  upr_link = fit_link + 1.96 * se_link
)]
newD[, `:=`(
  rate     = exp(fit_link),              # expected rate per km²
  lwr_rate = exp(lwr_link),
  upr_rate = exp(upr_link)
  )]

# plot on log scale
fy1 =
ggplot(newD, aes(x = year, y = fit_link, colour = holc_grade_D, fill = holc_grade_D)) +
  geom_ribbon(aes(ymin = lwr_link, ymax = upr_link), alpha = 0.2, colour = NA) +
  geom_line(size = 1) +
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_fill_manual(values = holc_pal, name = 'HOLC grade') + 
  labs(y = "Predicted sampling density", x = 'Year', subtitle = 'Log-scale; 2000 - 2020') +
  theme_light() +
  theme(plot.subtitle = element_text(size = 10, colour = "grey40"),
        axis.title.x = element_blank(),
        axis.text.x  = element_blank()
        )

# plot: back-transform to original scale (rates)
fy2 =
ggplot(newD, aes(x = year, y = rate, colour = holc_grade_D, fill = holc_grade_D)) +
  geom_ribbon(aes(ymin = lwr_rate, ymax = upr_rate), alpha = 0.2, colour = NA) +
  geom_line(size = 1) +
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_fill_manual(values = holc_pal, name = 'HOLC grade') + 
  labs(y = "Predicted sampling density", x = 'Year', subtitle = 'Original scale [km²]') +
  theme_light() + 
  theme(legend.position = 'none',
        plot.subtitle = element_text(size = 10, colour = "grey40")
        )

# plot predictions 2010 - 2020
# newdata grid
newD2 <- CJ(year = 2010:2020, holc_grade_D = unique(tt10$holc_grade))
newD2[, sum_area_holc_km2 := 1] # predictions are on a per-km² rate scale

# predictions + 95%CI on log scale
pr2 <- predict(sum_m_10_nb, newdata = newD2, se.fit = TRUE,  type    = "link")
newD2[, `:=`(fit_link = pr2$fit, se_link = pr2$se.fit)]
newD2[, `:=`(
  lwr_link = fit_link - 1.96 * se_link, 
  upr_link = fit_link + 1.96 * se_link
)]
newD2[, `:=`(
  rate     = exp(fit_link),              # expected rate per km²
  lwr_rate = exp(lwr_link),
  upr_rate = exp(upr_link)
  )]

# plot on log scale
fy1_b =
ggplot(newD2, aes(x = year, y = fit_link, colour = holc_grade_D, fill = holc_grade_D)) +
  geom_ribbon(aes(ymin = lwr_link, ymax = upr_link), alpha = 0.2, colour = NA) +
  geom_line(size = 1) +
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_fill_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_x_continuous(breaks = seq(2010,2020, by =2)) + 
  labs(y = "Predicted sampling density", x = 'Year', subtitle = '2010-2020') +
  theme_light() +
  theme(plot.subtitle = element_text(size = 10, colour = "grey40"),
        axis.title.x = element_blank(),
        axis.text.x  = element_blank()
        )

# plot: back-transform to original scale (rates)
fy2_b =
ggplot(newD2, aes(x = year, y = rate, colour = holc_grade_D, fill = holc_grade_D)) +
  geom_ribbon(aes(ymin = lwr_rate, ymax = upr_rate), alpha = 0.2, colour = NA) +
  geom_line(size = 1) +
  scale_color_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_fill_manual(values = holc_pal, name = 'HOLC grade') + 
  scale_x_continuous(breaks = seq(2010,2020, by =2)) + 
  labs(y = "Predicted sampling density", x = 'Year', subtitle = '') +
  theme_light() + 
  theme(legend.position = 'none',
        plot.subtitle = element_text(size = 10, colour = "grey40")
        )

  (
  fy1 | fy1_b |
  fy2 | fy2_b 
) + 
  plot_layout(ncol = 2, nrow = 2,
              axis_titles = "collect", guides = "collect") #; ggsave('Output/Fig_Y.jpg', width= 12, height = 15, units ='cm')   

#' <a name="F_S7">
#' **Figure S7</a> | Predicted sampling density across HOLC grades over time.** Lines with shaded areas represent predictions of sampling density on the log scale (top) and on the original per-km² scale (bottom), from negative binomial models fit to all years (N = 84 observations) or to 2010-2020 (N = 44), each with log(area(km²)) offset. To compare the estimates with those from models fitted to the yearly data per each polygon see our Extended Data Fig. [3](#F_E3).
#'
#' ***
#' 
#' Note that when we limit the dataset to the post-2010 period when absolute sampling density differences are no longer close to zero (coinciding with the rise of smartphone-driven community science), the difference between the A and D slope is no longer statistically distinguishable (Table [S4](S4)).
#' 
#' Moreover, the above models use aggregates per year and HOLC grade (overall number of observations per HOLC grade divided by the overall area of a given HOLC grade), which, as we discussed, is problematic and the results volatile to the chosen aggregation metric (Fig. [1](#F_1)). Aggregations also limit the ability to control for spatial/temporal non-independence of data. Thus, using raw data allows controlling for deeper levels of spatial and temporal non-independence, as well as for confounding variables, which we do in the following section.
#' 
#' To account for non-independence of unique polygons and their data across years, we have created a dataset with the number of observations for each unique polygon and year (i.e. city-specific HOLC grades and sampling-polygon ids; n = `r comma(nrow(d))` polygon-year records; for raw data distribution see our Extended Data Fig. [1](#F_E1)).
#'
#' ***
#' 
#' We then specified negative binomial mixed-effect models with number of observations as a response, ln-transformed polygon area (km²) as an offset, and year (continuous) in interaction with HOLC grade (four-level factor) as predictors while controlling for non-independence of data points in the random effects. 
#' 
#' We specified five models varying in the random effects and compared their estimates for the fixed effect predictors:
#'   
#' (1) Random intercepts of state, city within state and unique sampling polygon id.  
#' (2) Explicitly nested random intecerpts of state, city, HOLC grade and unique sampling polygon id.  
#' (3) Same as (1), but with a random slope of year within the city. 
#' (4) Same as (1), but with a random slope of year within the polygon.  
#' (5) Random intercept of sampling polygon id, and random slope of year within nested random intercepts of state, city, HOLC grade. Same as (2), but with a random slope of year.     
#' 
#' Note that for the model (5) we attempted a model (2) with random slope of year, but such random structure was non-identifiable (may inflate certainty or bias estimation of fixed effects). We thus simplified the random effects to levels where slope variance is identifiable and does not distort fixed-effect estimates, i.e. current (5) model specification.
#' 
#' Moreover, to accommodate slight heteroskedasticity with respect to HOLC grade and city area, in some models we modelled dispersion as a function of these predictors (`dispformula = ~ holc_grade_D + log(area_holc_km2)`). However, outputs of such models were nearly identical to the original ones and hence we report only those.  
#' 
#'  
#' #### A. Contrasts
#+ F_E3, fig.width = 25/2.5, fig.height = 15/2.5

  # check distributions
  #ggplot(d, aes(sum_bird_obs))+geom_density() + scale_x_continuous(trans ='log')
  #ggplot(d, aes(sampling_density))+geom_density() + scale_x_continuous(trans ='log')
  #ggplot(d00, aes(sampling_density))+geom_density() + scale_x_continuous(trans ='log')

  # MODELS 2000-2020
  if(recreate_data==TRUE){
  # 0) model per holc grade
  sum_m_nb  <- MASS::glm.nb(n_obs ~ holc_grade_D*scale(year) + 
              offset(log(sum_area_holc_km2)), 
              tt00)

  # 1) model set per polygon
  maD_nb <- glmmTMB(
      sum_bird_obs ~ holc_grade_D * scale(year) +
        offset(log(area_holc_km2)) +
        (1|state) + (1|city_state) + (1|id),
      data = d00,
      family = nbinom2()
    )

  mbD_nb = glmmTMB(sum_bird_obs ~ holc_grade_D*scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state/city_state/holc_grade/id), 
              d00,
              family = nbinom2()
            )
  
  mas1D_nb = glmmTMB(sum_bird_obs ~ holc_grade_D*scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state) + (scale(year)|city_state) + (1|id), 
              d00,
              family = nbinom2()
              ) # spatial asymetry without dispformula, given ok remaining plots, is a cosmetic imbalance, not a structural failure; if dispformula = ~ holc_grade_D + scale(log(area_holc_km2)) spatial asymetry disapears: the dispersion parameter (θ) varies slightly by socioeconomic class and area size, so smaller or denser areas — where variance tends to be higher relative to mean — are no longer “forced” into the same variance structure as large areas, which removes the systematic asymmetry between large negative and small positive residuals. The residual distribution now looks balanced, dispersion ≈ 1, and all structural panels are clean — that’s a near-optimal model.

   mas2D_nb= glmmTMB(sum_bird_obs ~ holc_grade_D*scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state) + (1|city_state) + (scale(year)|id), 
              d00,
              family = nbinom2()
              )
  mbs1D_nb = glmmTMB(sum_bird_obs ~ holc_grade_D*scale(year) + 
              offset(log(area_holc_km2)) +
              (scale(year)|state/city_state/holc_grade) + (1|id),#(scale(year)|state/city_state/holc_grade/id),
              d00,
              family = nbinom2()
              )

  save(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2000-2020.Rdata'),
      sum_m_nb, maD_nb, mbD_nb, mas1D_nb, mas2D_nb, mbs1D_nb)
  
  }else{
  load(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2000-2020.Rdata'))
  }

  # model ass
  if(recompute_diag){
  m_ass(name = 'Fig_E3_&_S8_2000-2020_a', mo = sum_m_nb, 
  dat = tt00, offset = TRUE, 
  cont = c("year", "sum_area_holc_km2"), categ = 'holc_grade_D',
  show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2000-2020_b', mo = maD_nb, 
    dat = d00, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2000-2020_c', mo = mbD_nb, 
    dat = d00, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2000-2020_d', mo = mas1D_nb, 
    dat = d00, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE) # spatial asymetry without dispformula, given ok remaining plots, is a cosmetic imbalance, not a structural failure; if dispformula = ~ holc_grade_D + scale(log(area_holc_km2)) spatial asymetry disapears: the dispersion parameter (θ) varies slightly by socioeconomic class and area size, so smaller or denser areas — where variance tends to be higher relative to mean — are no longer “forced” into the same variance structure as large areas, which removes the systematic asymmetry between large negative and small positive residuals. The residual distribution now looks balanced, dispersion ≈ 1, and all structural panels are clean — that’s a near-optimal model.

  m_ass(name = 'Fig_E3_&_S8_2000-2020_e', mo = mas2D_nb, 
    dat = d00, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2000-2020_f', mo = mbs1D_nb, 
    dat = d00, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)
  }

  models_T0020_D_nb <- rlang::set_names(
    list(maD_nb, mbD_nb, mas1D_nb, mas2D_nb, mbs1D_nb),
    c("maD_nb","mbD_nb","mas1D_nb", "mas2D_nb", "mbs1D_nb")
  )
  # labels
  models_T0020_labels_D_nb <- c(
    maD_nb      = "(1 | state) + (1 | city) + (1 | polygon)",
    mbD_nb      = "(1 | state / city / HOLC grade / polygon)",
    mas1D_nb    = "(1 | state) + (year | city) + (1 | polygon)",    
    mas2D_nb    = "(1 | state) + (1 | city) + (year | polygon)",
    mbs1D_nb    = "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # sort models
  models_T0020_order_D_nb <- c(
    "(1 | state) + (1 | city) + (1 | polygon)",
    "(1 | state / city / HOLC grade / polygon)",
    "(1 | state) + (year | city) + (1 | polygon)",
    "(1 | state) + (1 | city) + (year | polygon)",
    "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # 2) Extract fixed effects on the modeling scale (ln), fast Wald CIs
  # lm
  lm_df_nb <- ext_fixef_D_lm(sum_m_nb) |>
    mutate(
      type2 = fcase(type == "intercept", "Intercept",
                    type == "slope_per_SDyear", "Slope",
                    default = as.character(type)),
      holc_grade_dif = paste0(holc_grade, " vs D")  # match tr1D y labels
    ); #fwrite(file = 'Data/freeze/Fig_6_linear-models.csv',lm_df_nb) 

  lm_lab_nb <- paste0(
    sprintf("Negative binomial model\non bird observations per year\n(n = %s for 2000-2020)", nobs(sum_m_nb)), "\n(n = ", nrow(tt10), " for 2010-2010)") # Legend label text (no title, single key)

  # lmer
  coef_df_0020_D_nb <- purrr::imap_dfr(models_T0020_D_nb, ~ ext_fixef_D(.x) |> dplyr::mutate(model=.y))

  coef_df_0020_D_nb <- coef_df_0020_D_nb %>%
    mutate(model_label = factor(models_T0020_labels_D_nb[model], levels = models_T0020_order_D_nb)) %>% data.table()

  coef_df_0020_D_nb[, type2 := fcase(
    type == "intercept", "Intercept",
    type == "slope_per_SDyear", "Slope",
    default = as.character(type)
  )]

  coef_df_0020_D_nb[, holc_grade_dif := paste0(holc_grade, ' vs D')]; #fwrite(file = 'Data/freeze/Fig_6_mixed-models.csv',coef_df_0020_D_nb)  

  gap_pt <- 5 # adjusts subtitle spacing: how big a gap you want between the two rows (in points)

  leg_tit = paste0("Mixed-effect negative binomial model<br>on raw observations per year<br>random-effects specification:<br><span style='font-weight:400;font-size:9pt;'>(n = ", comma(nrow(d00)),' for 2000-2020)<br>(n = ', comma(nrow(d10)), ' for 2010-2020)</span>') #  leg_tit = paste0('Mixed-effect model\nrandom-effects specification:\n(n = ', nrow(d00),' for 2000-2020)\n(n = ', nrow(d10), ' for 2010-2020)') # legend title

  rm(sum_m_nb, maD_nb, mbD_nb, mas1D_nb, mas2D_nb, mbs1D_nb); invisible(gc()) # clean

  tr1D_nb = 
  ggplot(coef_df_0020_D_nb, aes(
          x= estimate, y = holc_grade_dif, 
          xmin=conf.low, xmax=conf.high, 
          color=forcats::fct_rev(model_label))) +
    geom_pointrange(position = position_dodge2(width = 0.6)) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_grid(~ type2, scales="free_x") +
    labs(y="Contrasts (relative to HOLC grade D)", x=NULL, subtitle ='2000 - 2020') +
    ggsci::scale_color_locuszoom(
      name   = leg_tit,
      breaks = models_T0020_order_D_nb,
      limits = models_T0020_order_D_nb  # keeps legend/order consistent
    ) +
    # add lm model
    ggnewscale::new_scale_color() +  # start a NEW color scale (separate legend)
    geom_pointrange(
      data = lm_df_nb,
      aes(x = estimate, y = holc_grade_dif,
          xmin = conf.low, xmax = conf.high,
          color = lm_lab_nb),
      position = position_nudge(y = 0.36),  # small vertical offset; remove if undesired
      inherit.aes = FALSE,
      linewidth = 0.4
    ) +
    scale_color_manual(
      name = NULL,                        # no legend title
      values = setNames("black", lm_lab_nb),
      breaks = lm_lab_nb,
      guide = guide_legend(order = 99, override.aes = list(linewidth = 0.8))
    ) +                     
    theme_light() +
    theme(
      plot.margin = margin(t = 3, r = 3, b = gap_pt, l = 3),
      plot.subtitle = element_text(size = 10, colour = "grey40",
                                  margin = margin(b = -22)), # adjust position above the box
      panel.spacing = unit(1.1, "lines"),
      strip.background = element_blank(), # remove grey panel background
      strip.text = element_text(color = "black", margin = margin(b=15)), # make labels black
      axis.text.x = element_blank(),
      legend.title = element_markdown()                             
    )

  # MODELS 2010-2020
  if(recreate_data==TRUE){
  # 0) lm model
  sum_m_10_nb <- MASS::glm.nb(n_obs ~ holc_grade_D*scale(year) + 
              offset(log(sum_area_holc_km2)), 
              tt10)

  # 1) model set per polygon
  ma_D_nb = glmmTMB(sum_bird_obs ~ 
               scale(year)*holc_grade_D + 
               offset(log(area_holc_km2)) +
               (1|state) + (1|city_state) + (1|id), 
               d10,
              family = nbinom2()
              )

  mb_D_nb = glmmTMB(sum_bird_obs ~  scale(year)*holc_grade_D + 
              offset(log(area_holc_km2)) +
              (1|state/city_state/holc_grade/id), 
              d10,
              family = nbinom2()
          )

  mas1_D_nb = glmmTMB(sum_bird_obs ~  scale(year)*holc_grade_D +            
                offset(log(area_holc_km2)) +
                (1|state) + (scale(year)|city_state) + (1|id), 
                d10,
                family = nbinom2()
              )

  mas2_D_nb = glmmTMB(sum_bird_obs ~  scale(year)*holc_grade_D +              
                offset(log(area_holc_km2)) +
                (1|state) + (1|city_state) + (scale(year)|id), 
                d10,
                family = nbinom2()
              )

  mbs1_D_nb = glmmTMB(sum_bird_obs ~  scale(year)*holc_grade_D +  
                 offset(log(area_holc_km2)) +
                 (scale(year)|state/city_state/holc_grade) + (1|id),
                d10,
                family = nbinom2()
              )

  save(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2010-2020.Rdata'),
      sum_m_10_nb, ma_D_nb, mb_D_nb, mas1_D_nb, mas2_D_nb, mbs1_D_nb)
  
  }else{
  load(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2010-2020.Rdata'))
  }

  # model ass
  if(recompute_diag){
  m_ass(name = 'Fig_E3_&_S8_2010-2020_a', mo = sum_m_10_nb, 
    dat = tt10, offset = TRUE, 
    cont = c("year", "sum_area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2010-2020_b', mo = ma_D_nb, 
    dat = d10, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2010-2020_c', mo = mb_D_nb, 
    dat = d10, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2010-2020_d', mo = mas1_D_nb, 
    dat = d10, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE) 

  m_ass(name = 'Fig_E3_&_S8_2010-2020_e', mo = mas2_D_nb, 
    dat = d10, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)

  m_ass(name = 'Fig_E3_&_S8_2010-2020_f', mo = mbs1_D_nb, 
    dat = d10, offset = TRUE, 
    cont = c("year", "area_holc_km2"), categ = 'holc_grade_D',
    show_binned = TRUE, PNG = TRUE)
  }

  # model set
  models_T1020_D_nb <- rlang::set_names(
    list(ma_D_nb, mb_D_nb, mas1_D_nb, mas2_D_nb, mbs1_D_nb),
    c("ma_D_nb","mb_D_nb","mas1_D_nb", "mas2_D_nb", "mbs1_D_nb")
  )
  # labels
  models_T1020_labels_D_nb <- c(
    ma_D_nb      = "(1 | state) + (1 | city) + (1 | polygon)",
    mb_D_nb      = "(1 | state / city / HOLC grade / polygon)",
    mas1_D_nb    = "(1 | state) + (year | city) + (1 | polygon)",
    mas2_D_nb    = "(1 | state) + (1 | city) + (year | polygon)",
    mbs1_D_nb    = "(year | state / city / HOLC grade) + (1 | polygon)"
  )
  
  # sort models
  models_T1020_order_D_nb <- c(
    "(1 | state) + (1 | city) + (1 | polygon)",
    "(1 | state / city / HOLC grade / polygon)",
    "(1 | state) + (year | city) + (1 | polygon)",
    "(1 | state) + (1 | city) + (year | polygon)",
    "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # 2) Extract fixed effects on the modeling scale (ln), fast Wald CIs
  # lm
  lm_df_10_nb <- ext_fixef_D_lm(sum_m_10_nb) |>
    mutate(
      type2 = fcase(type == "intercept", "Intercept",
                    type == "slope_per_SDyear", "Slope",
                    default = as.character(type)),
      holc_grade_dif = paste0(holc_grade, " vs D")  # match tr1D y labels
    )

  lm_lab_10_nb <-  paste0(
    sprintf("Negative binomial model\non bird observations per year\n(n = %s for 2000-2020, ", nobs(sum_m_10_nb)), "n = ", nrow(tt10), " for 2010-2010)")  # Legend label text (no title, single key) 

  # lmer
  coef_df_1020_D_nb <- purrr::imap_dfr(models_T1020_D_nb, ~ ext_fixef_D(.x) |> dplyr::mutate(model=.y))

  coef_df_1020_D_nb <- coef_df_1020_D_nb %>%
    mutate(model_label = factor(models_T1020_labels_D_nb[model], levels = models_T1020_order_D_nb)) %>% data.table()

  coef_df_1020_D_nb[, type2 := fcase(
    type == "intercept", "Intercept",
    type == "slope_per_SDyear", "Slope",
    default = as.character(type)
  )]

  coef_df_1020_D_nb[, holc_grade_dif := paste0(holc_grade, ' vs D')]

  rm(sum_m_10_nb, ma_D_nb, mb_D_nb, mas1_D_nb, mas2_D_nb, mbs1_D_nb); invisible(gc()) # clean

  tr2D_nb =     
  ggplot(coef_df_1020_D_nb, aes(
          x= estimate, y = holc_grade_dif, 
          xmin=conf.low, xmax=conf.high, 
          color=forcats::fct_rev(model_label))) +
    # lmer
    geom_pointrange(position = position_dodge2(width = 0.6)) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_grid(~ type2, scales="free_x") +
    labs(y="Contrasts (relative to HOLC grade D)", 
        x="Difference in log sampling density (per km² per year)\n(% difference in sampling density per km² per year)", subtitle ='2010 - 2020') +
    ggsci::scale_color_locuszoom(
      name   = leg_tit,
      breaks = models_T1020_order_D_nb,
      limits = models_T1020_order_D_nb  # keeps legend/order consistent
    ) +
    # lm 
    ggnewscale::new_scale_color() +
    geom_pointrange(
      data = lm_df_10_nb,
      aes(x = estimate, y = holc_grade_dif,
          xmin = conf.low, xmax = conf.high,
          color = lm_lab_10_nb),
      position = position_nudge(y = 0.36),
      inherit.aes = FALSE,
      linewidth = 0.4
    ) +
    scale_color_manual(
      name = NULL,
      values = setNames("black", lm_lab_10_nb),
      breaks = lm_df_10_nb,
      guide = guide_legend(order = 99, override.aes = list(linewidth = 0.8))
    ) +    
    scale_x_continuous(labels = lab_log_plus_pct) + #scale_x_continuous(labels = lab_log_rr) +                   
    theme_light() +
    theme(
      plot.margin = margin(t = gap_pt, r = 3, b = 5.5, l = 3),
      plot.subtitle = element_text(size = 10, colour = "grey40",
                                  margin = margin(b = 2)),
      panel.spacing = unit(1.1, "lines"),
      strip.background = element_blank(),       # remove grey panel background
      strip.text = element_text(color = "black", margin = margin(b=10)),
      strip.text.x = element_blank(),
      legend.position = "none"                            
    )

  # COMBINE - x-axis correspond accros facets  
  # facet-wise ranges across BOTH datasets
  rng <- bind_rows(coef_df_0020_D_nb, coef_df_1020_D_nb,lm_df_nb, lm_df_10_nb) |>
    group_by(type2) |>
    summarise(
      xmin = min(pmin(conf.low, conf.high), na.rm = TRUE),
      xmax = max(pmax(conf.low, conf.high), na.rm = TRUE),
      .groups = "drop"
    )

  add_02_break  <- function(lims) sort(unique(c(scales::pretty_breaks()(lims), 0.2)))  

  facet_scales <- lapply(seq_len(nrow(rng)), function(i) {
    tl <- rng$type2[i]
    lo <- rng$xmin[i]
    hi <- rng$xmax[i]

    if (grepl("^Slope", tl)) {
      rlang::new_formula(
        lhs = bquote(type2 == .(tl)),
        rhs = scale_x_continuous(
          limits = c(-0.4, 0.6), 
          breaks = add_02_break,
          labels = lab_log_plus_pct, 
          oob = scales::oob_keep, # keep data for stats, don't drop rows 
          expand = expansion(mult = c(0, 0)) 
        ) 
      ) 
    } else if (grepl("^Intercept", tl)) { 
      rlang::new_formula( 
        lhs = bquote(type2 == .(tl)), 
        rhs = scale_x_continuous( 
          limits = c(-0.2, 1.6), 
          breaks = add_02_break,
          labels = lab_log_plus_pct, 
          oob = scales::oob_keep, 
          expand = expansion(mult = c(0, 0)) 
        ) 
      ) 
    } else { 
      rlang::new_formula( 
        lhs = bquote(type2 == .(tl)), 
        rhs = scale_x_continuous( 
          limits = c(lo, hi),
          breaks = add_02_break, 
          labels = lab_log_plus_pct,
          oob = scales::oob_keep, 
          expand = expansion(mult = c(0, 0)) 
        ) 
      ) 
    } 
  })  

  tr1D_nb_adj <- tr1D_nb + ggh4x::facetted_pos_scales(x = facet_scales) 
  tr2D_nb_adj <- tr2D_nb + ggh4x::facetted_pos_scales(x = facet_scales)
  
  (tr1D_nb_adj / tr2D_nb_adj) + plot_layout(axis_titles = "collect") #; ggsave('Output/Fig_6.png', width= 27, height = 16, units ='cm')

#' <a name="F_E3">
#' **Extended Data Figure 3</a> | Estimated differences in HOLC grade sampling density over time.** Dots represent fixed-effect contrasts on the log scale, together with the implied percentage change in sampling density (observations per km² per year; relative to grade D), obtained by exponentiating those contrasts from negative binomial mixed models with log link and offset of `log(area (km²))`. Intercept panels show differences between each HOLC grade at the mean year, slope panels differences per standard deviation increase in year. Horizontal lines are 95% Wald confidence intervals. Colour indicates random-effects structures (variables left of `|` are random slopes, right of `|` random intercepts, and `/` indicates nesting). The top row contains estimates for a dataset spanning 2000-2020 (for the linear model n = `r comma(nrow(tt00))` sum of observations per grade and year, for the mixed models n = `r comma(nrow(d00))` sum of observations per polygon and year); the bottom row contains estimates for a dataset from 2010-2020 (n = `r comma(nrow(tt10))` and n = `r comma(nrow(d10))`, respectively). For per grade estimates see our Fig. [S8](#F_S8).
#' 
#' 
#' #### B. Mean and slope values
#+ F_S8, fig.width = 25/2.5, fig.height = 15/2.5
  # MODELS 2000-2020
  
  # 1) reparametrize models to estimate separte intercepts and slopes for each holc grade 
  if(recreate_data==TRUE){
  # lm model on sum per holc grade
  sum_mi  <- MASS::glm.nb(n_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
              offset(log(sum_area_holc_km2)), 
              tt00)
  
  # lmer models on all polygons
  mai <- glmmTMB(
      sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
        offset(log(area_holc_km2)) +
        (1|state) + (1|city_state) + (1|id),
      data = d00,
      family = nbinom2()
    )

  mbi = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state/city_state/holc_grade/id), 
              d00,
              family = nbinom2()
            )

  mas1i = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) +  
              offset(log(area_holc_km2)) +
              (1|state) + (scale(year)|city_state) + (1|id), 
              d00,
              family = nbinom2()
              )


  mas2i = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state) + (1|city_state) + (scale(year)|id), 
              d00,
              family = nbinom2()
              )

  mbs1i = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
              offset(log(area_holc_km2)) +
              (scale(year)|state/city_state/holc_grade) + (1|id),
              d00,
              family = nbinom2()
              )

  save(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2000-2020_no-int.Rdata'),
      sum_mi, mai, mbi, mas1i, mas2i, mbs1i)
  
  }else{
  load(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2000-2020_no-int.Rdata'))
  }

  # 2) model set and labels
  models_T0020<- list(
    mai       = mai,
    mbi       = mbi,
    mas1i       = mas1i,
    mas2i       = mas2i,
    mbs1i      = mbs1i
  )

  models_T0020_labels <- c(
    mai      = "(1 | state) + (1 | city) + (1 | polygon)",
    mbi      = "(1 | state / city / HOLC grade / polygon)",
    mas1i    = "(1 | state) + (year | city) + (1 | polygon)",
    mas2i    = "(1 | state) + (1 | city) + (year | polygon)",
    mbs1i    = "(year | state / city / HOLC grade) + (1 | polygon)"
  )
  # sort models
  models_T0020_order <- c(
    "(1 | state) + (1 | city) + (1 | polygon)",
    "(1 | state / city / HOLC grade / polygon)",
    "(1 | state) + (year | city) + (1 | polygon)",
    "(1 | state) + (1 | city) + (year | polygon)",
    "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # 3) Extract fixed effects on the modeling scale (ln), fast Wald CIs
  # lm
  lmi_df <- ext_fixef_lm(sum_mi) |>
    dplyr::mutate(
      type2 = fcase(type == "intercept", "Intercept",
                    type == "slope_per_SDyear", "Slope",
                    default = as.character(type))
    )

  lm_lab <- paste0(
    sprintf("Negative binomial model\non bird observations per year\n(n = %s for 2000-2020, ", nobs(sum_mi)), "n = ", nrow(tt10), " for 2010-2010)") # Legend label text (no title, single key)

  # lmer
  coef_df_0020 <- purrr::imap_dfr(models_T0020, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

  coef_df_0020 <- coef_df_0020 %>%
    dplyr::mutate(model_label = factor(models_T0020_labels[model], levels = models_T0020_order)) %>% data.table()

  coef_df_0020[, type2 := fcase(
    type == "intercept",        "Intercept",
    type == "slope_per_SDyear", "Slope",
    default = as.character(type)
  )]

  leg_tit = paste0("Mixed-effect negative binomial model<br>on raw observations per year<br>random-effects specification:<br><span style='font-weight:400;font-size:9pt;'>(n = ",  comma(nrow(d00)),' for 2000-2020)<br>(n = ', comma(nrow(d10)), ' for 2010-2020)</span>') # legend title

  gap_pt <- 5 # adjusts subtitle spacing: how big a gap you want between the two rows (in points)

  tr1 = 
  ggplot(coef_df_0020, aes(
          x= estimate, y = holc_grade, 
          xmin=conf.low, xmax=conf.high, 
          color=forcats::fct_rev(model_label))) +
    # lmer
    geom_pointrange(position = position_dodge2(width = 0.6)) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_grid(~ type2, scales="free_x") +
    labs(y="HOLC grade", x= NULL, subtitle ='2000 - 2020') +
    ggsci::scale_color_locuszoom(
      name   = leg_tit,
      breaks = models_T0020_order,
      limits = models_T0020_order  # keeps legend/order consistent
    ) +
    # lm model
    ggnewscale::new_scale_color() +  # start a NEW color scale (separate legend)
    geom_pointrange(
      data = lmi_df,
      aes(x = estimate, y = holc_grade,
          xmin = conf.low, xmax = conf.high,
          color = lm_lab),
      position = position_nudge(y = 0.36),  # small vertical offset
      inherit.aes = FALSE,
      linewidth = 0.4
    ) +
    scale_color_manual(
      name = NULL,                        # no legend title
      values = setNames("black", lm_lab),
      breaks = lm_lab,
      guide = guide_legend(order = 1, override.aes = list(linewidth = 0.8))
    ) + 
    theme_light() +
    theme(
      plot.margin = margin(t = 3, r = 3, b = gap_pt, l = 3),
      plot.subtitle = element_text(size = 10, colour = "grey40",
                                  margin = margin(b = -22)), # adjust position above the box
      axis.text.x = element_blank(),
      panel.spacing = unit(1.1, "lines"),
      strip.background = element_blank(), # remove grey panel background
      strip.text = element_text(color = "black", margin = margin(b=15)), # make labels black
      legend.title = element_markdown() 

    )

  # MODELS 2010-2020
  # 1) reparametrize models to estimate separte intercepts and slopes for each holc grade 
  if(recreate_data==TRUE){
  # lm model on sum per holc grade
    sum_m_i  <- MASS::glm.nb(n_obs ~ 0 + holc_grade + holc_grade:scale(year) + 
              offset(log(sum_area_holc_km2)), 
              tt10)

  # lmer models on all polygons  
    ma_i = glmmTMB(sum_bird_obs ~ 
               0 + holc_grade + holc_grade:scale(year) + 
               offset(log(area_holc_km2)) +
               (1|state) + (1|city_state) + (1|id), 
               d10,
              family = nbinom2()
              )

    mb_i = glmmTMB(sum_bird_obs ~  0 + holc_grade + holc_grade:scale(year) + 
              offset(log(area_holc_km2)) +
              (1|state/city_state/holc_grade/id), 
              d10,
              family = nbinom2()
          )
    mas1_i = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) +            
                offset(log(area_holc_km2)) +
                (1|state) + (scale(year)|city_state) + (1|id), 
                d10,
                family = nbinom2()
              )

    mas2_i = glmmTMB(sum_bird_obs ~ 0 + holc_grade + holc_grade:scale(year) +
                offset(log(area_holc_km2)) +
                (1|state) + (1|city_state) + (scale(year)|id), 
                d10,
                family = nbinom2()
              )

    mbs1_i = glmmTMB(sum_bird_obs ~  0 + holc_grade + holc_grade:scale(year) + 
                 offset(log(area_holc_km2)) +
                (scale(year)|state/city_state/holc_grade) + (1|id),
                d10,
                family = nbinom2()
              )
  save(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2010-2020_no-int.Rdata'),
      sum_m_i, ma_i, mb_i, mas1_i, mas2_i, mbs1_i)
  
  }else{
  load(file = here::here('Data/MaPe/DAT_glmmTMB_NB_2010-2020_no-int.Rdata'))
  }

  # 2) model set and labels
  models_T1020<- list(
    ma_i       = ma_i,
    mb_i       = mb_i,
    mas1_i       = mas1_i,
    mas2_i       = mas2_i,
    mbs1_i      = mbs1_i
  )

  models_T1020_labels <- c(
    ma_i      = "(1 | state) + (1 | city) + (1 | polygon)",
    mb_i      = "(1 | state / city / HOLC grade / polygon)",
    mas1_i    = "(1 | state) + (year | city) + (1 | polygon)",
    mas2_i    = "(1 | state) + (1 | city) + (year | polygon)",
    mbs1_i    = "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # sort models
  models_T1020_order <- c(
    "(1 | state) + (1 | city) + (1 | polygon)",
    "(1 | state / city / HOLC grade / polygon)",
    "(1 | state) + (year | city) + (1 | polygon)",
    "(1 | state) + (1 | city) + (year | polygon)",
    "(year | state / city / HOLC grade) + (1 | polygon)"
  )

  # 3) Extract fixed effects on the modeling scale (ln), fast Wald CIs
  # lm
  lmi_df_10 <- ext_fixef_lm(sum_m_i) |>
    dplyr::mutate(
      type2 = fcase(type == "intercept", "Intercept",
                    type == "slope_per_SDyear", "Slope",
                    default = as.character(type))
    )

  lm_10_lab <- paste0(
    sprintf("Negative binomial model\non bird observations per year\n(n = %s for 2000-2020, ", nobs(sum_m_i)), "\n(n = ", nrow(tt10), " for 2010-2010)") # Legend label text (no title, single key)
  
  # lmer
  coef_df_1020 <- purrr::imap_dfr(models_T1020, ~ ext_fixef(.x) |> dplyr::mutate(model=.y))

  coef_df_1020 <- coef_df_1020 %>%
    dplyr::mutate(model_label = factor(models_T1020_labels[model], levels = models_T1020_order)) %>% data.table()

  coef_df_1020[, type2 := fcase(
    type == "intercept",        "Intercept",
    type == "slope_per_SDyear", "Slope",
    default = as.character(type)
  )]

  tr2 =     
  ggplot(coef_df_1020, aes(
          x= estimate, y = holc_grade, 
          xmin=conf.low, xmax=conf.high, 
          color=forcats::fct_rev(model_label))) +
    # lmer
    geom_pointrange(position = position_dodge2(width = 0.6)) +
    geom_vline(xintercept = 0, linetype = "dotted", color = "grey40") +
    facet_grid(~ type2, scales="free_x") +
    labs(y="HOLC grade", x="Log sampling denisty (observations per km² per year)", subtitle ='2010 - 2020') +
    ggsci::scale_color_locuszoom(
      name   = leg_tit,
      breaks = models_T1020_order,
      limits = models_T1020_order  # keeps legend/order consistent
    ) +
    # lm 
    ggnewscale::new_scale_color() +
    geom_pointrange(
      data = lmi_df_10,
      aes(x = estimate, y = holc_grade,
          xmin = conf.low, xmax = conf.high,
          color = lm_10_lab),
      position = position_nudge(y = 0.36),
      inherit.aes = FALSE,
      linewidth = 0.4
    ) +
    scale_color_manual(
      name = NULL,
      values = setNames("black", lm_10_lab),
      breaks = lm_lab,
      guide = guide_legend(order = 99, override.aes = list(linewidth = 0.8))
    ) +  
    theme_light() +
    theme(
      plot.margin = margin(t = gap_pt, r = 3, b = 5.5, l = 3),
      plot.subtitle = element_text(size = 10, colour = "grey40",
                                  margin = margin(b = 2)),
      panel.spacing = unit(1.1, "lines"),
      strip.background = element_blank(),       # remove grey panel background
      strip.text = element_text(color = "black", margin = margin(b=10)),
      strip.text.x = element_blank(),
      legend.position = "none"                            
    )

  # COMBINE  
    rng <- bind_rows(coef_df_0020, coef_df_1020,lmi_df, lmi_df_10) |>
      group_by(type2) |>
      summarise(
        xmin = min(pmin(conf.low, conf.high), na.rm = TRUE),
        xmax = max(pmax(conf.low, conf.high), na.rm = TRUE),
        .groups = "drop"
      )

    x_int  <- c(rng$xmin[rng$type2 == "Intercept"],
            rng$xmax[rng$type2 == "Intercept"])
    x_slope <- c(rng$xmin[rng$type2 == "Slope"],
                rng$xmax[rng$type2 == "Slope"])

    facet_scales <- list(
      type2 == "Intercept" ~ scale_x_continuous(
        limits = x_int,            
        #breaks = add_02_break,
        oob    = scales::oob_keep,
        expand = expansion(mult = c(0, 0))
      ),
      type2 == "Slope" ~ scale_x_continuous(
        limits = x_slope,#x_slope,                 
        #breaks = add_02_break,
        oob    = scales::oob_keep,
        expand = expansion(mult = c(0, 0))
      )
    )

    tr1_adj <- tr1 + ggh4x::facetted_pos_scales(x = facet_scales) 
    tr2_adj <- tr2 + ggh4x::facetted_pos_scales(x = facet_scales) 
    
    (tr1_adj / tr2_adj) + plot_layout(axis_titles = "collect") #; ggsave('Output/rev_Fig_Z2_rates.png', width= 25, height = 15, units ='cm')

#' <a name="F_S8">
#' **Figure S8</a> | Estimates of log sampling density between HOLC grades over time.** Dots for intercepts give the log sampling density (observations per km² per year) for each HOLC grade at the mean year; dots for slopes give the change in log sampling density per 1 standard deviation increase in year. Horizontal lines are 95% Wald confidence intervals. Colour indicates random-effects structure (variables left of `|` are random slopes, right of `|` random intercepts, and '/' indicates nesting). The depicted estimates represent per grade estimates (not contrasts, which are shown in Extended Data Fig. [1](#F_E1)). The top row contains estimates for a dataset spanning 2000-2020 (n = `r comma(nrow(tt00))` sum of observations per grade and year for the linear model, and n = `r comma(nrow(d00))` sum of observations per polygon and year); the bottom row contains estimates for a dataset from 2010-2020 (n = `r comma(nrow(tt10))` and n = `r comma(nrow(d10))`, respectively).
#'
#' ***
#'  
#' The above models are intended as appropriate alternatives to the authors' inappropriate specified models. However, the authors' models impose linear trends that do not capture the non-linear structure visible in the data (our Fig. [1](#F_1), Extended Data Fig. [1](#F_E1), as well as differences in 2000-2020 vs 2010-2020 estimates in our Extended Data Fig. [3](#F_E3), and Fig. [S8](#F_S8)). Thus, to visualise the fluctuations we analysed temporal changes in observation rates using a generalised additive mixed model (GAM) fitted with `bam()` in `mgcv`. The response was the annual number of bird observations per HOLC polygon, modelled with a negative-binomial distribution and a centered `log(area)` offset to estimate rates per km². Year (scaled) was fitted with a penalised regression spline, with grade-specific deviations (`s(year, by=holc_grade)`), allowing flexible non-linear trends. Random intercepts for state and city, and random city-specific year slopes (`bs="re"`) accounted for spatial hierarchy and differing temporal trajectories among cities, which captures city-level departures from the global smooth but avoids over-parameterising rarely sampled city–grade combinations. Models were fitted via REML with discrete smooths to support large datasets (~200k rows).
#' 
#' **Model-based predictions and contrasts** To visualise temporal trends in sampling density for each HOLC grade, we obtained population-level predictions from the negative-binomial GAM by marginalising over all random effects. Predictions were generated using the model’s linear predictor matrix (`type = "lpmatrix"`) while excluding random-effect smooths (`s(state)`, `s(city_state)`, `s(id)`, and `s(city_state):year_s)`. This yields expected values for an “average” neighbourhood (i.e. with mean-zero random effects).
#' 
#' To quantify differences between grades, we constructed linear contrasts of the prediction matrices (e.g. A − D) and computed associated Wald intervals using the full model variance–covariance matrix via the delta method. Predicted values were transformed back to the original response scale (observations per km²), enabling interpretation on (i) the model scale, (ii) the percentage scale, and (iii) the absolute scale.
#' 
#' ***
#'  
#+ F_2, fig.width = 12/2.54, fig.height = 18/2.54, out.width = "80%"
# USE the following if ggploting the figure, instead of png sourcing: fig.width = 16/2.5, fig.height = 9/2.5
#rm(samp_d_binary_holc, m0, samp_d_binary_holc_rirs,m1, m1b, m2, m1p, m2p, m3p, samp_m0_g, m0_g, samp_m1_g, m1_g, m1b_g, m2_g, m1p_g, m2p_g, m3p_g, d_ri, mB0, d_rirs, mB1,mB2,  d_fe_rirs, mB1p,mB2p, mB3p, d_ri_a,mB0_a, d_rirs_a, mB1_a, mB2_a, d_fe_rirs_a, mB1p_a,  mB2p_a, mB3p_a, d_ri_r, mB0_r, d_rirs_r, mB1_r,  mB2_r, d_fe_rirs_r,mB1p_r,  mB2p_r, mB3p_r, c_ri, mC0, c_rirs, mC1, mC2, c_fe_rirs, mC1p, mC2p, mC3p, m_density_mul, m_density_ok, m_density_mul_ln, m_density_ok_ln, gam_density_mul, gam_density_ok, gam_density_mul_ln, gam_density_ok_ln, sum_m_nb, maD_nb, mbD_nb, mas1D_nb, mas2D_nb, mbs1D_nb, sum_m_10_nb, ma_D_nb, mb_D_nb, mas1_D_nb, mas2_D_nb, mbs1_D_nb, sum_mi, mai, mbi, mas1i, mas2i, mbs1i, sum_m_i, ma_i, mb_i, mas1_i, mas2_i, mbs1_i ); gc() # to free memory for the huge bam NB output 

rm(list=setdiff(ls(), c("d00", "d10", "holc_pal", "m_ass", "minor_breaks_log10", "recreate_data", "proj_root")));  invisible(gc()) # free memory for the huge bam NB output 

# center/scale year like in lmer
d00[, year_s := as.numeric(scale(year))]
sc <- scale(d00$year)                
c0 <- attr(sc, "scaled:center") 
s0 <- attr(sc, "scaled:scale")

d10[, year_s := as.numeric(scale(year))]
sc10 <- scale(d10$year)                
c0_10 <- attr(sc10, "scaled:center") 
s0_10 <- attr(sc10, "scaled:scale")

# prepare other data
d00$state      <- droplevels(factor(d00$state))
d00$city_state <- droplevels(factor(d00$city_state))
d00$id        <- droplevels(factor(d00$id))
d00$holc_grade <- droplevels(factor(d00$holc_grade))
d00[, off := log(area_holc_km2)]
d00[, off := off - mean(off)] # subtracting a constant from the offset is equivalent to adding that constant to the intercept; thus, the intercept will correspond to the expected log-count at mean log(area), which improves numerical stability and makes the intercept more interpretable

d10$state      <- droplevels(factor(d10$state))
d10$city_state <- droplevels(factor(d10$city_state))
d10$id        <- droplevels(factor(d10$id))
d10$holc_grade <- droplevels(factor(d10$holc_grade))
d10[, off := log(area_holc_km2)]
d10[, off := off - mean(off)]

plot_only = TRUE # avoids hitting RAM limits during html generation by loading only the resulting figures (that can be generated by the below code)

if(plot_only){
knitr::include_graphics(here::here("Output", "Fig_2.png"))
} else {
if(recreate_data){ # loading model outpus; the below models runs for long and often needs clean R session, else the memory capacity might be reached too early

# 2000-2020 data
m_nb <- bam(sum_bird_obs ~ 
            holc_grade + 
            s(year_s,k=10,bs="cr") +
            s(year_s,by=holc_grade,k=10,bs="cr") +
            s(state,bs="re") +
            s(city_state,bs="re")+s(id,bs="re") +
            s(city_state, by=year_s, bs="re"),
            offset = off, # offset for log(area) (centered so that the mean offset is zero)
            family = nb(),  # mgcv estimates theta
            data=d00, method="fREML", 
            discrete=TRUE, select=TRUE, gc.level=2,
            nthreads = max(1L, parallel::detectCores() - 1L)
)

save(file=here::here('Data/MaPe/DAT_bam-nb_output_v2.Rdata', m_nb))  
m_ass(name  = 'Fig_2_2000-2020', mo = m_nb, dat = d00, offset = TRUE, cont = c("year", "area_holc_km2"), categ = 'holc_grade',show_binned = TRUE, show_temporal_grouped = 'year', PNG = TRUE) 
# Good mean–variance handling (Pearson dispersion ≈ 0.886 → slight underdispersion, which is harmless and common in NB fits). No evidence of global spatial autocorrelation (Moran’s I ≈ 0; p=1). Residual patterns follow expected NB behaviour: right tail, heteroskedasticity decreasing with fitted μ, and no systematic grade/area/year structure. Autocorrelation negligible across lags. Offset treated correctly (residuals vs offset clean). In short: Model assumptions look fine; nothing invalidates inference or predictions. This is as good as one can reasonably expect from a 200k-row NB GAM with complex structure. Titles indicate specific models behind specific figures or tables and highlight model formula, family and, if present, dispersion parameter. Panel 1–2. Pearson residuals vs fitted / sqrt-residuals vs fitted: Classic NB funnel shape: residual variance compresses with increasing mean; no curvature or systematic deviation; no clumping by HOLC grade; the red dashed line is centered correctly → no bias. 3-4. Binned counts vs fitted: Points lie along the 95% envelopes except at extreme counts (>200), which is expected in NB with large city hotspots; no systematic over- or under-fitting in the low/mid ranges. 5. Normal QQ plot: NB residuals should be right-skewed; this is normal; the strong right tail is expected; no left-tail distortions; acceptable for NB GAMs. 6. Residuals vs offset: Clean; no correlation or pattern; confirms offset(log(area)) is working as intended. 7–9. Residuals vs year, area, HOLC grade: Year noisy but no long-term drift → the spline absorbed temporal trends; Area: expected decreasing variance with area (because large polygons produce higher mean → smaller Pearson residual magnitude); HOLC grade: same shape across grades; no grade-by-mean interaction leftover. 10-11. ACF & PACF: No significant autocorrelation; minor wiggles at early lags are well within bounds. 12–15. Spatial distribution maps (<0 and >0): No obvious clusters or directional gradients; positive/negative residuals appear randomly distributed; no spatial violation; spatial autocorrelation (Moran’s I) = −0.043, p = 1; no global spatial autocorrelation. 14. Dispersion summary: Pearson dispersion = 0.886 (slightly < 1) → mild underdispersion.NB fits are designed for overdispersion; slight underdispersion means the NB family is conservative; No risk of inflated false positives.

# Strip heavy stuff that m_ass and below code never touches
m_nb$model             <- NULL
m_nb$y                 <- NULL
m_nb$R                 <- NULL
m_nb$Ve                <- NULL
m_nb$X                 <- NULL
m_nb$residuals         <- NULL
m_nb$fitted.values     <- NULL
m_nb$linear.predictors <- NULL
m_nb$effects           <- NULL
save(file=here::here('Data/MaPe/DAT_bam-nb_output_lean.Rdata', m_nb))

rm(m_nb);  invisible(gc()) # clean

# 2010-2020 data only
m_nb_10 <- bam(sum_bird_obs ~ 
            holc_grade + 
            s(year_s,k=10,bs="cr") +
            s(year_s,by=holc_grade,k=10,bs="cr") +
            s(state,bs="re") +
            s(city_state,bs="re")+s(id,bs="re") +
            s(city_state, by=year_s, bs="re"),
            offset = off, # offset for log(area) (centered so that the mean offset is zero)
            family = nb(),  # mgcv estimates theta
            data=d10, method="fREML", 
            discrete=TRUE, select=TRUE, gc.level=2,
            nthreads = max(1L, parallel::detectCores() - 1L)
)
save(file=here::here('Data/MaPe/DAT_bam-nb_output_data1020.Rdata'), m_nb_10)
m_ass(name  = 'Fig_2_2010-2020', mo = m_nb_10, dat = d10, offset = TRUE, cont = c("year", "area_holc_km2"), categ = 'holc_grade', show_binned = TRUE, show_temporal_grouped = 'year', PNG = TRUE)  

m_nb_10$model             <- NULL
m_nb_10$y                 <- NULL
m_nb_10$R                 <- NULL
m_nb_10$Ve                <- NULL
m_nb_10$X                 <- NULL
m_nb_10$residuals         <- NULL
m_nb_10$fitted.values     <- NULL
m_nb_10$linear.predictors <- NULL
m_nb_10$effects           <- NULL

save(file=here::here('Data/MaPe/DAT_bam-nb_output_data1020_lean.Rdata'), m_nb_10) 
rm(m_nb_10);  invisible(gc()) # clean

# load newly generated model outpus
load(file=here::here('Data/MaPe/DAT_bam-nb_output_lean.Rdata'))
load(file=here::here('Data/MaPe/DAT_bam-nb_output_data1020_lean.Rdata')) 

} else {
  load(file=here::here('Data/MaPe/DAT_bam-nb_output_lean.Rdata'))
  load(file=here::here('Data/MaPe/DAT_bam-nb_output_data1020_lean.Rdata')) 
  }


# MARGINAL PREDICTIONS
# reference level
lev1 <- function(f) {
  f <- factor(f) 
  factor(levels(f)[1], levels = levels(f))
}

# 2000-2020

# prepare variables for prediction
yr  <- seq(min(d00$year), max(d00$year), by = 1)
yr_s <- (yr - c0)/s0

mean_log_area <- mean(log(d00$area_holc_km2))
off_1km2      <- log(1) - mean_log_area

# random terms to exclude (to predict for mean population)
excl <- c("s(state)","s(city_state)","s(id)","s(city_state):year_s")

# newdata builder
make_nd <- function(grade) {
  data.frame(
    holc_grade    = factor(grade, levels = levels(d00$holc_grade)),
    year          = yr,
    year_s        = yr_s,
    state         = lev1(d00$state),
    city_state    = lev1(d00$city_state),
    id            = lev1(d00$id),
    area_holc_km2 = 1,
    off           = off_1km2
  )
}

# per-grade marginal (population average) curves (per km²), with CIs

pred_one <- function(g){
  nd <- make_nd(g)
  pr <- predict(m_nb, newdata = nd, type = "link", se.fit = TRUE,
                exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)
  cbind(nd,
        fit = pr$fit,
        lwr = pr$fit - 1.96*pr$se.fit,
        upr = pr$fit + 1.96*pr$se.fit)
}

curves_nb <- rbindlist(lapply(levels(d00$holc_grade), pred_one))

# original scale (per km²)
curves_nb[, `:=`(fit_orig = exp(fit), lwr_orig = exp(lwr), upr_orig = exp(upr))]

# log breaks for obs/km²
brks <- c(0.1, 1, 10, 100)

# adj
minor_adj = 0.2

g_bam1_nb <-
  ggplot(curves_nb, aes(year, fit_orig, colour = holc_grade, fill = holc_grade)) +
  geom_ribbon(aes(ymin = lwr_orig, ymax = upr_orig), alpha = 0.15, colour = NA) +
  geom_line() +
  coord_cartesian(ylim = c(0.039, 100)) + 
  scale_y_log10(
      name   = "Sampling density (observations per km²)",
      breaks = brks, 
      labels = scales::label_number(drop0trailing = TRUE), 
      minor_breaks = minor_breaks_log10) +
  annotate("text", x = min(curves_nb$year) - minor_adj, y = 5,
           label = "5", size = 2.2, colour = "grey60", hjust = 1) +
  annotate("text", x = min(curves_nb$year) - minor_adj, y = 20,
           label = "20", size = 2.2, colour = "grey60", hjust = 1) +
  annotate("text", x = min(curves_nb$year) - minor_adj, y = 40,
           label = "40", size = 2.2, colour = "grey60", hjust = 1) +              
  labs(x = "Year", subtitle = "log scale") +
  scale_fill_manual(values = holc_pal, name = "HOLC grade") +
  scale_colour_manual(values = holc_pal, name = "HOLC grade") +
  theme_minimal(base_size = 8) +
  theme(plot.subtitle = element_text(colour = "grey40"),
        legend.position = c(1.03, 0.45),
        legend.justification = c("right", "top"),
        legend.text  = element_text(size = 6),
        legend.title = element_text(size = 7, margin = margin(b = 0.75)),
        legend.spacing.x = unit(0.2, "lines"),
        legend.spacing.y = unit(0.2, "lines"),
        legend.key.size = unit(0.5, "lines")
    )

g_bam2_nb = 
ggplot(curves_nb, aes(year, fit_orig, colour = holc_grade, fill = holc_grade)) +
  geom_ribbon(aes(ymin = lwr_orig, ymax = upr_orig), alpha = 0.15, colour = NA) +
  geom_line() +
  labs(y = "Sampling density (observations per km²)", x = "Year", subtitle = 'original scale') +
  scale_fill_manual(values = holc_pal) + 
  scale_colour_manual(values = holc_pal) + 
  theme_minimal(base_size = 8) +
  theme(legend.position = "none",
        plot.subtitle = element_text(colour = "grey40"))

#g_bam1_nb / g_bam2_nb  + plot_layout(axis_title = 'collect', axes = "collect"); #ggsave('Output/bam-nb_ABCD_v3.png', width = 8, height = 9, units = 'cm')

# CONTRAST A-D
ndA <- make_nd("A")
ndD <- make_nd("D")

# design matrices
XpA <- predict(m_nb, newdata = ndA, type = "lpmatrix",
               exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)
XpD <- predict(m_nb, newdata = ndD, type = "lpmatrix",
               exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)

beta <- coef(m_nb); V <-vcov(m_nb) # vcov(m_nb, unconditional = TRUE) 

# ln-diff
Xd   <- XpA - XpD
diff <- as.vector(Xd %*% beta)
se   <- sqrt(rowSums((Xd %*% V) * Xd))

# ratio & %
ratio <- exp(diff); loR <- exp(diff - 1.96*se); hiR <- exp(diff + 1.96*se)
pct   <- (ratio - 1) * 100

# absolute gap via delta method (approximates the variance of a non-linear transformation)
etaA <- as.vector(XpA %*% beta); muA <- exp(etaA)
etaD <- as.vector(XpD %*% beta); muD <- exp(etaD)
G    <- (muA * XpA) - (muD * XpD)
seAD <- sqrt(rowSums((G %*% V) * G))
abs  <- muA - muD; abs_lo <- abs - 1.96*seAD; abs_hi <- abs + 1.96*seAD

out_nb <- data.table(year = yr,
  diff = diff, lwr = diff - 1.96*se, upr = diff + 1.96*se,
  ratio = ratio, lo = loR, hi = hiR, pct = pct,
  abs = abs, abs_lwr = abs_lo, abs_upr = abs_hi)

# clean 
rm(m_nb);  invisible(gc())

# plot on ln-scale
g_t1_nb = 
ggplot(out_nb, aes(year, diff)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted A - D differences", x = "Year", subtitle = 'absolute, log scale') +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as ratio (A:D)
g_t2_nb = 
ggplot(out_nb, aes(year, ratio)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.15) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(y = "Predicted A - D differences", x = "Year", subtitle = "relative to D, original scale") +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as percent difference
g_t3_nb = 
ggplot(out_nb, aes(year, pct)) +
  geom_ribbon(aes(ymin = (lo-1)*100, ymax = (hi-1)*100), alpha = 0.15) +
  geom_line() +
  #geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted differences (A - D)", x = "Year",  subtitle = "%, relative to D") +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as absolute differences
g_t4_nb = 
ggplot(out_nb, aes(year, abs)) +
  geom_ribbon(aes(ymin = abs_lwr, ymax = abs_upr), alpha = 0.15) +
  geom_line() +
  #geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted differences (A - D)", x = "Year",  subtitle = "absolute, observations per km²") +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))


#left_bam = g_bam1_nb / g_bam2_nb  + plot_layout(axis_title = 'collect', axes = "collect")
#right_bam = g_t3_nb / g_t4_nb + plot_layout(axis_title = 'collect', axes = "collect")

#(left_bam | right_bam)+ plot_layout(axis_title = 'collect_x'); #ggsave('Output/bam-nb_4-panel.png', width = 12, height = 9, units = 'cm'); absolute gaps depend on raw increases; relative gaps depend on raw increases divided by a rising baseline — so early low-sampling years can flatten percentages even when divergence is real.

# 2010-2020
# prepare variables for prediction
yr_10  <- seq(min(d10$year), max(d10$year), by = 1)
yr_s_10 <- (yr_10 - c0_10)/s0_10

mean_log_area_10 <- mean(log(d10$area_holc_km2))
off_1km2_10      <- log(1) - mean_log_area_10

# random terms to exclude (to predict for mean population)
excl <- c("s(state)","s(city_state)","s(id)","s(city_state):year_s")

# newdata builder
make_nd_10 <- function(grade) {
  data.frame(
    holc_grade    = factor(grade, levels = levels(d10$holc_grade)),
    year          = yr_10,
    year_s        = yr_s_10,
    state         = lev1(d10$state),
    city_state    = lev1(d10$city_state),
    id            = lev1(d10$id),
    area_holc_km2 = 1,
    off           = off_1km2
  )
}

# per-grade marginal (population average) curves (per km²), with CIs

pred_one_10 <- function(g){
  nd <- make_nd_10(g)
  pr <- predict(m_nb_10, newdata = nd, type = "link", se.fit = TRUE,
                exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)
  cbind(nd,
        fit = pr$fit,
        lwr = pr$fit - 1.96*pr$se.fit,
        upr = pr$fit + 1.96*pr$se.fit)
}

curves_nb_10 <- rbindlist(lapply(levels(d10$holc_grade), pred_one_10))

# original scale (per km²)
curves_nb_10[, `:=`(fit_orig = exp(fit), lwr_orig = exp(lwr), upr_orig = exp(upr))]

# log breaks for obs/km²
brks <- c(0.1, 1, 10, 100)

# adj
minor_adj = 0.2

g_bam1_nb_10 <-
  ggplot(curves_nb_10, aes(year, fit_orig, colour = holc_grade, fill = holc_grade)) +
  geom_ribbon(aes(ymin = lwr_orig, ymax = upr_orig), alpha = 0.15, colour = NA) +
  geom_line() +
  coord_cartesian(ylim = c(0.039, 100)) + 
  scale_y_log10(
      name   = "Sampling density (observations per km²)",
      breaks = brks, 
      labels = scales::label_number(drop0trailing = TRUE), 
      minor_breaks = minor_breaks_log10) +
  annotate("text", x = min(curves_nb_10$year) - minor_adj, y = 5,
           label = "5", size = 2.2, colour = "grey60", hjust = 1) +
  annotate("text", x = min(curves_nb_10$year) - minor_adj, y = 20,
           label = "20", size = 2.2, colour = "grey60", hjust = 1) +
  annotate("text", x = min(curves_nb_10$year) - minor_adj, y = 40,
           label = "40", size = 2.2, colour = "grey60", hjust = 1) +              
  labs(x = "Year", subtitle = "") +
  scale_fill_manual(values = holc_pal, name = "HOLC grade") +
  scale_colour_manual(values = holc_pal, name = "HOLC grade") +
  scale_x_continuous(breaks = c(2010,2015, 2020)) +
  theme_minimal(base_size = 8) +
  theme(plot.subtitle = element_text(colour = "grey40"),
        legend.position = c(1.01, 0.475),
        legend.justification = c("right", "top"),
        legend.text  = element_text(size = 6),
        legend.title = element_text(size = 7, margin = margin(b = 0.75)),
        legend.spacing.x = unit(0.2, "lines"),
        legend.spacing.y = unit(0.2, "lines"),
        legend.key.size = unit(0.5, "lines")
    )

g_bam2_nb_10 = 
ggplot(curves_nb_10, aes(year, fit_orig, colour = holc_grade, fill = holc_grade)) +
  geom_ribbon(aes(ymin = lwr_orig, ymax = upr_orig), alpha = 0.15, colour = NA) +
  geom_line() +
  labs(y = "Sampling density (observations per km²)", x = "Year", subtitle = '') +
  scale_fill_manual(values = holc_pal) + 
  scale_colour_manual(values = holc_pal) + 
  scale_x_continuous(breaks = c(2010,2015, 2020)) +
  theme_minimal(base_size = 8) +
  theme(legend.position = "none",
        plot.subtitle = element_text(colour = "grey40"))

#g_bam1_nb / g_bam2_nb  + plot_layout(axis_title = 'collect', axes = "collect"); #ggsave('Output/bam-nb_ABCD_v3.png', width = 8, height = 9, units = 'cm')

# CONTRAST A-D
ndA_10 <- make_nd_10("A")
ndD_10 <- make_nd_10("D")

# design matrices
XpA_10 <- predict(m_nb_10, newdata = ndA_10, type = "lpmatrix",
               exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)
XpD_10 <- predict(m_nb_10, newdata = ndD_10, type = "lpmatrix",
               exclude = excl, newdata.guaranteed = TRUE, block.size = 1000)

beta_10 <- coef(m_nb_10); V_10 <-vcov(m_nb_10) # vcov(m_nb, unconditional = TRUE) 

# ln-diff
Xd_10   <- XpA_10 - XpD_10
diff_10 <- as.vector(Xd_10 %*% beta_10)
se_10   <- sqrt(rowSums((Xd_10 %*% V_10) * Xd_10))

# ratio & %
ratio_10 <- exp(diff_10); loR_10 <- exp(diff_10 - 1.96*se_10); hiR_10 <- exp(diff_10 + 1.96*se_10)
pct_10   <- (ratio_10 - 1) * 100

# absolute gap via delta method (approximates the variance of a non-linear transformation)
etaA_10 <- as.vector(XpA_10 %*% beta_10); muA_10 <- exp(etaA_10)
etaD_10 <- as.vector(XpD_10 %*% beta_10); muD_10 <- exp(etaD_10)
G_10    <- (muA_10 * XpA_10) - (muD_10 * XpD_10)
seAD_10 <- sqrt(rowSums((G_10 %*% V_10) * G_10))
abs_10  <- muA_10 - muD_10; abs_lo_10 <- abs_10 - 1.96*seAD_10; abs_hi_10 <- abs_10 + 1.96*seAD_10

out_nb_10 <- data.table(year = yr_10,
  diff = diff_10, lwr = diff_10 - 1.96*se_10, upr = diff_10 + 1.96*se_10,
  ratio = ratio_10, lo = loR_10, hi = hiR_10, pct = pct_10,
  abs = abs_10, abs_lwr = abs_lo_10, abs_upr = abs_hi_10)

# clean
rm(m_nb_10);  invisible(gc()) 

# plot on ln-scale
g_t1_nb_10 = 
ggplot(out_nb_10, aes(year, diff)) +
  geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.15) +
  geom_line() +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted A - D differences", x = "Year", subtitle = '') +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as ratio (A:D)
g_t2_nb_10 = 
ggplot(out_nb_10, aes(year, ratio)) +
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.15) +
  geom_line() +
  geom_hline(yintercept = 1, linetype = 2) +
  labs(y = "Predicted A - D differences", x = "Year", subtitle = "") +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as percent difference
g_t3_nb_10 = 
ggplot(out_nb_10, aes(year, pct)) +
  geom_ribbon(aes(ymin = (lo-1)*100, ymax = (hi-1)*100), alpha = 0.15) +
  geom_line() +
  #geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted differences (A - D)", x = "Year",  subtitle = "") +
  scale_x_continuous(breaks = c(2010,2015, 2020)) +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# plot as absolute differences
g_t4_nb_10 = 
ggplot(out_nb_10, aes(year, abs)) +
  geom_ribbon(aes(ymin = abs_lwr, ymax = abs_upr), alpha = 0.15) +
  geom_line() +
  #geom_hline(yintercept = 0, linetype = 2) +
  labs(y = "Predicted differences (A - D)", x = "Year",  subtitle = "") +
  scale_x_continuous(breaks = c(2010,2015, 2020)) +
  theme_minimal(base_size = 8)+
  theme(plot.subtitle = element_text(colour = "grey40"))

# COMBINE

g_bam1_nb_nox = g_bam1_nb + theme(axis.title.x = element_blank())  
g_bam2_nb_nox = g_bam2_nb + theme(axis.title.x = element_blank())  
g_t3_nb_nox = g_t3_nb + theme(axis.title.x = element_blank())  
g_t4_nb_nox = g_t4_nb + theme(axis.title.x = element_blank()) 

left =
  (g_bam1_nb_nox / g_bam2_nb_nox / g_t3_nb_nox / g_t4_nb_nox) +
  plot_layout(
    axes = "collect",
    axis_titles = "collect",
    heights = rep(1, 4)          # equal height for all 4 panels
  ) +
  plot_annotation(
    title = "2000-2020",
    theme = theme(
      plot.title = element_text(size = 7, face = "bold", hjust = 0.5)
    )
  ) #save(file = 'Output/freeze/Fig_2_left.Rdata', left)
g_bam1_nb_10_noy = g_bam1_nb_10 + theme(axis.title = element_blank(), legend.position = "none")
g_bam2_nb_10_noy = g_bam2_nb_10 + theme(axis.title = element_blank())  
g_t3_nb_10_noy = g_t3_nb_10 + theme(axis.title = element_blank())  
g_t4_nb_10_noy = g_t4_nb_10 + theme(axis.title = element_blank())  

right =
  (g_bam1_nb_10_noy / g_bam2_nb_10_noy / g_t3_nb_10_noy / g_t4_nb_10_noy) +
  plot_layout(
    axes = "collect",
    axis_titles = "collect",
    heights = rep(1, 4)          # equal height for all 4 panels
  ) +
  plot_annotation(
    title = "2000-2020",
    theme = theme(
      plot.title = element_text(size = 7, face = "bold", hjust = 0.5)
    )
  )#save(file = 'Output/freeze/Fig_2_right.Rdata', right)

x_title <- ggplot() +
  theme_void() +
  labs(x = "Year") +
  theme(
    axis.title.x = element_text(size = 8),
    plot.margin = margin(t = 0, r = 0, b = 0, l = 0)
  )

col_title <- function(txt, size = 7) {
  ggplot() +
    theme_void() +
    labs(title = txt) +
    theme(
      plot.title = element_text(size = size, face = "bold", hjust = 0.5),
      plot.margin = margin(0, 0, 0, 0)
    )
}

left_tit = col_title("2000–2020") / left  + plot_layout(heights = c(0.0125, 1))
right_tit = col_title("2010–2020") / right  + plot_layout(heights = c(0.0125, 1))

both = 
  (left_tit | right_tit) /
  x_title +
  plot_layout(heights = c(1, 0.0125))

both; #ggsave('Output/Fig_2_v2.png', both, width = 10, height = 9*2, units = 'cm')

}

#' <a name="F_2">
#' **Figure 2</a> | Non-linear temporal changes in bird-sampling density by HOLC grade and disparity between grades A and D.** **Top two rows**, population-level (marginal) predictions from a negative binomial generalised additive model (`bam`) with log link and centered `log(area)` offset, fitted to polygon-level counts for 2000-2020 (left) and 2010-2020 (right) data. The model included a smooth for year, grade-specific smooth deviations, and random effects for state, city, and polygon, and city-specific temporal slopes. Curves show predicted sampling density (observations per km²) for each HOLC grade on a log scale (1st row) and original scale (2nd row). **Bottom two rows**, predicted differences between grades A and D from the same model fitted to observations from 2000-2020 (left) and 2010-2020 (right), expressed as percent difference relative to D (3rd row) and as absolute difference in observations per km² (bottom row). Relative disparity varies non-linearly over time and exceeds 350% by 2020, whereas absolute differences remain small (< ~25 observations per km²), indicating that large proportional disparities do not translate into large absolute changes in sampling intensity. *In all panels*, shaded ribbons are 95% confidence intervals.
#'
#' ***
#'  
#' The non-linear smooth model confirms marked non-linear temporal variation in sampling density across HOLC grades (our Fig. [2](#F_2)). All grades show an increase in sampling rates after ~2010, reflecting the general expansion of community-science activity, but grade A neighbourhoods consistently exhibit higher rates than grade D.
#' 
#' The model-based A–D contrast on the log scale changes little over time, corresponding to percentage differences that fluctuate between ~100% and ~350% across years. Specifically, relative disparity of ~100% remains stable between 2000–2005, increases by ~150% between 2005–2010, is broadly stable or slightly declines between 2010–2017, and then increases sharply reaching ~350% by 2020. In contrast, absolute disparity shows virtually no difference between grades until approximately 2010, after which A exceeds D by only 5 observations per km² by ~2017 and up to ~25 observations per km² by 2020, a small absolute magnitude despite large relative changes.
#' 
#'In other words, pre-2010 absolute gaps between HOLC A and D are tiny though the relative difference is already >0; after ~2010 both relative and absolute gaps increase (coinciding with the post-2010 surge in smartphone-driven community science activity), albeit maximum absolute difference is still only ~5 observations per km² and year by 2017. The sharp increase thereafter (up to ~25 observations) coincides with COVID-19-restrictions driven increase in human activity in urban green areas in spring 2020.
#' 
#' #### Conclusions
#' These non-linear patterns contradict the authors’ reported 35.6% linear increase. Our estimates show that (i) A-D disparity does not follow a linear trajectory, (ii) the relative magnitude of change exceed ~200% across the 2000-2020 period, i.e. is substantially greater than 35.6%, and (iii) absolute differences remain small (typically <25 observations per km²), indicating that the apparent inflation of disparity is driven largely by relative scaling rather than large biological effects. 
#'  
#' ***
#' 
#' # Model assumptions {#Model_ass}

#' The model formula consists of the model type (lm - linear model, glm - generalised linear model, lmer - linear mixed-effect model, glmer - generalised linear mixed-effect model, glmmTMB – generalised linear mixed‐effects model fitted via Template Model Builder, gam - generalised additive model, glm.nb – negative binomial generalised linear model, bam – generalised additive model for large dataset), the response (left of ~), predictors (right of ~) with predictors in paranthesis indicating random effects, the predictors without paranthesis fixed effects. Random effects left of '|' are random slopes, right of '|' are random intercepts. Explicitly nested random intercepts are separated by '/'. 'scale' indicates z-transformation whereas log (in R) means natural logarithm.
#' 
#' ## Fig. 2
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_2_2000-2020.png"))

knitr::include_graphics(here::here("Output", "Model_ass", "Fig_2_2010-2020.png"))
#' 
#' ## Extended Data Fig. 3 & Fig. S8
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2000-2020_f.png"))

knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_E3_&_S8_2010-2020_f.png"))
#' 
#' ## Fig S4 - Sampled yes or no
#' Below diagnostics of binomial models (Pearson residuals, binned fits, QQ-plots, partial autocorrelation, and spatial semivariogram) indicate good fit and no substantial temporal or spatial autocorrelation (Moran’s I ≈ 0). Pearson dispersion ≈ 0.6 suggested mild underdispersion, which makes inference conservative rather than inflated, which is desired. Gaussian mixed models, despite non-normal residuals, yielded consistent effect directions and magnitudes compared to binomial models (see Fig. [S4](#F_S4)), supporting the robustness of the main results (cf. Knief & Forstmeier 2021).
#'   
#' ### binomial
rm(list=setdiff(ls(), c("holc_pal"))); invisible(gc())

knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S4i.png"))
#'  
#' ### gaussian
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "gaus_Fig_S4i.png"))

#' ## Fig. S5 - Sampling density
#' Across all model structures, the below diagnostics confirmed that the Negative Binomial models of record counts with offset(log area km²) provided a much better fit than Gaussian log-rate formulations. Residuals showed no systematic mean–variance relationship, dispersion values (~2–4) indicated acceptable extra-Poisson variation, and Moran’s I and PACF tests revealed no spatial or temporal autocorrelation. Normal QQ plots displayed only mild right-tail deviations expected for count data with a few extreme observations. 
#' 
#' Models using msa_NAME (metropolitan area) as the grouping factor exhibited slightly more homogeneous residuals than those using city_state, but the difference was minor. Models with city-state as the grouping factor captured finer spatial structure and represent a more conservative specification. Overall, the Negative Binomial models with offset approach accurately captured variation in sampling intensity while maintaining well-behaved residuals and stable dispersion across predictors.

model_comp <- data.table(
  Model_type = c(
    "log(rate), non-zero",
    "log(rate + ε)",
    "counts + offset(log(area)) (NB2)"
  ),
  Zeros_included = c("No", "Yes (ε-adjusted)", "Yes (native)"),
  Heteroscedasticity = c("Mild", "Strong (fan shape)", "Minimal"),
  Pearson_dispersion = c("3–4", "6+", "≈2–4"),
  Spatial_autocorr = c("Weak", "Weak", "Weak"),
  Conceptual_validity = c(
    "Biased (zero omission)",
    "Arbitrary transform",
    "Correct data-generating process"
  )
)

model_comp %>%
  kableExtra::kbl(align='l', linesep = "", caption = "Comparison of model formulations for sampling intensity analysis") %>%
  kableExtra::kable_paper(c("striped", "condensed"), full_width = F, position = "left")
#' 
#' ### Gaussian models of ln-transformed sampling density excluding zeros
# sampling density non-zero; log(sempling density) as response; Gaussian
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S5i.png"))
#'  
#' ### Gaussian models of ln-transformed sampling density, including zeros
#' A small data-derived offset of 0.125 added to the sampling density (including zeros) before ln-transformation.

# sampling density including-zero; log(sempling density + 0.125) as response; Gaussian
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "a_Fig_S5i.png"))
#'
#' ### Negative-binomial count models with `ln(area km²)` offset
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "r_Fig_S5i.png"))
#' 

#' ## Fig. S6 - Completeness of sampling
#' The plots of model assumptions reveal that the replicated models are statistically sound, well behaved, and reproduce the expected ecological trend. The diagnostics show no major violations, only mild heteroscedasticity consistent with percentage data. The elevated dispersion (≈ 400–480) stems from the 0–100 % response scale and is therefore not problematic. The comparison indicates that our implementation reduces potential bias from omitted variables and yields more reliable estimates of completeness disparities across HOLC grades.
#' 
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6a.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6b.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6c.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6d.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6e.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6f.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6g.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6h.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Fig_S6i.png"))

#' ## Table S3
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3a_correct_data.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3a_multiplied_data.png"))

knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3b_correct_data.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3b_multiplied_data.png"))

knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3c_correct_data.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3c_multiplied_data.png"))

knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3d_correct_data.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S3d_multiplied_data.png"))

#' Table S4
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S4_all_years.png"))
knitr::include_graphics(here::here("Output", "Model_ass", "Table_S4_2010-2020_years.png"))

#' ***
#' # References 
#' <div id="refs"></div>
#' 
#' ***
#' 
#' # Session info  
#' <br>
#' 
#' <a name="T_S5">
#' **Table S5 | System session info.** </a>
df_session_platform <- devtools::session_info()$platform %>%
    unlist(.) %>%
    as.data.frame(.) %>%
    tibble::rownames_to_column(.)

colnames(df_session_platform) <- c("Item", "Value")

df_session_platform %>%
  kableExtra::kbl(align=c('r', 'l'), linesep = "") %>%
  kableExtra::kable_paper(c("striped", "condensed"), full_width = F, position = "left")
#'    
#' <br>   
#'
#' <a name="T_S6">
#' **Table S6 | Info about used packages.** </a>
df_session_packages <- devtools::session_info()$packages %>%
    as.data.frame(.) %>%
    # filter(attached == TRUE) %>%
    dplyr::select(loadedversion, date, source) %>%
    tibble::rownames_to_column()

colnames(df_session_packages) <- c("Package", "Loaded version", "Date", "Source")
df_session_packages %>%
    kableExtra::kbl(align = c("l", "l","l","l"), linesep = "") %>%
    kableExtra::kable_paper(c("striped", "condensed"), full_width = F, position = "left") %>%
        kableExtra::scroll_box(width = "90%", height = "350px")
#'
#' ***
#' 